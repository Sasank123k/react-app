package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.config.JsonGenerationConfig;
import com.wellsfargo.utcap.exception.ConfigurationException;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(JsonRequirementService.class);
    private static final List<Map<String, Object>> DIMC_CHECKS_MASTER_LIST = new ArrayList<>();

    private final ApplicationAuthRepository applicationAuthRepository;
    private final RequirementRepository requirementRepository;
    private final JsonRequirementRepository jsonRequirementRepository;
    private final JiraStoryIntakeRepository jiraStoryIntakeRepository;
    private final SorRepository sorRepository;
    private final SQLScriptRepository sqlScriptRepository;
    private final ConnJsonRequirementRepository connJsonRequirementRepository;
    private final ObjectMapper objectMapper;

    @Autowired
    public JsonRequirementService(ApplicationAuthRepository applicationAuthRepository,
            RequirementRepository requirementRepository,
            JsonRequirementRepository jsonRequirementRepository,
            JiraStoryIntakeRepository jiraStoryIntakeRepository,
            SorRepository sorRepository,
            SQLScriptRepository sqlScriptRepository,
            ConnJsonRequirementRepository connJsonRequirementRepository,
            ObjectMapper objectMapper) {
        this.applicationAuthRepository = applicationAuthRepository;
        this.requirementRepository = requirementRepository;
        this.jsonRequirementRepository = jsonRequirementRepository;
        this.jiraStoryIntakeRepository = jiraStoryIntakeRepository;
        this.sorRepository = sorRepository;
        this.sqlScriptRepository = sqlScriptRepository;
        this.connJsonRequirementRepository = connJsonRequirementRepository;
        this.objectMapper = objectMapper;
        populateDimcChecksMasterList();
    }

    public JsonRequirement saveJson(String requirementId, Object newJson) {
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId)
                .orElse(new JsonRequirement(requirementId, null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public JsonRequirement getOrCreateJsonRequirement(String requirementId)
            throws JsonProcessingException, ConfigurationException {
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);
        if (existingJsonRequirement.isPresent()) {
            LOG.debug("Returning existing JSON requirement for ID: {}", requirementId);
            return existingJsonRequirement.get();
        }

        LOG.info("Generating new JSON requirement for ID: {}", requirementId);
        JsonGenerationConfig config = buildJsonGenerationConfig(requirementId);

        ObjectNode rootNode = objectMapper.createObjectNode();
        rootNode.set("application", createApplicationNode(config));

        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {
        });
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);
        return jsonRequirementRepository.save(jsonRequirement);
    }

    private JsonGenerationConfig buildJsonGenerationConfig(String requirementId)
            throws ConfigurationException, JsonProcessingException {
        String jiraStoryId = requirementId.split("_")[0];

        JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
        Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                .orElseThrow(
                        () -> new ConfigurationException("Sor not found for name: " + jira.getApplicationSorName()));
        ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName()).stream()
                .findFirst()
                .orElseThrow(() -> new ConfigurationException(
                        "ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
        SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                .orElseThrow(
                        () -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

        JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
        JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

        if (sourceTypeEnum == JsonGenerationConfig.SourceType.UNKNOWN)
            throw new ConfigurationException("Unknown Source Type: " + jira.getSorType());
        if (targetTypeEnum == JsonGenerationConfig.TargetType.UNKNOWN)
            throw new ConfigurationException("Unknown Target Type: " + jira.getTargetType());

        String inputTableName = jira.getTableName() != null ? jira.getTableName().toLowerCase(Locale.ROOT) : "";
        String inputSourceSchema = jira.getSourceSchema() != null ? jira.getSourceSchema().toLowerCase(Locale.ROOT)
                : "";

        // Change #4: Modify resolvedJobNameString for file types with BMG SOR
        String resolvedJobNameString;
        if ("bmg".equalsIgnoreCase(sor.getSorName()) && !isFileSource(sourceTypeEnum)) { // Added !isFileSource
                                                                                         // condition
            resolvedJobNameString = (!inputSourceSchema.isEmpty() ? inputSourceSchema + "_" : "") + inputTableName;
        } else {
            resolvedJobNameString = inputTableName;
        }

        // Change #7: Determine jobSpecificAppValue for "app" key in job object
        String jobSpecificAppValue;
        if (auth.getApplicationQualifier() != null && !auth.getApplicationQualifier().isEmpty()) {
            jobSpecificAppValue = auth.getApplicationQualifier();
        } else if (auth.getApplicationName() != null) {
            jobSpecificAppValue = auth.getApplicationName().toLowerCase(Locale.ROOT);
        } else {
            jobSpecificAppValue = jira.getApplicationName() != null ? jira.getApplicationName().toLowerCase(Locale.ROOT)
                    : "default_app_value"; // Fallback
            LOG.warn(
                    "ApplicationQualifier and ApplicationName from ApplicationAuthorizer are missing for JIRA app: {}. Using JIRA app name or default for job 'app' key.",
                    jira.getApplicationName());
        }

        boolean includeIngestions = (sourceTypeEnum != JsonGenerationConfig.SourceType.HIVE);
        boolean includeFlows = (targetTypeEnum != JsonGenerationConfig.TargetType.GCP);
        boolean includeDataChecks = true;
        boolean includeFileExtracts = (targetTypeEnum == JsonGenerationConfig.TargetType.GCP);

        Set<String> jobLevelFieldsToInclude = determineJobLevelFieldsToInclude(targetTypeEnum);
        Set<String> ingestionFieldsToInclude = determineIngestionFieldsToInclude(sourceTypeEnum, includeIngestions,
                targetTypeEnum);

        String resolvedIngestionSourceTypeValue = includeIngestions ? resolveIngestionSourceTypeValue(sourceTypeEnum)
                : null;
        String resolvedIngestionConnectionHeaderValue = includeIngestions
                ? resolveIngestionConnectionHeaderValue(sourceTypeEnum)
                : null;

        // MODIFICATION #2: Simplified resolvedIngestionDataFrameName derivation
        String resolvedIngestionDataFrameName = includeIngestions ? resolvedJobNameString + "_df" : null;

        String resolvedIngestionSourceSQLQuery = null;
        if (includeIngestions && isDatabaseSource(sourceTypeEnum) && sqlScript != null) {
            resolvedIngestionSourceSQLQuery = sqlScript.getSrcTableQuery();
        }
        String resolvedIngestionSourceName = (includeIngestions
                && sourceTypeEnum == JsonGenerationConfig.SourceType.ORACLE) ? "${BDDMCBD_SRC_SCHEMA}." + inputTableName
                        : null;

        String resolvedIngestionDataFilePath = (includeIngestions && isFileSource(sourceTypeEnum))
                ? "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}"
                : null;
        String resolvedIngestionSchemaFilePath = (includeIngestions && isFileSource(sourceTypeEnum))
                ? "${DCI_CONFIG_DIR_PATH}/config/" + sor.getSorName().toLowerCase() + "/${FEED_NAME}/metadata/"
                        + inputTableName + ".schema"
                : null;
        // MODIFICATION #1: Call updated determineStagingFilePath
        String resolvedIngestionStagingFilePath = (includeIngestions && isFileSource(sourceTypeEnum))
                ? determineStagingFilePath()
                : null;
        String resolvedIngestionDelimiter = includeIngestions ? resolveIngestionDelimiter(sourceTypeEnum) : null;

        String resolvedIngestionIsHeaderEnabledValue = (includeIngestions && isFileSource(sourceTypeEnum)) ? "Y" : null;

        String ingestionIsCreateTargetTableValue = includeIngestions ? "Y" : null;
        String ingestionIsCacheEnabledValue = includeIngestions ? "Y" : null;
        String ingestionIsOverWritePartitionValue = includeIngestions ? "Y" : null;
        String ingestionIsRowCountDisabledValue = (includeIngestions
                && sourceTypeEnum == JsonGenerationConfig.SourceType.TERADATA) ? "N" : null;

        ObjectNode prebuiltIngestionAdditionalReadConfigsNode = includeIngestions
                ? buildPrebuiltIngestionAdditionalReadConfigs(sourceTypeEnum, this.objectMapper)
                : null;

        boolean includeDimcChecksForThisSource = includeIngestions &&
                (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE)
                        .contains(sourceTypeEnum) ||
                        isFileSource(sourceTypeEnum));
        List<Integer> chosenDimcCheckIndices = includeDimcChecksForThisSource
                ? determineSourceSpecificDimcIndices(sourceTypeEnum)
                : Collections.emptyList();
        ObjectNode prebuiltIngestionDimcChecksNode = (includeDimcChecksForThisSource
                && !chosenDimcCheckIndices.isEmpty())
                        ? buildPrebuiltDimcChecksNode(chosenDimcCheckIndices, this.objectMapper)
                        : null;

        List<JsonGenerationConfig.FlowDetailConfig> flowDetailsList = includeFlows
                ? prepareFlowDetails(sourceTypeEnum, targetTypeEnum, jira, sor, auth, sqlScript,
                        resolvedIngestionDataFrameName, resolvedJobNameString)
                : Collections.emptyList();

        List<JsonGenerationConfig.CheckDetailConfig> checkDetailsList = includeDataChecks
                ? prepareCheckDetails(sourceTypeEnum, targetTypeEnum, jira, auth, flowDetailsList,
                        resolvedIngestionDataFrameName, resolvedJobNameString)
                : Collections.emptyList();

        // MODIFICATION #1: Call updated determineDataChecksAcDcDatabase
        String resolvedDataChecksAcDcDatabase = includeDataChecks ? determineDataChecksAcDcDatabase() : null;
        ObjectNode prebuiltDataChecksPreChecksNode = includeDataChecks
                ? buildPrebuiltDataChecksPreChecksNode(sourceTypeEnum, this.objectMapper)
                : null;

        return new JsonGenerationConfig(
                jira, sor, auth, sqlScript, sourceTypeEnum, targetTypeEnum, resolvedJobNameString, jobSpecificAppValue,
                includeIngestions, includeFlows, includeDataChecks, includeFileExtracts,
                jobLevelFieldsToInclude, ingestionFieldsToInclude,
                resolvedIngestionSourceTypeValue, resolvedIngestionConnectionHeaderValue,
                resolvedIngestionDataFrameName, resolvedIngestionSourceSQLQuery,
                resolvedIngestionSourceName, resolvedIngestionDataFilePath,
                resolvedIngestionSchemaFilePath, resolvedIngestionStagingFilePath,
                resolvedIngestionDelimiter, resolvedIngestionIsHeaderEnabledValue,
                ingestionIsCreateTargetTableValue, ingestionIsCacheEnabledValue, ingestionIsOverWritePartitionValue,
                ingestionIsRowCountDisabledValue,
                prebuiltIngestionAdditionalReadConfigsNode,
                prebuiltIngestionDimcChecksNode,
                flowDetailsList,
                resolvedDataChecksAcDcDatabase, checkDetailsList, prebuiltDataChecksPreChecksNode);
    }

    private boolean isDatabaseSource(JsonGenerationConfig.SourceType sourceType) {
        return EnumSet
                .of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.MSSQL,
                        JsonGenerationConfig.SourceType.ORACLE, JsonGenerationConfig.SourceType.HIVE)
                .contains(sourceType);
    }

    private boolean isFileSource(JsonGenerationConfig.SourceType sourceType) {
        return EnumSet.of(JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH, JsonGenerationConfig.SourceType.FILE_PIPE,
                JsonGenerationConfig.SourceType.FILE_COMMA).contains(sourceType);
    }

    private Set<String> determineJobLevelFieldsToInclude(JsonGenerationConfig.TargetType targetType) {
        return new HashSet<>(Arrays.asList("app", "appRemedyId", "sorRemedyId", "queryGroup", "zone", "sor"));
    }

    private Set<String> determineIngestionFieldsToInclude(JsonGenerationConfig.SourceType sourceType,
            boolean includeIngestionsFlag, JsonGenerationConfig.TargetType targetType) {
        if (!includeIngestionsFlag)
            return Collections.emptySet();
        Set<String> fields = new HashSet<>(Arrays.asList(
                "ingestionId", "ingestionDesc", "isIngestionEnabled", "dataFrameName",
                "isCreateTargetTable", "isCacheEnabled", "isOverWritePartition",
                "abortIfDuplicate", "abortIfEmpty", "isRunEMHReport", "sourceType"));

        if (targetType == JsonGenerationConfig.TargetType.HIVE
                && sourceType == JsonGenerationConfig.SourceType.TERADATA) {
            fields.add("targetType");
        }

        if (isDatabaseSource(sourceType)) {
            fields.add("ConnectionHeader");
            fields.add("sourceSQLQuery");
            if (sourceType == JsonGenerationConfig.SourceType.ORACLE)
                fields.add("sourceName");
            if (sourceType == JsonGenerationConfig.SourceType.TERADATA)
                fields.add("isRowCountDisabled");
        } else if (isFileSource(sourceType)) {
            fields.addAll(Arrays.asList("isHeaderEnabled", "dataFilePath", "schemaFilePath", "stagingFilePath",
                    "additionalReadConfigs"));
            if (sourceType != JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH)
                fields.add("delimiter");
        }
        if (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE)
                .contains(sourceType) || isFileSource(sourceType)) {
            fields.add("dimcChecks");
        }
        return fields;
    }

    private String resolveIngestionSourceTypeValue(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA:
                return "teradb";
            case FILE_FIXEDWIDTH:
                return "fixedwidthFile";
            case FILE_PIPE:
            case FILE_COMMA:
                return "file";
            case MSSQL:
                return "jdbc";
            case ORACLE:
                return "oracle";
            case HIVE:
                return "hive";
            default:
                return "unknown";
        }
    }

    private String resolveIngestionConnectionHeaderValue(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA:
                return "teradbConn";
            case MSSQL:
                return "jdbcConn";
            case ORACLE:
                return "oracleConn";
            case HIVE:
                return "hiveConn";
            default:
                return null;
        }
    }

    // MODIFICATION #1: Removed unused parameters
    private String determineStagingFilePath() {
        return "${FILE_PATH}";
    }

    private String resolveIngestionDelimiter(JsonGenerationConfig.SourceType sourceType) {
        if (sourceType == JsonGenerationConfig.SourceType.FILE_PIPE)
            return "|";
        if (sourceType == JsonGenerationConfig.SourceType.FILE_COMMA)
            return ",";
        return null;
    }

    private ObjectNode buildPrebuiltIngestionAdditionalReadConfigs(JsonGenerationConfig.SourceType sourceType,
            ObjectMapper mapper) {
        if (isFileSource(sourceType)) {
            ObjectNode arNode = mapper.createObjectNode();
            arNode.put("quote", "\"");
            arNode.put("escape", "\"");
            return arNode;
        }
        return null;
    }

    // Change #8: Unify DIMC Check Indices for all file types
    private List<Integer> determineSourceSpecificDimcIndices(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA:
            case HIVE:
                return Arrays.asList(4, 5, 3, 1);
            case FILE_FIXEDWIDTH:
            case FILE_PIPE:
            case FILE_COMMA:
                return Arrays.asList(6, 0, 1, 2, 3, 7, 8); // Unified list
            default:
                return Collections.emptyList();
        }
    }

    private ObjectNode buildPrebuiltDimcChecksNode(List<Integer> chosenIndices, ObjectMapper mapper)
            throws JsonProcessingException {
        List<Map<String, Object>> selectedChecks = getChosenDimcChecksListInternal(chosenIndices);
        if (selectedChecks.isEmpty())
            return null;

        ObjectNode dimcChecksWrapper = mapper.createObjectNode();
        dimcChecksWrapper.put("isDimcChecksEnabled", "Y");
        ArrayNode dimcCheckArray = mapper.valueToTree(selectedChecks);
        dimcChecksWrapper.set("dimcCheck", dimcCheckArray);
        return dimcChecksWrapper;
    }

    private static List<Map<String, Object>> getChosenDimcChecksListInternal(List<Integer> chosenIndices) {
        if (chosenIndices == null || chosenIndices.isEmpty() || DIMC_CHECKS_MASTER_LIST.isEmpty()) {
            return Collections.emptyList();
        }
        return chosenIndices.stream()
                .filter(index -> index != null && index >= 0 && index < DIMC_CHECKS_MASTER_LIST.size())
                .map(index -> new HashMap<>(DIMC_CHECKS_MASTER_LIST.get(index)))
                .collect(Collectors.toList());
    }

    private List<JsonGenerationConfig.FlowDetailConfig> prepareFlowDetails(
            JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
            JiraStoryIntake jira, Sor sor, ApplicationAuthorizer auth, SQLScript sqlScript,
            String ingestionDataFrameNameIfAny, String currentResolvedJobName) {

        List<JsonGenerationConfig.FlowDetailConfig> flowConfigs = new ArrayList<>();
        int numberOfFlows = (sourceType == JsonGenerationConfig.SourceType.MSSQL
                || sourceType == JsonGenerationConfig.SourceType.ORACLE) ? 2 : 1;

        String appNameForFlowLogic = jira.getApplicationName() != null
                ? jira.getApplicationName().toLowerCase(Locale.ROOT)
                : "defaultapp";
        String commonHivePartitionColumn = appNameForFlowLogic + "_business_effective_date";

        String flowIsCreateTargetTable = "N";
        String flowIsCacheEnabled = "Y";
        String flowSendEmail = "N";
        String flowSaveOutput = "Y";
        String flowIsOverWritePartition = "Y";
        String flowAbortIfEmpty = "N";

        String baseNameForFlow = currentResolvedJobName;

        for (int i = 0; i < numberOfFlows; i++) {
            String flowId = String.valueOf(i + 1);
            boolean isFirstFlow = (i == 0);

            String flowZone = JsonGenerationConfig.SANITIZED_ZONE;
            if (numberOfFlows == 2 && isFirstFlow) {
                flowZone = JsonGenerationConfig.RAW_ZONE;
            }
            String appNameForDescription = jira.getApplicationName() != null ? jira.getApplicationName()
                    : "UNKNOWN_APP";
            String flowDesc = sor.getSorName().toUpperCase(Locale.ROOT) +
                    JsonGenerationConfig.TABLE_TEXT + baseNameForFlow +
                    JsonGenerationConfig.INGESTION_TO_TEXT + appNameForDescription + " - " + flowZone;

            String query = sqlScript.getSrcDataFrameQuery();
            String registerName = baseNameForFlow + "_" + flowZone;
            String flowDatabaseName = "";
            String storageName = "";
            String targetTablePath = null;

            if (targetType == JsonGenerationConfig.TargetType.HIVE) {
                if (isFileSource(sourceType)) {
                    storageName = "${TARGET_TABLE_NAME}";
                    targetTablePath = "${FILE_PATH}/${TARGET_TABLE_NAME}";
                    flowDatabaseName = "${HIVE_TARGET_DBNAME}";
                } else {
                    Map<String, String> storageNameMap = Map.of("CNAPP", "consumer", "UTCAP", "utcap");
                    String appSubDir = storageNameMap.getOrDefault(jira.getApplicationName(),
                            jira.getApplicationName() != null ? jira.getApplicationName().toLowerCase() : "defaultapp");
                    storageName = "/datalake${EDL_ENV}/" + appSubDir + "/" +
                            sor.getDomainName().toLowerCase(Locale.ROOT) + "/" +
                            sor.getSorName().toLowerCase(Locale.ROOT) + "/" +
                            flowZone + "/" + baseNameForFlow;
                    flowDatabaseName = "${ETL_ENV}" + appSubDir + "_" + sor.getSorName().toLowerCase() + "_" + flowZone;
                }
            }

            flowConfigs.add(new JsonGenerationConfig.FlowDetailConfig(
                    flowId, flowDesc, query, registerName, flowDatabaseName, storageName, targetTablePath,
                    commonHivePartitionColumn, flowIsCreateTargetTable, flowIsOverWritePartition, flowIsCacheEnabled,
                    flowSendEmail, flowAbortIfEmpty, flowSaveOutput));
        }
        return flowConfigs;
    }

    private List<JsonGenerationConfig.CheckDetailConfig> prepareCheckDetails(
            JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
            JiraStoryIntake jira, ApplicationAuthorizer auth,
            List<JsonGenerationConfig.FlowDetailConfig> flowDetails, String ingestionDataFrameName,
            String currentResolvedJobName) {

        List<JsonGenerationConfig.CheckDetailConfig> checkConfigs = new ArrayList<>();
        if (targetType == JsonGenerationConfig.TargetType.GCP || flowDetails.isEmpty()) {
            return checkConfigs;
        }

        String tableNameForCheck = currentResolvedJobName;
        String appNameForPartitionPlaceholder = jira.getApplicationName() != null
                ? jira.getApplicationName().toUpperCase()
                : "DEFAULTAPP";

        for (JsonGenerationConfig.FlowDetailConfig currentFlow : flowDetails) {
            String checkId = currentFlow.getFlowId();
            String checkName = "HHRCNTCHK";
            String checkDesc = "Data Count Check";

            String checkSourceQueryDataFrame;
            String sourceHiveDbForCheck = jira.getSourceSchema() != null ? jira.getSourceSchema()
                    : "${SOURCE_HIVE_DATABASE}";
            if (sourceType == JsonGenerationConfig.SourceType.HIVE) {
                String hiveSourceTableName = "bmg".equalsIgnoreCase(jira.getApplicationSorName())
                        ? currentResolvedJobName
                        : (jira.getSourceTableName() != null ? jira.getSourceTableName() : jira.getTableName());
                checkSourceQueryDataFrame = sourceHiveDbForCheck + "." + hiveSourceTableName;
            } else if (ingestionDataFrameName != null) {
                checkSourceQueryDataFrame = ingestionDataFrameName;
            } else {
                LOG.warn("Cannot determine source DataFrame for data check for source: {} and flow: {}", sourceType,
                        currentFlow.getFlowId());
                continue;
            }

            String checkSourceQuery = "select count(*) from " + checkSourceQueryDataFrame;
            String partitionValuePlaceholder = "${" + appNameForPartitionPlaceholder + "_BUSINESS_EFFECTIVE_DATE}";
            String checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowDatabaseName() + "."
                    + tableNameForCheck +
                    " where " + currentFlow.getResolvedFlowHivePartitionColumn() + "='" + partitionValuePlaceholder
                    + "'";

            checkConfigs.add(new JsonGenerationConfig.CheckDetailConfig(
                    checkId, checkName, checkDesc, checkSourceQuery, checkTargetQuery,
                    "Y", "Y", "N", "N",
                    "Y" // Change #2: isFailJobOnDataCheckErrorValue set to "Y"
            ));
        }
        return checkConfigs;
    }

    // MODIFICATION #1: Removed unused parameters
    private String determineDataChecksAcDcDatabase() {
        return "${AUDIT_DATABASE}";
    }

    private String toUpperSnakeCase(String camelOrPascalCase) {
        if (camelOrPascalCase == null || camelOrPascalCase.isEmpty()) {
            return "";
        }
        Pattern pattern = Pattern.compile("(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])");
        return pattern.matcher(camelOrPascalCase).replaceAll("_").toUpperCase(Locale.ROOT);
    }

    private ObjectNode buildPrebuiltDataChecksPreChecksNode(JsonGenerationConfig.SourceType sourceType,
            ObjectMapper mapper) {
        ObjectNode preChecksNode = mapper.createObjectNode();
        preChecksNode.put("isPreCheckEnabled", "${PRECHECK_ENABLED}");
        ArrayNode preCheckArray = preChecksNode.putArray("preCheck");

        List<String> preCheckNames = Arrays.asList(
                "ServiceIDCheck", "TargetDataBaseAvailabilityCheck", "TargetTableAvailabilityCheck",
                "TargetTablePathWritableCheck", "FlowSeqDependenciesCheck", "SourceDataBaseAvailabilityCheck",
                "SourceTableAvailabilityCheck", "SourceTableEmptyCheck", "SourcePathAvailabilityCheck",
                "SourcePathEmptyCheck");

        for (String checkName : preCheckNames) {
            String upperSnakeCaseName = toUpperSnakeCase(checkName);
            String enabledPlaceholder = "${" + upperSnakeCaseName + "_ENABLED}";
            String failureFlagPlaceholder = "${" + upperSnakeCaseName + "_FAILURE_FLAG}";
            addPreCheckItem(preCheckArray, checkName, enabledPlaceholder, failureFlagPlaceholder, mapper);
        }
        return preChecksNode;
    }

    private void addPreCheckItem(ArrayNode array, String name, String enabledPlaceholder, String failOnErrorPlaceholder,
            ObjectMapper mapper) {
        ObjectNode item = mapper.createObjectNode();
        item.put("preCheckName", name);
        item.put("isPreCheckEnabled", enabledPlaceholder);
        item.put("isFailJobOnPreCheckError", failOnErrorPlaceholder);
        array.add(item);
    }

    private ObjectNode createApplicationNode(JsonGenerationConfig config) {
        ObjectNode applicationNode = objectMapper.createObjectNode();
        applicationNode.put("isParallelProcessing", "N");
        applicationNode.set("jobs", createJobsNode(config));
        return applicationNode;
    }

    private ObjectNode createJobsNode(JsonGenerationConfig config) {
        ObjectNode jobsNode = objectMapper.createObjectNode();
        jobsNode.set("job", createJobNode(config));
        return jobsNode;
    }

    private ObjectNode createJobNode(JsonGenerationConfig config) {
        ObjectNode jobNode = objectMapper.createObjectNode();
        Sor sor = config.getSor();
        ApplicationAuthorizer auth = config.getApplicationAuthorizer();

        jobNode.put("JobName", config.getResolvedJobName());

        Set<String> jobFields = config.getJobLevelFieldsToInclude();
        // Change #7: Use resolvedJobAppKeySetting for "app" key
        if (jobFields.contains("app"))
            jobNode.put("app", config.getResolvedJobAppKeySetting());
        if (jobFields.contains("appRemedyId"))
            jobNode.put("appRemedyId", auth.getAppRemedyId());
        if (jobFields.contains("sorRemedyId"))
            jobNode.put("sorRemedyId", sor.getRemedyId());
        if (jobFields.contains("queryGroup"))
            jobNode.put("queryGroup", "dimc_check_" + sor.getSorName().toLowerCase(Locale.ROOT));

        String zone = JsonGenerationConfig.SANITIZED_ZONE;
        if (jobFields.contains("zone"))
            jobNode.put("zone", zone);
        if (jobFields.contains("sor"))
            jobNode.put("sor", sor.getSorName().toLowerCase(Locale.ROOT));

        jobNode.put("jobId", "1");
        jobNode.put("jobDescription", determineJobDescription(config));
        jobNode.put("isJobEnabled", "Y");
        jobNode.set("Email", createEmailNode(config));

        if (config.isIncludeIngestions()) {
            ObjectNode ingestionsNode = buildGenericIngestionNode(config);
            if (ingestionsNode != null && ingestionsNode.size() > 0)
                jobNode.set("ingestions", ingestionsNode);
        }
        if (config.isIncludeFlows()) {
            ObjectNode flowsNode = buildGenericFlowsNode(config);
            if (flowsNode != null && flowsNode.size() > 0)
                jobNode.set("flows", flowsNode);
        }
        if (config.isIncludeDataChecks()) {
            ObjectNode dataChecksNode = buildGenericDataChecksNode(config);
            if (dataChecksNode != null && dataChecksNode.size() > 0)
                jobNode.set("dataChecks", dataChecksNode);
        }
        if (config.isIncludeFileExtracts()) {
            ObjectNode fileExtractsNode = buildGenericFileExtractsNode(config);
            if (fileExtractsNode != null && fileExtractsNode.size() > 0)
                jobNode.set("fileExtracts", fileExtractsNode);
        }
        return jobNode;
    }

    // Change #1: Job Description for Hive target new format
    // Change #3: Job Description for GCP target - remove tableName condition,
    // always use resolvedJobName
    // Change #6: Job Description for GCP target - "Extract" to "Ingestion"
    private String determineJobDescription(JsonGenerationConfig config) {
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        String jobNameForDesc = config.getResolvedJobName();
        String appNameForDesc = jira.getApplicationName() != null ? jira.getApplicationName() : "UNKNOWN_APP";

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            return "CDMP Ingestion for Hive table - " + jobNameForDesc; // Change #3 & #6
        } else { // For HIVE target
            return sor.getSorName().toUpperCase(Locale.ROOT) + // Change #1
                    JsonGenerationConfig.TABLE_TEXT + jobNameForDesc +
                    JsonGenerationConfig.INGESTION_TO_TEXT + appNameForDesc;
        }
    }

    // Change #6: Email subject for GCP target - "Extraction" to "Ingestion"
    private ObjectNode createEmailNode(JsonGenerationConfig config) {
        ObjectNode emailNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();

        String fromAddress = "${EMAIL_TO_LIST}";
        String toAddress = "${EMAIL_CC_LIST}";

        String subject;
        String subjectNamePart = config.getResolvedJobName();
        String appNameForSubject = jira.getApplicationName() != null ? jira.getApplicationName() : "UNKNOWN_APP";

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            subject = "CDMP Ingestion - " + subjectNamePart; // Change #6 applied & simplified from jira.getTableName
        } else if (isFileSource(config.getSourceType())) {
            subject = "utcap ingestion for feed " + subjectNamePart;
        } else {
            subject = "Data Extract from " + sor.getSorName().toUpperCase(Locale.ROOT) + " to " + appNameForSubject
                    + " - " + subjectNamePart;
        }

        emailNode.put("fromAddress", fromAddress);
        emailNode.put("toAddress", toAddress);
        emailNode.put("subject", subject);
        emailNode.put("isPriorityAlert", "Y");
        return emailNode;
    }

    private ObjectNode buildGenericIngestionNode(JsonGenerationConfig config) {
        Set<String> fields = config.getIngestionFieldsToInclude();
        if (fields.isEmpty())
            return null;

        ObjectNode ingestionItemNode = objectMapper.createObjectNode();
        if (fields.contains("ingestionId"))
            ingestionItemNode.put("ingestionId", "1");
        if (fields.contains("ingestionDesc"))
            ingestionItemNode.put("ingestionDesc", determineIngestionDescription(config)); // Change #1
        if (fields.contains("isIngestionEnabled"))
            ingestionItemNode.put("isIngestionEnabled", "Y");
        if (fields.contains("sourceType"))
            ingestionItemNode.put("sourceType", config.getResolvedIngestionSourceTypeValue());
        if (fields.contains("ConnectionHeader") && config.getResolvedIngestionConnectionHeaderValue() != null)
            ingestionItemNode.put("ConnectionHeader", config.getResolvedIngestionConnectionHeaderValue());
        if (fields.contains("delimiter") && config.getResolvedIngestionDelimiter() != null)
            ingestionItemNode.put("delimiter", config.getResolvedIngestionDelimiter());
        if (fields.contains("dataFrameName"))
            ingestionItemNode.put("dataFrameName", config.getResolvedIngestionDataFrameName());

        if (fields.contains("isCreateTargetTable"))
            ingestionItemNode.put("isCreateTargetTable", config.getIngestionIsCreateTargetTableValue());
        if (fields.contains("isCacheEnabled"))
            ingestionItemNode.put("isCacheEnabled", config.getIngestionIsCacheEnabledValue());
        if (fields.contains("isOverWritePartition"))
            ingestionItemNode.put("isOverWritePartition", config.getIngestionIsOverWritePartitionValue());
        if (fields.contains("isRowCountDisabled") && config.getIngestionIsRowCountDisabledValue() != null)
            ingestionItemNode.put("isRowCountDisabled", config.getIngestionIsRowCountDisabledValue());

        if (fields.contains("abortIfDuplicate"))
            ingestionItemNode.put("abortIfDuplicate", "N");
        if (fields.contains("abortIfEmpty"))
            ingestionItemNode.put("abortIfEmpty", "N");
        if (fields.contains("isRunEMHReport"))
            ingestionItemNode.put("isRunEMHReport", "N");

        if (fields.contains("sourceSQLQuery") && config.getResolvedIngestionSourceSQLQuery() != null)
            ingestionItemNode.put("sourceSQLQuery", config.getResolvedIngestionSourceSQLQuery());
        if (fields.contains("sourceName") && config.getResolvedIngestionSourceName() != null)
            ingestionItemNode.put("sourceName", config.getResolvedIngestionSourceName());
        if (fields.contains("dataFilePath") && config.getResolvedIngestionDataFilePath() != null)
            ingestionItemNode.put("dataFilePath", config.getResolvedIngestionDataFilePath());
        if (fields.contains("schemaFilePath") && config.getResolvedIngestionSchemaFilePath() != null)
            ingestionItemNode.put("schemaFilePath", config.getResolvedIngestionSchemaFilePath());
        if (fields.contains("stagingFilePath") && config.getResolvedIngestionStagingFilePath() != null)
            ingestionItemNode.put("stagingFilePath", config.getResolvedIngestionStagingFilePath());
        if (fields.contains("isHeaderEnabled") && config.getResolvedIngestionIsHeaderEnabledValue() != null)
            ingestionItemNode.put("isHeaderEnabled", config.getResolvedIngestionIsHeaderEnabledValue());

        if (fields.contains("targetType") && config.getTargetType() == JsonGenerationConfig.TargetType.HIVE
                && config.getSourceType() == JsonGenerationConfig.SourceType.TERADATA) {
            ingestionItemNode.put("targetType", "parquet");
        }

        if (fields.contains("additionalReadConfigs") && config.getPrebuiltIngestionAdditionalReadConfigsNode() != null)
            ingestionItemNode.set("additionalReadConfigs", config.getPrebuiltIngestionAdditionalReadConfigsNode());

        if (fields.contains("dimcChecks") && config.getPrebuiltIngestionDimcChecksNode() != null)
            ingestionItemNode.set("dimcChecks", config.getPrebuiltIngestionDimcChecksNode());

        ObjectNode finalIngestionsNode = objectMapper.createObjectNode();
        if (ingestionItemNode.size() > 0) {
            addToArray(finalIngestionsNode, "ingestion", ingestionItemNode);
            return finalIngestionsNode;
        }
        return null;
    }

    // Change #1: Ingestion description to match new job description format
    private String determineIngestionDescription(JsonGenerationConfig config) {
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        String jobNameForDesc = config.getResolvedJobName();
        String appNameForDesc = jira.getApplicationName() != null ? jira.getApplicationName() : "UNKNOWN_APP";

        return sor.getSorName().toUpperCase(Locale.ROOT) +
                JsonGenerationConfig.TABLE_TEXT + jobNameForDesc +
                JsonGenerationConfig.INGESTION_TO_TEXT + appNameForDesc;
    }

    private ObjectNode buildGenericFlowsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFlows() || config.getFlowDetailsList().isEmpty())
            return null;

        ObjectNode flowsNode = objectMapper.createObjectNode();
        ArrayNode flowArray = flowsNode.putArray("flow");

        for (JsonGenerationConfig.FlowDetailConfig flowDetail : config.getFlowDetailsList()) {
            ObjectNode flowItemNode = objectMapper.createObjectNode();
            if (flowDetail.getFlowId() != null)
                flowItemNode.put("flowId", flowDetail.getFlowId());
            if (flowDetail.getResolvedFlowDescription() != null)
                flowItemNode.put("flowDesc", flowDetail.getResolvedFlowDescription());
            flowItemNode.put("isFlowEnabled", "Y");
            if (flowDetail.getResolvedFlowQuery() != null)
                flowItemNode.put("query", flowDetail.getResolvedFlowQuery());
            if (flowDetail.getResolvedFlowRegisterName() != null)
                flowItemNode.put("registerName", flowDetail.getResolvedFlowRegisterName());

            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) {
                if (flowDetail.getSaveOutputValue() != null)
                    flowItemNode.put("saveOutput", flowDetail.getSaveOutputValue());
                if (flowDetail.getResolvedFlowDatabaseName() != null)
                    flowItemNode.put("flowDataBaseName", flowDetail.getResolvedFlowDatabaseName());
                if (flowDetail.getResolvedFlowStorageName() != null)
                    flowItemNode.put("storageName", flowDetail.getResolvedFlowStorageName());
                if (flowDetail.getResolvedFlowTargetTablePath() != null)
                    flowItemNode.put("targetTablePath", flowDetail.getResolvedFlowTargetTablePath());
                if (flowDetail.getResolvedFlowHivePartitionColumn() != null)
                    flowItemNode.put("hivePartitionColumn", flowDetail.getResolvedFlowHivePartitionColumn());
                if (flowDetail.getIsCreateFlowTargetTableValue() != null)
                    flowItemNode.put("isCreateFlowTargetTable", flowDetail.getIsCreateFlowTargetTableValue());
                if (flowDetail.getIsOverWritePartitionValue() != null)
                    flowItemNode.put("isOverWritePartition", flowDetail.getIsOverWritePartitionValue());
                if (flowDetail.getIsCacheEnabledValue() != null)
                    flowItemNode.put("isCacheEnabled", flowDetail.getIsCacheEnabledValue());
                if (flowDetail.getSendEmailValue() != null)
                    flowItemNode.put("sendEmail", flowDetail.getSendEmailValue());
                if (flowDetail.getAbortIfEmptyValue() != null)
                    flowItemNode.put("abortIfEmpty", flowDetail.getAbortIfEmptyValue());
            }
            flowArray.add(flowItemNode);
        }
        return flowsNode;
    }

    // MODIFICATION #3: Simplified skipping logic
    private ObjectNode buildGenericDataChecksNode(JsonGenerationConfig config) {
        if (!config.isIncludeDataChecks()) {
            return null;
        }

        ObjectNode dataChecksNode = objectMapper.createObjectNode();

        // Add acDCDatabase if resolved
        if (config.getResolvedDataChecksAcDcDatabase() != null) {
            dataChecksNode.put("acDCDatabase", config.getResolvedDataChecksAcDcDatabase());
        }

        // Add "check" array if there are check details
        if (!config.getCheckDetailsList().isEmpty()) {
            ArrayNode checkArray = dataChecksNode.putArray("check");
            for (JsonGenerationConfig.CheckDetailConfig detail : config.getCheckDetailsList()) {
                ObjectNode checkItemNode = objectMapper.createObjectNode();
                if (detail.getCheckId() != null)
                    checkItemNode.put("checkId", detail.getCheckId());
                if (detail.getCheckName() != null)
                    checkItemNode.put("checkName", detail.getCheckName());
                if (detail.getCheckDescription() != null)
                    checkItemNode.put("checkDesc", detail.getCheckDescription());
                if (detail.getIsCheckEnabledValue() != null)
                    checkItemNode.put("isCheckEnabled", detail.getIsCheckEnabledValue());
                if (detail.getIsUserSupplyQueryValue() != null)
                    checkItemNode.put("isUserSupplyQuery", detail.getIsUserSupplyQueryValue());
                if (detail.getReadSourceQueryFromPathValue() != null)
                    checkItemNode.put("readSourceQueryFromPath", detail.getReadSourceQueryFromPathValue());
                if (detail.getResolvedCheckSourceQuery() != null)
                    checkItemNode.put("sourceQuery", detail.getResolvedCheckSourceQuery());
                if (detail.getResolvedCheckTargetQuery() != null)
                    checkItemNode.put("targetQuery", detail.getResolvedCheckTargetQuery());
                if (detail.getReadTargetQueryFromPathValue() != null)
                    checkItemNode.put("readTargetQueryFromPath", detail.getReadTargetQueryFromPathValue());
                if (detail.getIsFailJobOnDataCheckErrorValue() != null)
                    checkItemNode.put("isFailJobOnDataCheckError", detail.getIsFailJobOnDataCheckErrorValue()); // Change
                                                                                                                // #2
                checkArray.add(checkItemNode);
            }
        }

        // Add "preChecks" node if it's prebuilt and has content
        ObjectNode preChecksNodeFromConfig = config.getPrebuiltDataChecksPreChecksNode();
        if (preChecksNodeFromConfig != null && preChecksNodeFromConfig.has("preCheck") &&
                preChecksNodeFromConfig.get("preCheck").size() > 0) {
            dataChecksNode.set("preChecks", preChecksNodeFromConfig);
        }

        // If the node is empty after all attempts, return null
        if (dataChecksNode.isEmpty()) {
            return null;
        }

        // If the node ONLY contains the default acDCDatabase, return null
        if (dataChecksNode.size() == 1 && dataChecksNode.has("acDCDatabase") &&
                "${AUDIT_DATABASE}".equals(dataChecksNode.get("acDCDatabase").asText())) {
            return null;
        }

        return dataChecksNode;
    }

    private ObjectNode buildGenericFileExtractsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFileExtracts())
            return null;
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = fileExtractsNode.putArray("fileExtract");
        ObjectNode fileExtractItemNode = objectMapper.createObjectNode();
        fileExtractItemNode.put("fileExtractId", "1");
        fileExtractItemNode.put("isFileExtractEnabled", "${IS_FILE_EXTRACT_ENABLED}");

        String sourceDescriptionPart = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "hive data"
                : config.getResolvedJobName();
        fileExtractItemNode.put("fileDescription", "Extracting " + sourceDescriptionPart + " to avro file for CDMP");
        fileExtractItemNode.put("subjectArea",
                config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "cdmp using FileExtract" : "CDMP");

        fileExtractItemNode.put("zone", JsonGenerationConfig.RAW_ZONE);
        fileExtractItemNode.put("system", "cdmp");
        fileExtractItemNode.put("fileType", "avro");

        String flowRegisterNameForExtract = config.getFlowDetailsList().isEmpty()
                || config.getFlowDetailsList().get(0).getResolvedFlowRegisterName() == null
                        ? config.getResolvedJobName() + "_extract_df"
                        : config.getFlowDetailsList().get(0).getResolvedFlowRegisterName();
        fileExtractItemNode.put("extractQuery", "select * from " + flowRegisterNameForExtract);

        fileExtractItemNode.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        fileExtractItemNode.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        ObjectNode cdmpParamsNode = objectMapper.createObjectNode();
        cdmpParamsNode.put("outputFormat", "avro");
        String cdmpFileNamePrefix = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "UTCAPT_" : "";
        cdmpParamsNode.put("cdmpFileName", cdmpFileNamePrefix + config.getResolvedJobName() + "_cdmp_extract");

        cdmpParamsNode.put("cdmpPatternMode", "daily");
        cdmpParamsNode.put("isCustomPartitionRequired", "true");
        cdmpParamsNode.put("desiredPartFileSize", "${PART_FILE_SIZE}");
        fileExtractItemNode.set("cdmpAdditionalParams", cdmpParamsNode);
        fileExtractArray.add(fileExtractItemNode);
        return fileExtractsNode;
    }

    // Change #5 & #9: Parameterize values for specific DIMC checks
    private static synchronized void populateDimcChecksMasterList() {
        if (!DIMC_CHECKS_MASTER_LIST.isEmpty())
            return;
        LOG.info("Populating DIMC Checks Master List (with latest parameterizations)...");

        Map<String, Object> check0 = new HashMap<>();
        check0.put("dimcCheckname", "duplicateFileLoadChecks");
        check0.put("isDimcCheckEnabled", "${DUPLICATE_FILELOAD_CHECK_ENABLED}");
        check0.put("isFailJobOnError", "${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check0));
        Map<String, Object> check1 = new HashMap<>();
        check1.put("dimcCheckname", "dataVolumeConsistencyCheck");
        check1.put("isDimcCheckEnabled", "${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}");
        check1.put("upperThreshold", "${UPPER_THRESHOLD}");
        check1.put("lowerThreshold", "${LOWER_THRESHOLD}");
        check1.put("numPastDays", "${NUM_PAST_DAYS}");
        check1.put("isFailJobOnError", "${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check1));
        Map<String, Object> check2 = new HashMap<>();
        check2.put("dimcCheckname", "missingFileCheck");
        check2.put("isDimcCheckEnabled", "${MISSING_FILE_CHECK_ENABLED}");
        check2.put("isFailJobOnError", "${MISSING_FILE_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check2));
        Map<String, Object> check3 = new HashMap<>();
        check3.put("dimcCheckname", "deliveryCheck");
        check3.put("isDimcCheckEnabled", "${DELIVERY_CHECK_ENABLED}");
        check3.put("SLA", "${DELIVERY_CHECK_SLA}");
        check3.put("isFailJobOnError", "${DELIVERY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check3));
        Map<String, Object> check4 = new HashMap<>();
        check4.put("dimcCheckname", "missingDbObjectCheck");
        check4.put("isDimcCheckEnabled", "${MISSING_DB_OBJECT_CHECK_ENABLED}");
        check4.put("isFailJobOnError", "${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check4));
        Map<String, Object> check5 = new HashMap<>();
        check5.put("dimcCheckname", "emptySourceCheck");
        check5.put("isDimcCheckEnabled", "${EMPTY_SOURCE_CHECK_ENABLED}");
        check5.put("isFailJobOnError", "${EMPTY_SOURCE_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check5));

        Map<String, Object> check6 = new HashMap<>(); // dataVolumeCompletenessCheck
        check6.put("dimcCheckname", "dataVolumeCompletenessCheck");
        check6.put("controlFileRowFilterByIndex", "${DVCC_CTL_FILE_ROW_FILTER_SPEC}");
        check6.put("controlFileFieldValueByIndex", "${DVCC_CTL_FILE_FIELD_VALUE_SPEC}");
        check6.put("isDimcCheckEnabled", "${DVCC_ENABLED}");
        check6.put("isFailJobOnError", "${DVCC_FAIL_ON_ERROR}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check6));

        Map<String, Object> check7 = new HashMap<>(); // fileDateCheck - Change #9
        check7.put("dimcCheckname", "fileDateCheck");
        check7.put("isDimcCheckEnabled", "${FILE_DATE_CHECK_ENABLED}");
        check7.put("isFailJobOnError", "${FILE_DATE_CHECK_FAILURE_FLAG}");
        check7.put("fileDateFromFileNamePattern", "${FDC_FILENAME_PATTERN}"); // Parameterized
        check7.put("fileDateFromFileNameFormat", "${FDC_FILENAME_DATE_FORMAT}"); // Parameterized
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check7));

        Map<String, Object> check8 = new HashMap<>(); // businessDateCheck
        check8.put("dimcCheckname", "businessDateCheck");
        check8.put("businessDateFromFileNamePattern", "${BDC_FILENAME_PATTERN}");
        check8.put("businessDateFromFileNameFormat", "${BDC_FILENAME_DATE_FORMAT}");
        check8.put("upperThresholdInDays", "${BDC_UPPER_THRESHOLD_DAYS}");
        check8.put("lowerThresholdInDays", "${BDC_LOWER_THRESHOLD_DAYS}");
        check8.put("controlFileRowFilterByIndex", "${BDC_CTL_FILE_ROW_FILTER_SPEC}");
        check8.put("controlFileFieldValueByIndex", "${BDC_CTL_FILE_FIELD_VALUE_SPEC}");
        check8.put("isDimcCheckEnabled", "${BDC_ENABLED}");
        check8.put("isFailJobOnError", "${BDC_FAIL_ON_ERROR}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check8));

        LOG.info("DIMC Checks Master List populated with {} items.", DIMC_CHECKS_MASTER_LIST.size());
    }

    private JsonGenerationConfig.SourceType parseSourceType(String sorTypeStr) {
        if (sorTypeStr == null)
            return JsonGenerationConfig.SourceType.UNKNOWN;
        String typeLower = sorTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "teradata":
                return JsonGenerationConfig.SourceType.TERADATA;
            case "file-fixedwidth":
            case "fixedwidthfile":
                return JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
            case "file-pipe":
                return JsonGenerationConfig.SourceType.FILE_PIPE;
            case "file-comma":
                return JsonGenerationConfig.SourceType.FILE_COMMA;
            case "mssql":
                return JsonGenerationConfig.SourceType.MSSQL;
            case "oracle":
                return JsonGenerationConfig.SourceType.ORACLE;
            case "hive":
                return JsonGenerationConfig.SourceType.HIVE;
            case "file":
                LOG.warn("Generic 'file' sourceType string found. Defaulting to Comma. Input: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.FILE_COMMA;
            default:
                LOG.warn("Unrecognized Source Type string: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.UNKNOWN;
        }
    }

    private JsonGenerationConfig.TargetType parseTargetType(String targetTypeStr) {
        if (targetTypeStr == null)
            return JsonGenerationConfig.TargetType.UNKNOWN;
        String typeLower = targetTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "hive":
                return JsonGenerationConfig.TargetType.HIVE;
            case "gcp":
                return JsonGenerationConfig.TargetType.GCP;
            default:
                LOG.warn("Unrecognized Target Type string: {}", targetTypeStr);
                return JsonGenerationConfig.TargetType.UNKNOWN;
        }
    }

    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) {
        if (item == null || item.size() == 0)
            return;
        ArrayNode array = parent.has(arrayName) && parent.get(arrayName).isArray() ? (ArrayNode) parent.get(arrayName)
                : parent.putArray(arrayName);
        array.add(item);
    }

    public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) {
        Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository
                .findById(requirementId);
        if (existingConnJsonRequirement.isPresent()) {
            return existingConnJsonRequirement.get();
        }
        ObjectNode rootNode = objectMapper.createObjectNode();
        ObjectNode application = objectMapper.createObjectNode();
        ObjectNode email = objectMapper.createObjectNode();
        String sorName;
        try {
            String baseRequirementId = requirementId.contains("_") ? requirementId.split("_")[0] : requirementId;
            List<Requirement> reqs = requirementRepository.findByRequirementId(baseRequirementId);
            if (reqs.isEmpty())
                throw new ConfigurationException("Requirement not found for: " + baseRequirementId);
            sorName = reqs.get(0).getSorName();
        } catch (Exception e) { // Consider making this catch more specific
            LOG.error("Failed to get SorName for ConnJson: {}. Error: {}", requirementId, e.getMessage(), e);
            sorName = "UNKNOWN_SOR";
        }
        String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName + " with Job Name $jobName has completed. " +
                "</p><p> SourceDate =$sourceDate , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
        email.put("body", emailBody);
        ObjectNode job = objectMapper.createObjectNode();
        job.set("Email", email);
        application.set("job", job);
        rootNode.set("application", application);
        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {
        });
        ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
        return connJsonRequirementRepository.save(connJsonRequirement);
    }
}
