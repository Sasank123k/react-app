package com.wellsfargo.utcap.config;

import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.model.*;
import lombok.Getter;
import lombok.ToString;

import java.util.List;
import java.util.Map; // Keep for Map<String, Object> if used in future complex prebuilt parts
import java.util.Objects;
import java.util.Set;

/**
 * Configuration object holding all necessary resolved data, pre-built structures,
 * and flags to drive the DCI JSON artifact generation for a specific
 * source-target combination.
 */
@Getter
@ToString
public class JsonGenerationConfig {

    public enum SourceType {
        TERADATA,
        FILE_FIXEDWIDTH,
        FILE_PIPE,
        FILE_COMMA,
        MSSQL,
        ORACLE,
        HIVE,
        UNKNOWN
    }

    public enum TargetType {
        HIVE,
        GCP,
        UNKNOWN
    }

    // --- Core Input Data Objects ---
    private final JiraStoryIntake jiraStoryIntake;
    private final Sor sor;
    private final ApplicationAuthorizer applicationAuthorizer;
    private final SQLScript sqlScript;

    // --- Determined Types & Overall Structural Flags ---
    private final SourceType sourceType;
    private final TargetType targetType;
    private final boolean includeIngestions;
    private final boolean includeFlows;
    private final boolean includeDataChecks;
    private final boolean includeFileExtracts;

    // --- Field Inclusion Sets ---
    private final Set<String> jobLevelFieldsToInclude;
    private final Set<String> ingestionFieldsToInclude;

    // --- Resolved Values / Pre-built Structures for INGESTION ---
    private final String resolvedIngestionSourceTypeValue;
    private final String resolvedIngestionConnectionHeaderValue;
    private final String resolvedIngestionDataFrameName;
    private final String resolvedIngestionSourceSQLQuery;
    private final String resolvedIngestionSourceName; // For Oracle
    private final String resolvedIngestionDataFilePath;
    private final String resolvedIngestionSchemaFilePath;
    private final String resolvedIngestionStagingFilePath;
    private final String resolvedIngestionDelimiter;
    private final String resolvedIngestionIsHeaderEnabled; // "Y" for all files currently
    private final String resolvedIngestionSaveOutputFlag;
    private final String resolvedIngestionTargetPartition;
    private final String ingestionIsCreateTargetTableValue; // Standardized to "Y"
    private final String ingestionIsCacheEnabledValue;      // Standardized to "Y"
    private final String ingestionIsOverWritePartitionValue; // Standardized to "Y"
    private final ObjectNode prebuiltIngestionAdditionalReadConfigsNode;
    private final boolean includeDimcChecksInIngestion;
    private final ObjectNode prebuiltIngestionDimcChecksNode;

    // --- Resolved Values / Pre-built Structures for FLOWS ---
    private final List<FlowDetailConfig> flowDetailsList;

    // --- Resolved Values / Pre-built Structures for DATA CHECKS ---
    private final String resolvedDataChecksAcDcDatabase;
    private final List<CheckDetailConfig> checkDetailsList;
    private final ObjectNode prebuiltDataChecksPreChecksNode;

    @Getter
    @ToString
    public static class FlowDetailConfig {
        private final String flowId;
        private final String resolvedFlowDescription;
        private final String resolvedFlowQuery;
        private final String resolvedFlowRegisterName;
        private final String resolvedFlowDatabaseName;
        private final String resolvedFlowStorageName;
        private final String resolvedFlowTargetTablePath;
        private final String resolvedFlowHivePartitionColumn;
        private final String isCreateFlowTargetTableValue;
        private final String isOverWritePartitionValue;
        private final String isCacheEnabledValue;
        private final String sendEmailValue;
        private final String abortIfEmptyValue;
        private final String saveOutputValue;

        public FlowDetailConfig(String flowId, String resolvedFlowDescription, String resolvedFlowQuery,
                                String resolvedFlowRegisterName, String resolvedFlowDatabaseName,
                                String resolvedFlowStorageName, String resolvedFlowTargetTablePath,
                                String resolvedFlowHivePartitionColumn, String isCreateFlowTargetTableValue,
                                String isOverWritePartitionValue, String isCacheEnabledValue,
                                String sendEmailValue, String abortIfEmptyValue, String saveOutputValue) {
            this.flowId = flowId;
            this.resolvedFlowDescription = resolvedFlowDescription;
            this.resolvedFlowQuery = resolvedFlowQuery;
            this.resolvedFlowRegisterName = resolvedFlowRegisterName;
            this.resolvedFlowDatabaseName = resolvedFlowDatabaseName;
            this.resolvedFlowStorageName = resolvedFlowStorageName;
            this.resolvedFlowTargetTablePath = resolvedFlowTargetTablePath;
            this.resolvedFlowHivePartitionColumn = resolvedFlowHivePartitionColumn;
            this.isCreateFlowTargetTableValue = isCreateFlowTargetTableValue;
            this.isOverWritePartitionValue = isOverWritePartitionValue;
            this.isCacheEnabledValue = isCacheEnabledValue;
            this.sendEmailValue = sendEmailValue;
            this.abortIfEmptyValue = abortIfEmptyValue;
            this.saveOutputValue = saveOutputValue;
        }
    }

    @Getter
    @ToString
    public static class CheckDetailConfig {
        private final String checkId;
        private final String checkName;
        private final String checkDescription;
        private final String resolvedCheckSourceQuery;
        private final String resolvedCheckTargetQuery;
        private final String isCheckEnabledValue;
        private final String isUserSupplyQueryValue;
        private final String readSourceQueryFromPathValue;
        private final String readTargetQueryFromPathValue;
        private final String isFailJobOnDataCheckErrorValue;

        public CheckDetailConfig(String checkId, String checkName, String checkDescription,
                                 String resolvedCheckSourceQuery, String resolvedCheckTargetQuery,
                                 String isCheckEnabledValue, String isUserSupplyQueryValue,
                                 String readSourceQueryFromPathValue, String readTargetQueryFromPathValue,
                                 String isFailJobOnDataCheckErrorValue) {
            this.checkId = checkId;
            this.checkName = checkName;
            this.checkDescription = checkDescription;
            this.resolvedCheckSourceQuery = resolvedCheckSourceQuery;
            this.resolvedCheckTargetQuery = resolvedCheckTargetQuery;
            this.isCheckEnabledValue = isCheckEnabledValue;
            this.isUserSupplyQueryValue = isUserSupplyQueryValue;
            this.readSourceQueryFromPathValue = readSourceQueryFromPathValue;
            this.readTargetQueryFromPathValue = readTargetQueryFromPathValue;
            this.isFailJobOnDataCheckErrorValue = isFailJobOnDataCheckErrorValue;
        }
    }

    public JsonGenerationConfig(
            JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer, SQLScript sqlScript,
            SourceType sourceType, TargetType targetType,
            boolean includeIngestions, boolean includeFlows, boolean includeDataChecks, boolean includeFileExtracts,
            Set<String> jobLevelFieldsToInclude, Set<String> ingestionFieldsToInclude,
            String resolvedIngestionSourceTypeValue, String resolvedIngestionConnectionHeaderValue,
            String resolvedIngestionDataFrameName, String resolvedIngestionSourceSQLQuery,
            String resolvedIngestionSourceName, String resolvedIngestionDataFilePath,
            String resolvedIngestionSchemaFilePath, String resolvedIngestionStagingFilePath,
            String resolvedIngestionDelimiter, String resolvedIngestionIsHeaderEnabled,
            String resolvedIngestionSaveOutputFlag, String resolvedIngestionTargetPartition,
            String ingestionIsCreateTargetTableValue, String ingestionIsCacheEnabledValue, String ingestionIsOverWritePartitionValue,
            ObjectNode prebuiltIngestionAdditionalReadConfigsNode,
            boolean includeDimcChecksInIngestion, ObjectNode prebuiltIngestionDimcChecksNode,
            List<FlowDetailConfig> flowDetailsList,
            String resolvedDataChecksAcDcDatabase, List<CheckDetailConfig> checkDetailsList,
            ObjectNode prebuiltDataChecksPreChecksNode
    ) {
        this.jiraStoryIntake = Objects.requireNonNull(jiraStoryIntake, "JiraStoryIntake cannot be null");
        this.sor = Objects.requireNonNull(sor, "Sor cannot be null");
        this.applicationAuthorizer = Objects.requireNonNull(applicationAuthorizer, "ApplicationAuthorizer cannot be null");
        this.sqlScript = Objects.requireNonNull(sqlScript, "SQLScript cannot be null");
        this.sourceType = Objects.requireNonNull(sourceType, "SourceType cannot be null");
        this.targetType = Objects.requireNonNull(targetType, "TargetType cannot be null");
        this.includeIngestions = includeIngestions;
        this.includeFlows = includeFlows;
        this.includeDataChecks = includeDataChecks;
        this.includeFileExtracts = includeFileExtracts;
        this.jobLevelFieldsToInclude = Objects.requireNonNull(jobLevelFieldsToInclude, "jobLevelFieldsToInclude cannot be null");
        this.ingestionFieldsToInclude = Objects.requireNonNull(ingestionFieldsToInclude, "ingestionFieldsToInclude cannot be null");
        this.resolvedIngestionSourceTypeValue = resolvedIngestionSourceTypeValue;
        this.resolvedIngestionConnectionHeaderValue = resolvedIngestionConnectionHeaderValue;
        this.resolvedIngestionDataFrameName = resolvedIngestionDataFrameName;
        this.resolvedIngestionSourceSQLQuery = resolvedIngestionSourceSQLQuery;
        this.resolvedIngestionSourceName = resolvedIngestionSourceName;
        this.resolvedIngestionDataFilePath = resolvedIngestionDataFilePath;
        this.resolvedIngestionSchemaFilePath = resolvedIngestionSchemaFilePath;
        this.resolvedIngestionStagingFilePath = resolvedIngestionStagingFilePath;
        this.resolvedIngestionDelimiter = resolvedIngestionDelimiter;
        this.resolvedIngestionIsHeaderEnabled = resolvedIngestionIsHeaderEnabled;
        this.resolvedIngestionSaveOutputFlag = resolvedIngestionSaveOutputFlag;
        this.resolvedIngestionTargetPartition = resolvedIngestionTargetPartition;
        this.ingestionIsCreateTargetTableValue = ingestionIsCreateTargetTableValue;
        this.ingestionIsCacheEnabledValue = ingestionIsCacheEnabledValue;
        this.ingestionIsOverWritePartitionValue = ingestionIsOverWritePartitionValue;
        this.prebuiltIngestionAdditionalReadConfigsNode = prebuiltIngestionAdditionalReadConfigsNode;
        this.includeDimcChecksInIngestion = includeDimcChecksInIngestion;
        this.prebuiltIngestionDimcChecksNode = prebuiltIngestionDimcChecksNode;
        this.flowDetailsList = Objects.requireNonNull(flowDetailsList, "flowDetailsList cannot be null");
        this.resolvedDataChecksAcDcDatabase = resolvedDataChecksAcDcDatabase;
        this.checkDetailsList = Objects.requireNonNull(checkDetailsList, "checkDetailsList cannot be null");
        this.prebuiltDataChecksPreChecksNode = prebuiltDataChecksPreChecksNode;
    }
}
























package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.config.JsonGenerationConfig;
import com.wellsfargo.utcap.exception.ConfigurationException;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;
import java.util.stream.Stream;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(JsonRequirementService.class);
    private static final String SANITIZED_ZONE = "sanitized";
    private static final String RAW_ZONE = "raw";
    private static final String TABLE_TEXT = " Table ";
    private static final String INGESTION_TO_TEXT = " ingestion to ";
    // Corrected type for DIMC_CHECKS_MASTER_LIST
    private static final List<Map<String, Object>> DIMC_CHECKS_MASTER_LIST = new ArrayList<>();
    private static final String DIMC_CHECKS_ENABLED_PLACEHOLDER = "${DIMCCHECK_ENABLED}";


    private final ApplicationAuthRepository applicationAuthRepository;
    private final RequirementRepository requirementRepository;
    private final JsonRequirementRepository jsonRequirementRepository;
    private final JiraStoryIntakeRepository jiraStoryIntakeRepository;
    private final SorRepository sorRepository;
    private final SQLScriptRepository sqlScriptRepository;
    private final ConnJsonRequirementRepository connJsonRequirementRepository;
    private final ObjectMapper objectMapper;

    @Autowired
    public JsonRequirementService(ApplicationAuthRepository applicationAuthRepository,
                                  RequirementRepository requirementRepository,
                                  JsonRequirementRepository jsonRequirementRepository,
                                  JiraStoryIntakeRepository jiraStoryIntakeRepository,
                                  SorRepository sorRepository,
                                  SQLScriptRepository sqlScriptRepository,
                                  ConnJsonRequirementRepository connJsonRequirementRepository,
                                  ObjectMapper objectMapper) {
        this.applicationAuthRepository = applicationAuthRepository;
        this.requirementRepository = requirementRepository;
        this.jsonRequirementRepository = jsonRequirementRepository;
        this.jiraStoryIntakeRepository = jiraStoryIntakeRepository;
        this.sorRepository = sorRepository;
        this.sqlScriptRepository = sqlScriptRepository;
        this.connJsonRequirementRepository = connJsonRequirementRepository;
        this.objectMapper = objectMapper;
        populateDimcChecksMasterList();
    }

    public JsonRequirement saveJson(String requirementId, Object newJson) {
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId)
                .orElse(new JsonRequirement(requirementId, null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public JsonRequirement getOrCreateJsonRequirement(String requirementId) throws JsonProcessingException, ConfigurationException {
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);
        if (existingJsonRequirement.isPresent()) {
            LOG.debug("Returning existing JSON requirement for ID: {}", requirementId);
            return existingJsonRequirement.get();
        }

        LOG.info("Generating new JSON requirement for ID: {}", requirementId);
        JsonGenerationConfig config = buildJsonGenerationConfig(requirementId);

        ObjectNode rootNode = objectMapper.createObjectNode();
        rootNode.set("application", createApplicationNode(config));

        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);
        return jsonRequirementRepository.save(jsonRequirement);
    }

    private JsonGenerationConfig buildJsonGenerationConfig(String requirementId) throws ConfigurationException, JsonProcessingException {
        String jiraStoryId = requirementId.split("_")[0];

        JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
        Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("Sor not found for name: " + jira.getApplicationSorName()));
        ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
        SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

        JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
        JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

        if (sourceTypeEnum == JsonGenerationConfig.SourceType.UNKNOWN)
            throw new ConfigurationException("Unknown Source Type: " + jira.getSorType());
        if (targetTypeEnum == JsonGenerationConfig.TargetType.UNKNOWN)
            throw new ConfigurationException("Unknown Target Type: " + jira.getTargetType());

        boolean includeIngestions = (sourceTypeEnum != JsonGenerationConfig.SourceType.HIVE);
        boolean includeFlows = true;
        boolean includeDataChecks = true;
        boolean includeFileExtracts = (targetTypeEnum == JsonGenerationConfig.TargetType.GCP);

        Set<String> jobLevelFieldsToInclude = determineJobLevelFieldsToInclude(targetTypeEnum);
        Set<String> ingestionFieldsToInclude = determineIngestionFieldsToInclude(sourceTypeEnum, includeIngestions);

        String resolvedIngestionSourceTypeValue = includeIngestions ? resolveIngestionSourceTypeValue(sourceTypeEnum) : null;
        String resolvedIngestionConnectionHeaderValue = includeIngestions ? resolveIngestionConnectionHeaderValue(sourceTypeEnum) : null;
        String resolvedIngestionDataFrameName = includeIngestions ? resolveIngestionDataFrameName(sourceTypeEnum, jira.getTableName()) : null;
        
        String resolvedIngestionSourceSQLQuery = null;
        if (includeIngestions && isDatabaseSource(sourceTypeEnum) && sqlScript != null) {
             resolvedIngestionSourceSQLQuery = sqlScript.getSrcTableQuery(); // Base query
             // Specific enrichments or alternative handling if needed (e.g. Oracle's sourceName usage)
        }
        String resolvedIngestionSourceName = (includeIngestions && sourceTypeEnum == JsonGenerationConfig.SourceType.ORACLE) ?
                "${BDDMCBD_SRC_SCHEMA}." + jira.getTableName() : null; // Example from Oracle JSON

        String resolvedIngestionDataFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ? "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}" : null;
        String resolvedIngestionSchemaFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ?
                "${DCI_CONFIG_DIR_PATH}/config/" + sor.getSorName().toLowerCase() + "/${FEED_NAME}/metadata/" + jira.getTableName().toLowerCase() + ".schema" : null;
        String resolvedIngestionStagingFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ?
                determineStagingFilePath(sourceTypeEnum, sor, jira) : null;
        String resolvedIngestionDelimiter = includeIngestions ? resolveIngestionDelimiter(sourceTypeEnum) : null;
        String resolvedIngestionIsHeaderEnabled = (includeIngestions && isFileSource(sourceTypeEnum)) ? "Y" : null; // Standardized to "Y" for all files

        boolean isPipeOrComma = sourceTypeEnum == JsonGenerationConfig.SourceType.FILE_PIPE || sourceTypeEnum == JsonGenerationConfig.SourceType.FILE_COMMA;
        String resolvedIngestionSaveOutputFlag = (includeIngestions && isPipeOrComma) ? "Y" : null;
        String resolvedIngestionTargetPartition = (includeIngestions && isPipeOrComma) ? "utcap_business_effective_date" : null;

        String ingestionIsCreateTargetTableValue = includeIngestions ? "Y" : null;
        String ingestionIsCacheEnabledValue = includeIngestions ? "Y" : null;
        String ingestionIsOverWritePartitionValue = includeIngestions ? "Y" : null;

        ObjectNode prebuiltIngestionAdditionalReadConfigsNode = includeIngestions ?
                buildPrebuiltIngestionAdditionalReadConfigs(sourceTypeEnum) : null;
        
        boolean includeDimcChecksInIngestion = includeIngestions &&
                (sourceTypeEnum == JsonGenerationConfig.SourceType.TERADATA ||
                 isFileSource(sourceTypeEnum) ||
                 sourceTypeEnum == JsonGenerationConfig.SourceType.HIVE);
        
        List<Integer> chosenDimcCheckIndices = Arrays.asList(0, 1, 2, 3, 4, 5); // Hardcoded for now
        ObjectNode prebuiltIngestionDimcChecksNode = includeDimcChecksInIngestion ?
                buildPrebuiltDimcChecksNode(chosenDimcCheckIndices) : null;

        List<JsonGenerationConfig.FlowDetailConfig> flowDetailsList = includeFlows ?
                prepareFlowDetails(sourceTypeEnum, targetTypeEnum, jira, sor, auth, sqlScript, resolvedIngestionDataFrameName) : Collections.emptyList();
        
        List<JsonGenerationConfig.CheckDetailConfig> checkDetailsList = includeDataChecks ?
                prepareCheckDetails(sourceTypeEnum, targetTypeEnum, jira, auth, flowDetailsList, resolvedIngestionDataFrameName) : Collections.emptyList();
        
        String resolvedDataChecksAcDcDatabase = includeDataChecks ?
                determineDataChecksAcDcDatabase(sourceTypeEnum, targetTypeEnum, auth, sor, !checkDetailsList.isEmpty()) : null;
        ObjectNode prebuiltDataChecksPreChecksNode = includeDataChecks ?
                buildPrebuiltDataChecksPreChecksNode(sourceTypeEnum, targetTypeEnum) : null;

        return new JsonGenerationConfig(
                jira, sor, auth, sqlScript, sourceTypeEnum, targetTypeEnum,
                includeIngestions, includeFlows, includeDataChecks, includeFileExtracts,
                jobLevelFieldsToInclude, ingestionFieldsToInclude,
                resolvedIngestionSourceTypeValue, resolvedIngestionConnectionHeaderValue,
                resolvedIngestionDataFrameName, resolvedIngestionSourceSQLQuery,
                resolvedIngestionSourceName, resolvedIngestionDataFilePath,
                resolvedIngestionSchemaFilePath, resolvedIngestionStagingFilePath,
                resolvedIngestionDelimiter, resolvedIngestionIsHeaderEnabled,
                resolvedIngestionSaveOutputFlag, resolvedIngestionTargetPartition,
                ingestionIsCreateTargetTableValue, ingestionIsCacheEnabledValue, ingestionIsOverWritePartitionValue,
                prebuiltIngestionAdditionalReadConfigsNode,
                includeDimcChecksInIngestion, prebuiltIngestionDimcChecksNode,
                flowDetailsList,
                resolvedDataChecksAcDcDatabase, checkDetailsList, prebuiltDataChecksPreChecksNode
        );
    }

    // --- Helper methods for buildJsonGenerationConfig ---
    private boolean isDatabaseSource(JsonGenerationConfig.SourceType sourceType) {
        return EnumSet.of(JsonGenerationConfig.SourceType.TERADATA,
                          JsonGenerationConfig.SourceType.MSSQL,
                          JsonGenerationConfig.SourceType.ORACLE,
                          JsonGenerationConfig.SourceType.HIVE).contains(sourceType);
    }

    private boolean isFileSource(JsonGenerationConfig.SourceType sourceType) {
         return EnumSet.of(JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH,
                           JsonGenerationConfig.SourceType.FILE_PIPE,
                           JsonGenerationConfig.SourceType.FILE_COMMA).contains(sourceType);
    }

    private Set<String> determineJobLevelFieldsToInclude(JsonGenerationConfig.TargetType targetType) {
        if (targetType != JsonGenerationConfig.TargetType.GCP) {
            return new HashSet<>(Arrays.asList("app", "appRemedyId", "sorRemedyId", "queryGroup", "zone", "sor"));
        }
        return Collections.emptySet();
    }
    
    private Set<String> determineIngestionFieldsToInclude(JsonGenerationConfig.SourceType sourceType, boolean includeIngestionsFlag) {
        if (!includeIngestionsFlag) return Collections.emptySet();

        Set<String> fields = new HashSet<>(Arrays.asList(
            "ingestionId", "ingestionDesc", "isIngestionEnabled", "dataFrameName",
            "isCreateTargetTable", "isCacheEnabled", "isOverWritePartition",
            "abortIfDuplicate", "abortIfEmpty", "isRunEMHReport", "sourceType"
            // "targetType" (e.g. parquet) is added if target is Hive by generic builder
        ));

        if (isDatabaseSource(sourceType)) {
            fields.add("ConnectionHeader");
            fields.add("sourceSQLQuery");
            if (sourceType == JsonGenerationConfig.SourceType.ORACLE) {
                fields.add("sourceName");
            }
        } else if (isFileSource(sourceType)) {
            fields.addAll(Arrays.asList("isHeaderEnabled", "dataFilePath", "schemaFilePath", "stagingFilePath"));
            if (sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) {
                // No additionalReadConfigs, no delimiter for FW
            } else { // Pipe or Comma
                fields.add("delimiter");
                fields.add("additionalReadConfigs");
                fields.add("saveOutput");
                fields.add("targetPartition");
            }
        }
        
        if (sourceType == JsonGenerationConfig.SourceType.TERADATA ||
            isFileSource(sourceType) ||
            sourceType == JsonGenerationConfig.SourceType.HIVE) { // Assuming Hive as source can have DIMC
            fields.add("dimcChecks");
        }
        return fields;
    }

    private String resolveIngestionSourceTypeValue(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA: return "teradb";
            case FILE_FIXEDWIDTH: return "fixedwidthFile";
            case FILE_PIPE: case FILE_COMMA: return "file";
            case MSSQL: return "jdbc";
            case ORACLE: return "oracle";
            case HIVE: return "hive";
            default: return "unknown";
        }
    }

    private String resolveIngestionConnectionHeaderValue(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA: return "teradbConn";
            case MSSQL: return "jdbcConn";
            case ORACLE: return "oracleConn";
            case HIVE: return "hiveConn"; // Assuming a connection header for Hive as source
            default: return null;
        }
    }

    private String resolveIngestionDataFrameName(JsonGenerationConfig.SourceType sourceType, String tableName) {
        String safeTableName = tableName.toLowerCase(Locale.ROOT);
        if (isDatabaseSource(sourceType) || sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) {
            return safeTableName + "_raw_df";
        } else { // Pipe, Comma
            return safeTableName + "_df";
        }
    }
    
    private String determineStagingFilePath(JsonGenerationConfig.SourceType sourceType, Sor sor, JiraStoryIntake jira) {
        if (sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "Hemi".equalsIgnoreCase(sor.getSorName()) && "demog_exp_e3_load".equalsIgnoreCase(jira.getTableName())) {
            return "/datalake/utcap/hemi/sanitized";
        }
        return "${EXP_EDL_PATH}";
    }

    private String resolveIngestionDelimiter(JsonGenerationConfig.SourceType sourceType) {
        if (sourceType == JsonGenerationConfig.SourceType.FILE_PIPE) return "|";
        if (sourceType == JsonGenerationConfig.SourceType.FILE_COMMA) return ",";
        return null;
    }

    private ObjectNode buildPrebuiltIngestionAdditionalReadConfigs(JsonGenerationConfig.SourceType sourceType) {
        if (sourceType == JsonGenerationConfig.SourceType.FILE_PIPE || sourceType == JsonGenerationConfig.SourceType.FILE_COMMA) {
            ObjectNode arNode = objectMapper.createObjectNode();
            arNode.put("quote", "\"");
            arNode.put("escape", "\"");
            return arNode;
        }
        return null; // FixedWidth has no additionalReadConfigs per user
    }

    private ObjectNode buildPrebuiltDimcChecksNode(List<Integer> chosenIndices) throws JsonProcessingException {
        List<Map<String, Object>> selectedChecks = getChosenDimcChecksList(chosenIndices);
        ObjectNode dimcChecksWrapper = objectMapper.createObjectNode();
        dimcChecksWrapper.put("isDimcChecksEnabled", DIMC_CHECKS_ENABLED_PLACEHOLDER); // Standardized placeholder
        ArrayNode dimcCheckArray = objectMapper.valueToTree(selectedChecks);
        dimcChecksWrapper.set("dimcCheck", dimcCheckArray);
        return dimcChecksWrapper;
    }
    
    private List<JsonGenerationConfig.FlowDetailConfig> prepareFlowDetails(
        JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
        JiraStoryIntake jira, Sor sor, ApplicationAuthorizer auth, SQLScript sqlScript, String ingestionDataFrameNameIfAny) {

        List<JsonGenerationConfig.FlowDetailConfig> flowConfigs = new ArrayList<>();
        int numberOfFlows = (sourceType == JsonGenerationConfig.SourceType.MSSQL || sourceType == JsonGenerationConfig.SourceType.ORACLE) ? 2 : 1;

        for (int i = 0; i < numberOfFlows; i++) {
            String flowId = String.valueOf(i + 1);
            boolean isFirstFlow = (i == 0);
            
            String flowZone = SANITIZED_ZONE; // Default for single flows or second flow of DB
            if (numberOfFlows == 2 && isFirstFlow) {
                flowZone = RAW_ZONE; // First flow for MSSQL/Oracle is "raw"
            }

            String querySourceDf = ingestionDataFrameNameIfAny; // Default
             // If Hive is source, ingestion might be skipped, flow query needs direct source.
            if (sourceType == JsonGenerationConfig.SourceType.HIVE && targetType == JsonGenerationConfig.TargetType.GCP) {
                querySourceDf = jira.getSourceTableName(); // Example: flow queries directly from source Hive table for Hive->GCP
            } else if (sourceType == JsonGenerationConfig.SourceType.HIVE && targetType == JsonGenerationConfig.TargetType.HIVE) {
                 querySourceDf = jira.getSourceTableName(); // Example for Hive->Hive
            }


            String flowDesc = sor.getSorName().toUpperCase(Locale.ROOT) + TABLE_TEXT + jira.getTableName() + INGESTION_TO_TEXT + jira.getApplicationName() + " - " + flowZone;
            String query;
            String registerName = jira.getTableName().toLowerCase(Locale.ROOT) + "_" + flowZone;
            String flowDatabaseName = "";
            String storageName = "";
            String targetTablePath = null;
            String hivePartitionColumn = auth.getHivePartition(); // Default
            String isCreateFlowTargetTable = "N";
            String isOverWritePartition = "Y";
            String isCacheEnabledFlow = "N";
            String sendEmailFlow = "N";
            String saveOutputFlow = "Y";

            if (targetType == JsonGenerationConfig.TargetType.GCP) {
                query = sqlScript.getSrcTableQuery() + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                 if (sourceType == JsonGenerationConfig.SourceType.HIVE) { // Hive to GCP Example
                     query = "SELECT * FROM " + jira.getSourceDatabase() + "." + jira.getSourceTableName() + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                 }
                flowDesc = "Extraction of " + (jira.getSourceTableName() != null ? jira.getSourceTableName() : jira.getTableName()) + " for CDMP processing.";
                registerName = jira.getTableName().toLowerCase(Locale.ROOT) + "_extract_df";
                saveOutputFlow = "N"; 
            } else { // Hive Target
                query = sqlScript.getSrcDataFrameQuery(); // Default for ingestion existing
                 if (sourceType == JsonGenerationConfig.SourceType.HIVE) { // Hive to Hive example
                     query = "SELECT * FROM " + jira.getSourceDatabase() + "." + jira.getSourceTableName(); // Direct query
                     querySourceDf = null; // No intermediate DF from ingestion
                 } else if (querySourceDf != null) { // If ingestion produced a DF
                     // Default for most source-to-Hive flows that use ingestion output
                     query = "SELECT * FROM " + querySourceDf;
                 }


                // Apply example-specific query structures for files
                if (isFileSource(sourceType)) {
                    String baseSelect = querySourceDf != null ? "SELECT df.*" : "SELECT *"; // df.* if querySourceDf existed
                    String auditCols = ", '${UTCAP_CRTE_BY_ID}' as utcap_crte_by_id, '${UTCAP_CRTE_DTTM}' as utcap_crte_dttm, '${UTCAP_DELTA_CD}' as utcap_delta_cd, '${UTCAP_ERR_FLG_IND}' as utcap_err_flg_ind, '${UTCAP_ERR_TXT}' as utcap_err_txt, '${UTCAP_RUN_ID}' as utcap_run_id, '${UTCAP_SOR_CD}' as utcap_sor_cd, '${UTCAP_BUSINESS_EFFECTIVE_DATE}' as utcap_business_effective_date";
                    query = baseSelect + auditCols + (querySourceDf != null ? (" FROM " + querySourceDf + " df") : ""); // Handle if querySourceDf is null

                    // File specific paths (following Pipe/Comma JSONs as per instruction)
                    storageName = "${TARGET_TABLE_NAME}";
                    targetTablePath = "${EXP_EDL_PATH}/${TARGET_TABLE_NAME}";
                    flowDatabaseName = "${HIVE_TARGET_DBNAME}";
                    hivePartitionColumn = "utcap_business_effective_date"; // Common for these file examples
                    sendEmailFlow = "Y";
                    isCacheEnabledFlow = "N"; // Explicitly N for files in examples
                    // isCreateFlowTargetTable is N for files
                } else { // DB sources to Hive
                    Map<String, String> storageNameMap = Map.of("CNAPP", "consumer", "UTCAP", "utcap");
                    String appSubDir = storageNameMap.getOrDefault(jira.getApplicationName(), jira.getApplicationName().toLowerCase());
                    storageName = "/datalake${EDL_ENV}/" + appSubDir + "/" +
                                  sor.getDomainName().toLowerCase(Locale.ROOT) + "/" +
                                  sor.getSorName().toLowerCase(Locale.ROOT) + "/" +
                                  flowZone + "/" + jira.getTableName();
                    flowDatabaseName = "${ETL_ENV}" + appSubDir + "_" + sor.getSorName().toLowerCase() + "_" + flowZone;
                    // Specific partition columns from DB examples
                    if (sourceType == JsonGenerationConfig.SourceType.ORACLE) hivePartitionColumn = "businessdate";
                    else if (sourceType == JsonGenerationConfig.SourceType.MSSQL) hivePartitionColumn = "eapp_dct_business_effective_date";
                    // else defaults to auth.getHivePartition() (covers Teradata, Hive as source)
                    isCreateFlowTargetTable = (sourceType == JsonGenerationConfig.SourceType.ORACLE) ? "Y" : "N"; // Oracle example creates tables in flows
                    isCacheEnabledFlow = (sourceType == JsonGenerationConfig.SourceType.TERADATA) ? "Y" : "N"; // Teradata example had Y
                }
            }

            flowConfigs.add(new JsonGenerationConfig.FlowDetailConfig(
                    flowId, flowDesc, query, registerName, flowDatabaseName, storageName, targetTablePath,
                    hivePartitionColumn, isCreateFlowTargetTable, isOverWritePartition, isCacheEnabledFlow,
                    sendEmailFlow, "N", saveOutputFlow // abortIfEmpty = "N" for all
            ));
        }
        return flowConfigs;
    }
    
    private List<JsonGenerationConfig.CheckDetailConfig> prepareCheckDetails(
        JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
        JiraStoryIntake jira, ApplicationAuthorizer auth,
        List<JsonGenerationConfig.FlowDetailConfig> flowDetails, String ingestionDataFrameName) {

        List<JsonGenerationConfig.CheckDetailConfig> checkConfigs = new ArrayList<>();
        if (targetType == JsonGenerationConfig.TargetType.GCP || flowDetails.isEmpty()) {
            return checkConfigs; // No data checks for GCP target or if no flows defined
        }

        for (JsonGenerationConfig.FlowDetailConfig currentFlow : flowDetails) {
            String checkId = currentFlow.getFlowId(); // Match checkId to flowId
            String checkName = "HHRCNTCHK";
            String checkDesc = "Data Count Check";
            
            String checkSourceQueryDataFrame;
            // Source for check is typically the output of ingestion, or initial table if no ingestion (Hive source)
            if (sourceType == JsonGenerationConfig.SourceType.HIVE) { // If source is Hive, ingestion is skipped.
                checkSourceQueryDataFrame = jira.getSourceDatabase() + "." + jira.getSourceTableName(); // Check raw Hive table
            } else if (ingestionDataFrameName != null) {
                checkSourceQueryDataFrame = ingestionDataFrameName; // Use output of ingestion step
            } else {
                LOG.warn("Cannot determine source DataFrame for data check for source: {} and flow: {}", sourceType, currentFlow.getFlowId());
                continue; // Skip this check if source DF is unclear
            }

            String checkSourceQuery = "select count(*) from " + checkSourceQueryDataFrame;
            String checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowDatabaseName() + "." + jira.getTableName() +
                                     " where " + currentFlow.getResolvedFlowHivePartitionColumn() + "='${EAPP_DCT_BUSINESS_EFFECTIVE_DATE}'";
            
            // Specific targetQuery for FixedWidth's single flow which uses registerName as target
            if (sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "demog_exp_e3_load_sanitized".equals(currentFlow.getResolvedFlowRegisterName())) {
                checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowRegisterName();
            }


            String isFailJobOnDataCheckError = (isFileSource(sourceType)) ? "Y" : "N";

            checkConfigs.add(new JsonGenerationConfig.CheckDetailConfig(
                    checkId, checkName, checkDesc, checkSourceQuery, checkTargetQuery,
                    "Y", "Y", "N", "N", isFailJobOnDataCheckError // isEnabled, isUserSupply, readSourcePath, readTargetPath
            ));
        }
        return checkConfigs;
    }
    
    private String determineDataChecksAcDcDatabase(JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType, ApplicationAuthorizer auth, Sor sor, boolean checksArePresent) {
        if (checksArePresent && targetType != JsonGenerationConfig.TargetType.GCP) {
            if(sourceType == JsonGenerationConfig.SourceType.MSSQL || sourceType == JsonGenerationConfig.SourceType.ORACLE) return "${ETL_ENV}consumer_audit";
            if(sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "utcap".equalsIgnoreCase(jira.getApplicationName())) return "utcap_curated_mu_audit"; // jira is not in scope. Use auth.
            if(sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "utcap".equalsIgnoreCase(auth.getApplicationName())) return "utcap_curated_mu_audit";
            if((sourceType == JsonGenerationConfig.SourceType.FILE_PIPE || sourceType == JsonGenerationConfig.SourceType.FILE_COMMA) &&
                "utcap".equalsIgnoreCase(auth.getApplicationName()) && "corelogic".equalsIgnoreCase(sor.getSorName())) return "${UTCAP_AUDIT_DB}";
            return auth.getAuditTable();
        }
        return "${AUDIT_DATABASE}";
    }

    private ObjectNode buildPrebuiltDataChecksPreChecksNode(JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType) {
        ObjectNode preChecksNode = objectMapper.createObjectNode();
        // Default isPreCheckEnabled to "Y" from metadata, specific examples may override to "N"
        boolean overallPreCheckEnabled = true; 
        // Example: FixedWidth JSON set root isPreCheckEnabled to N
        if(sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) overallPreCheckEnabled = false;

        preChecksNode.put("isPreCheckEnabled", overallPreCheckEnabled ? "Y" : "${PRECHECK_ENABLED}"); // Use Y/N or placeholder
        ArrayNode preCheckArray = preChecksNode.putArray("preCheck");

        boolean isDbAsSource = isDatabaseSource(sourceType);
        boolean isFileAsSource = isFileSource(sourceType);

        // Values "Y" / "N" for isPreCheckEnabled / isFailJobOnPreCheckError based on dominant patterns or specific example if available
        addPreCheckItem(preCheckArray, "ServiceIDCheck", overallPreCheckEnabled ? "Y" : "N", "Y");
        addPreCheckItem(preCheckArray, "TargetDataBaseAvailabilityCheck", overallPreCheckEnabled && targetType == JsonGenerationConfig.TargetType.HIVE ? "Y" : "N", "Y");
        addPreCheckItem(preCheckArray, "TargetTableAvailabilityCheck", overallPreCheckEnabled && targetType == JsonGenerationConfig.TargetType.HIVE ? "Y" : "N", "Y");
        addPreCheckItem(preCheckArray, "TargetTablePathWritableCheck", overallPreCheckEnabled && targetType == JsonGenerationConfig.TargetType.HIVE ? "Y" : "N", "Y");
        addPreCheckItem(preCheckArray, "FlowSeqDependenciesCheck", "N", "N"); // Commonly N
        addPreCheckItem(preCheckArray, "SourceDataBaseAvailabilityCheck", overallPreCheckEnabled && isDbAsSource ? "Y" : "N", "Y");
        addPreCheckItem(preCheckArray, "SourceTableAvailabilityCheck", overallPreCheckEnabled && isDbAsSource ? "Y" : "N", "Y");
        addPreCheckItem(preCheckArray, "SourceTableEmptyCheck", "N", "N"); // Commonly N
        addPreCheckItem(preCheckArray, "SourcePathAvailabilityCheck", overallPreCheckEnabled && isFileAsSource ? "Y" : "N", "Y");
        addPreCheckItem(preCheckArray, "SourcePathEmptyCheck", "N", "N"); // Commonly N
        return preChecksNode;
    }

    private void addPreCheckItem(ArrayNode array, String name, String enabled, String failOnError) {
        ObjectNode item = objectMapper.createObjectNode();
        item.put("preCheckName", name);
        item.put("isPreCheckEnabled", enabled);
        item.put("isFailJobOnPreCheckError", failOnError);
        array.add(item);
    }

    private ObjectNode createApplicationNode(JsonGenerationConfig config) {
        ObjectNode applicationNode = objectMapper.createObjectNode();
        applicationNode.put("isParallelProcessing", "N");
        applicationNode.set("jobs", createJobsNode(config));
        return applicationNode;
    }

    private ObjectNode createJobsNode(JsonGenerationConfig config) {
        ObjectNode jobsNode = objectMapper.createObjectNode();
        jobsNode.set("job", createJobNode(config));
        return jobsNode;
    }

    private ObjectNode createJobNode(JsonGenerationConfig config) {
        ObjectNode jobNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        ApplicationAuthorizer auth = config.getApplicationAuthorizer();

        jobNode.put("JobName", jira.getTableName()); // Capital J

        Set<String> jobFields = config.getJobLevelFieldsToInclude();
        if (jobFields.contains("app")) jobNode.put("app", jira.getApplicationName());
        if (jobFields.contains("appRemedyId")) jobNode.put("appRemedyId", auth.getAppRemedyId());
        if (jobFields.contains("sorRemedyId")) jobNode.put("sorRemedyId", sor.getRemedyId());
        if (jobFields.contains("queryGroup")) jobNode.put("queryGroup", "dimc_check_" + sor.getSorName().toLowerCase(Locale.ROOT));
        if (jobFields.contains("zone")) jobNode.put("zone", SANITIZED_ZONE);
        if (jobFields.contains("sor")) jobNode.put("sor", sor.getSorName().toLowerCase(Locale.ROOT)); // Use lowercase sor from examples

        jobNode.put("jobId", "1");
        jobNode.put("jobDescription", determineJobDescription(config));
        jobNode.put("isJobEnabled", "Y");
        jobNode.set("Email", createEmailNode(config));

        if (config.isIncludeIngestions()) {
            ObjectNode ingestionsNode = buildGenericIngestionNode(config);
            if (ingestionsNode != null && ingestionsNode.size() > 0) jobNode.set("ingestions", ingestionsNode);
        }
        if (config.isIncludeFlows()) {
            ObjectNode flowsNode = buildGenericFlowsNode(config);
            if (flowsNode != null && flowsNode.size() > 0) jobNode.set("flows", flowsNode);
        }
        if (config.isIncludeDataChecks()) {
            ObjectNode dataChecksNode = buildGenericDataChecksNode(config);
             if (dataChecksNode != null && dataChecksNode.size() > 0) jobNode.set("dataChecks", dataChecksNode);
        }
        if (config.isIncludeFileExtracts()) {
            ObjectNode fileExtractsNode = buildGenericFileExtractsNode(config);
             if (fileExtractsNode != null && fileExtractsNode.size() > 0) jobNode.set("fileExtracts", fileExtractsNode);
        }
        return jobNode;
    }
    
    private String determineJobDescription(JsonGenerationConfig config){
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            return "CDMP Extract for Hive table - " + jira.getTableName().toLowerCase(Locale.ROOT);
        } else {
            // Specific example overrides for description if needed
            if ("BKPF".equalsIgnoreCase(sor.getSorName()) && "mappings_consolidations".equalsIgnoreCase(jira.getTableName())) { // MSSQL example
                 return sor.getSorName() + " " + jira.getTableName() + INGESTION_TO_TEXT + jira.getApplicationName();
            } else if ("Hemi".equalsIgnoreCase(sor.getSorName()) && "demog_exp_e3_load".equalsIgnoreCase(jira.getTableName())) { // FW Example
                return sor.getSorName() + TABLE_TEXT + jira.getTableName() + INGESTION_TO_TEXT + "Hive"; // Example has "to Hive"
            } else if ("corelogic".equalsIgnoreCase(sor.getSorName()) && "utcap vol_lien_load".equalsIgnoreCase(jira.getTableName())) { // CSV/Pipe uses different JobName from table
                 return "utcap " + sor.getSorName() + " ingestion for feed " + jira.getTableName();
            }
            return sor.getSorName() + TABLE_TEXT + jira.getTableName().toLowerCase(Locale.ROOT) + INGESTION_TO_TEXT + jira.getApplicationName();
        }
    }

    private ObjectNode createEmailNode(JsonGenerationConfig config) {
        ObjectNode emailNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();

        String fromAddress = "${EMAIL_TO_LIST}";
        String toAddress = "${EMAIL_CC_LIST}";
        String subject = "Data Extract from " + sor.getSorName().toUpperCase(Locale.ROOT) +
                         " to " + jira.getApplicationName() + " - " +
                         jira.getTableName().toLowerCase(Locale.ROOT);

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            fromAddress = "${FROM_EMAIL}"; toAddress = "${TO_EMAIL}";
            subject = "CDMP Extraction - " + jira.getTableName().toLowerCase(Locale.ROOT);
        } else if (config.getSourceType() == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) {
            fromAddress = "${EMAIL_FROM_LIST}";
            subject = "Fixed width UTCAP Demog Feed ingestion " + jira.getTableName() + " hive table";
        } else if (isFileSource(config.getSourceType()) && "corelogic".equalsIgnoreCase(sor.getSorName())) {
            subject = "utcap ingestion for feed " + jira.getTableName();
        }

        emailNode.put("fromAddress", fromAddress);
        emailNode.put("toAddress", toAddress);
        emailNode.put("subject", subject);
        emailNode.put("isPriorityAlert", "Y");
        return emailNode;
    }

    private ObjectNode buildGenericIngestionNode(JsonGenerationConfig config) {
        Set<String> fields = config.getIngestionFieldsToInclude();
        if (fields.isEmpty()) return null;

        ObjectNode ingestionItemNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();

        if (fields.contains("ingestionId")) ingestionItemNode.put("ingestionId", "1");
        if (fields.contains("ingestionDesc")) {
             String desc = sor.getSorName() + TABLE_TEXT + jira.getTableName() + INGESTION_TO_TEXT + jira.getApplicationName();
             // Example-based overrides
             if ("BKPF".equalsIgnoreCase(sor.getSorName())) desc = sor.getSorName() + " " + jira.getTableName() + " ingestion";
             else if ("HEMI".equalsIgnoreCase(sor.getSorName())) desc = sor.getSorName() + TABLE_TEXT + jira.getTableName() + " ingestion to EDL";
             else if ("corelogic".equalsIgnoreCase(sor.getSorName())) desc = jira.getTableName() + " ingestion to utcap sanitized";
             ingestionItemNode.put("ingestionDesc", desc);
        }
        if (fields.contains("isIngestionEnabled")) ingestionItemNode.put("isIngestionEnabled", "Y");
        if (fields.contains("sourceType")) ingestionItemNode.put("sourceType", config.getResolvedIngestionSourceTypeValue());
        if (fields.contains("ConnectionHeader") && config.getResolvedIngestionConnectionHeaderValue() != null)
            ingestionItemNode.put("ConnectionHeader", config.getResolvedIngestionConnectionHeaderValue());
        if (fields.contains("delimiter") && config.getResolvedIngestionDelimiter() != null)
            ingestionItemNode.put("delimiter", config.getResolvedIngestionDelimiter());
        if (fields.contains("dataFrameName")) ingestionItemNode.put("dataFrameName", config.getResolvedIngestionDataFrameName());
        if (fields.contains("isCreateTargetTable")) ingestionItemNode.put("isCreateTargetTable", config.getIngestionIsCreateTargetTableValue());
        if (fields.contains("isCacheEnabled")) ingestionItemNode.put("isCacheEnabled", config.getIngestionIsCacheEnabledValue());
        if (fields.contains("isOverWritePartition")) ingestionItemNode.put("isOverWritePartition", config.getIngestionIsOverWritePartitionValue());
        if (fields.contains("abortIfDuplicate")) ingestionItemNode.put("abortIfDuplicate", "N");
        if (fields.contains("abortIfEmpty")) ingestionItemNode.put("abortIfEmpty", "N");
        if (fields.contains("isRunEMHReport")) ingestionItemNode.put("isRunEMHReport", "N");

        if (fields.contains("sourceSQLQuery") && config.getResolvedIngestionSourceSQLQuery() != null)
            ingestionItemNode.put("sourceSQLQuery", config.getResolvedIngestionSourceSQLQuery());
        if (fields.contains("sourceName") && config.getResolvedIngestionSourceName() != null)
            ingestionItemNode.put("sourceName", config.getResolvedIngestionSourceName());
        if (fields.contains("dataFilePath") && config.getResolvedIngestionDataFilePath() != null)
            ingestionItemNode.put("dataFilePath", config.getResolvedIngestionDataFilePath());
        if (fields.contains("schemaFilePath") && config.getResolvedIngestionSchemaFilePath() != null)
            ingestionItemNode.put("schemaFilePath", config.getResolvedIngestionSchemaFilePath());
        if (fields.contains("stagingFilePath") && config.getResolvedIngestionStagingFilePath() != null)
            ingestionItemNode.put("stagingFilePath", config.getResolvedIngestionStagingFilePath());
        if (fields.contains("isHeaderEnabled") && config.getResolvedIngestionIsHeaderEnabled() != null)
            ingestionItemNode.put("isHeaderEnabled", config.getResolvedIngestionIsHeaderEnabled());
        if (fields.contains("saveOutput") && config.getResolvedIngestionSaveOutputFlag() != null)
            ingestionItemNode.put("saveOutput", config.getResolvedIngestionSaveOutputFlag());
        if (fields.contains("targetPartition") && config.getResolvedIngestionTargetPartition() != null)
            ingestionItemNode.put("targetPartition", config.getResolvedIngestionTargetPartition());
        if (fields.contains("targetType") && config.getTargetType() == JsonGenerationConfig.TargetType.HIVE)
             ingestionItemNode.put("targetType", "parquet");


        if (fields.contains("additionalReadConfigs") && config.getPrebuiltIngestionAdditionalReadConfigsNode() != null)
            ingestionItemNode.set("additionalReadConfigs", config.getPrebuiltIngestionAdditionalReadConfigsNode());
        if (fields.contains("dimcChecks") && config.isIncludeDimcChecksInIngestion() && config.getPrebuiltIngestionDimcChecksNode() != null)
            ingestionItemNode.set("dimcChecks", config.getPrebuiltIngestionDimcChecksNode());
        
        ObjectNode finalIngestionsNode = objectMapper.createObjectNode();
        if(ingestionItemNode.size() > 0) { // Only add if the item node has content
             addToArray(finalIngestionsNode, "ingestion", ingestionItemNode);
             return finalIngestionsNode;
        }
        return null;
    }

    private ObjectNode buildGenericFlowsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFlows() || config.getFlowDetailsList().isEmpty()) return null;

        ObjectNode flowsNode = objectMapper.createObjectNode();
        ArrayNode flowArray = flowsNode.putArray("flow");

        for (JsonGenerationConfig.FlowDetailConfig flowDetail : config.getFlowDetailsList()) {
            ObjectNode flowItemNode = objectMapper.createObjectNode();
            flowItemNode.put("flowId", flowDetail.getFlowId());
            flowItemNode.put("flowDesc", flowDetail.getResolvedFlowDescription());
            flowItemNode.put("isFlowEnabled", "Y");
            flowItemNode.put("query", flowDetail.getResolvedFlowQuery());
            flowItemNode.put("registerName", flowDetail.getResolvedFlowRegisterName());
            
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) {
                flowItemNode.put("saveOutput", flowDetail.getSaveOutputValue());
                if(flowDetail.getResolvedFlowDatabaseName()!=null) flowItemNode.put("flowDataBaseName", flowDetail.getResolvedFlowDatabaseName());
                if(flowDetail.getResolvedFlowStorageName()!=null) flowItemNode.put("storageName", flowDetail.getResolvedFlowStorageName());
                if(flowDetail.getResolvedFlowTargetTablePath()!=null) flowItemNode.put("targetTablePath", flowDetail.getResolvedFlowTargetTablePath());
                if(flowDetail.getResolvedFlowHivePartitionColumn()!=null) flowItemNode.put("hivePartitionColumn", flowDetail.getResolvedFlowHivePartitionColumn());
                flowItemNode.put("isCreateFlowTargetTable", flowDetail.getIsCreateFlowTargetTableValue());
                flowItemNode.put("isOverWritePartition", flowDetail.getIsOverWritePartitionValue());
                flowItemNode.put("isCacheEnabled", flowDetail.getIsCacheEnabledValue());
                flowItemNode.put("sendEmail", flowDetail.getSendEmailValue());
                flowItemNode.put("abortIfEmpty", flowDetail.getAbortIfEmptyValue());
            }
            flowArray.add(flowItemNode);
        }
        return flowsNode;
    }

    private ObjectNode buildGenericDataChecksNode(JsonGenerationConfig config) {
        if (!config.isIncludeDataChecks()) return null;
        // Only build if there are actual checks or preChecks to add, beyond the base properties.
        boolean hasCheckDetails = !config.getCheckDetailsList().isEmpty();
        boolean hasPreChecks = config.getPrebuiltDataChecksPreChecksNode() != null && config.getPrebuiltDataChecksPreChecksNode().size() > 0;

        if (!hasCheckDetails && !hasPreChecks && config.getTargetType() == JsonGenerationConfig.TargetType.GCP) { // GCP has no checks usually
             return null;
        }


        ObjectNode dataChecksNode = objectMapper.createObjectNode();
        if(config.getResolvedDataChecksAcDcDatabase() != null) {
             dataChecksNode.put("acDCDatabase", config.getResolvedDataChecksAcDcDatabase());
        }
        dataChecksNode.put("isTrendingEnabled", "N");

        if (hasCheckDetails) {
            ArrayNode checkArray = dataChecksNode.putArray("check");
            for (JsonGenerationConfig.CheckDetailConfig detail : config.getCheckDetailsList()) {
                ObjectNode checkItemNode = objectMapper.createObjectNode();
                checkItemNode.put("checkId", detail.getCheckId());
                checkItemNode.put("checkName", detail.getCheckName());
                checkItemNode.put("checkDesc", detail.getCheckDescription());
                checkItemNode.put("isCheckEnabled", detail.getIsCheckEnabledValue());
                checkItemNode.put("isUserSupplyQuery", detail.getIsUserSupplyQueryValue());
                checkItemNode.put("readSourceQueryFromPath", detail.getReadSourceQueryFromPathValue());
                checkItemNode.put("sourceQuery", detail.getResolvedCheckSourceQuery());
                checkItemNode.put("targetQuery", detail.getResolvedCheckTargetQuery());
                checkItemNode.put("readTargetQueryFromPath", detail.getReadTargetQueryFromPathValue());
                checkItemNode.put("isFailJobOnDataCheckError", detail.getIsFailJobOnDataCheckErrorValue());
                checkArray.add(checkItemNode);
            }
        }
        
        if(hasPreChecks) { // Add preChecks only if it's populated
            dataChecksNode.set("preChecks", config.getPrebuiltDataChecksPreChecksNode());
        }
        
        // Return node only if it has meaningful content
        if (dataChecksNode.size() == 0 || (dataChecksNode.size() == 2 && dataChecksNode.has("acDCDatabase") && dataChecksNode.has("isTrendingEnabled") && !hasCheckDetails && !hasPreChecks) ) {
            return null;
        }
        return dataChecksNode;
    }

    private ObjectNode buildGenericFileExtractsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFileExtracts()) return null;
        // (Structure from previous response, seems stable for GCP)
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = fileExtractsNode.putArray("fileExtract");
        JiraStoryIntake jira = config.getJiraStoryIntake();
        ObjectNode fileExtractItemNode = objectMapper.createObjectNode();
        fileExtractItemNode.put("fileExtractId", "1");
        fileExtractItemNode.put("isFileExtractEnabled", "Y");
        fileExtractItemNode.put("fileDescription", "Extracting " + jira.getTableName().toLowerCase(Locale.ROOT) + " to avro file for CDMP");
        fileExtractItemNode.put("subjectArea", "CDMP");
        fileExtractItemNode.put("zone", RAW_ZONE);
        fileExtractItemNode.put("system", "cdmp");
        fileExtractItemNode.put("fileType", "avro");
        fileExtractItemNode.put("extractQuery", "select * from " + jira.getTableName().toLowerCase(Locale.ROOT) + "_extract_df");
        fileExtractItemNode.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        fileExtractItemNode.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        ObjectNode cdmpParamsNode = objectMapper.createObjectNode();
        cdmpParamsNode.put("outputFormat", "avro");
        cdmpParamsNode.put("cdmpFileName", jira.getTableName().toLowerCase(Locale.ROOT) + "_cdmp_extract");
        cdmpParamsNode.put("cdmpPatternMode", "daily");
        cdmpParamsNode.put("isCustomPartitionEnabled", "true");
        cdmpParamsNode.put("desiredPartFileSize", "${PART_FILE_SIZE}");
        fileExtractItemNode.set("cdmpAdditionalParams", cdmpParamsNode);
        fileExtractArray.add(fileExtractItemNode);
        return fileExtractsNode;
    }

    private static synchronized void populateDimcChecksMasterList() {
        if (!DIMC_CHECKS_MASTER_LIST.isEmpty()) return;
        LOG.info("Populating DIMC Checks Master List...");
        // Using HashMap<String, Object> explicitly
        Map<String, Object> check1 = new HashMap<>(); check1.put("dimcCheckname", "duplicateFileLoadChecks"); check1.put("isDimcCheckEnabled", "${DUPLICATE_FILELOAD_CHECK_ENABLED}"); check1.put("isFailJobOnError", "${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check1));
        Map<String, Object> check2 = new HashMap<>(); check2.put("dimcCheckname", "dataVolumeConsistencyCheck"); check2.put("isDimcCheckEnabled", "${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}"); check2.put("upperThreshold", "${UPPER_THRESHOLD}"); check2.put("lowerThreshold", "${LOWER_THRESHOLD}"); check2.put("numPastDays", "${NUM_PAST_DAYS}"); check2.put("isFailJobOnError", "${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check2));
        Map<String, Object> check3 = new HashMap<>(); check3.put("dimcCheckname", "missingFileCheck"); check3.put("isDimcCheckEnabled", "${MISSING_FILE_CHECK_ENABLED}"); check3.put("isFailJobOnError", "${MISSING_FILE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check3));
        Map<String, Object> check4 = new HashMap<>(); check4.put("dimcCheckname", "deliveryCheck"); check4.put("isDimcCheckEnabled", "${DELIVERY_CHECK_ENABLED}"); check4.put("SLA", "${DELIVERY_CHECK_SLA}"); check4.put("isFailJobOnError", "${DELIVERY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check4));
        Map<String, Object> check5 = new HashMap<>(); check5.put("dimcCheckname", "missingDbObjectCheck"); check5.put("isDimcCheckEnabled", "${MISSING_DB_OBJECT_CHECK_ENABLED}"); check5.put("isFailJobOnError", "${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check5));
        Map<String, Object> check6 = new HashMap<>(); check6.put("dimcCheckname", "emptySourceCheck"); check6.put("isDimcCheckEnabled", "${EMPTY_SOURCE_CHECK_ENABLED}"); check6.put("isFailJobOnError", "${EMPTY_SOURCE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check6));
        LOG.info("DIMC Checks Master List populated with {} items.", DIMC_CHECKS_MASTER_LIST.size());
    }

    private static List<Map<String, Object>> getChosenDimcChecksList(List<Integer> chosenIndices) {
        if (chosenIndices == null || chosenIndices.isEmpty()) return Collections.emptyList();
        return chosenIndices.stream()
                .filter(index -> index != null && index >= 0 && index < DIMC_CHECKS_MASTER_LIST.size())
                .map(index -> new HashMap<>(DIMC_CHECKS_MASTER_LIST.get(index)))
                .collect(Collectors.toList());
    }
    
    // ... (parseSourceType, parseTargetType, addToArray, getOrCreateConnectionJsonReq as before) ...
    // (Make sure parseSourceType includes all new source types and maps them to the enum)
    // (Make sure RequirementRepository is available for getOrCreateConnectionJsonReq if that method is still used)
     private JsonGenerationConfig.SourceType parseSourceType(String sorTypeStr) {
        if (sorTypeStr == null) return JsonGenerationConfig.SourceType.UNKNOWN;
        String typeLower = sorTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "teradata": return JsonGenerationConfig.SourceType.TERADATA;
            case "file-fixedwidth": case "fixedwidthfile": // from example
                 return JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
            case "file-pipe": // from user requirement
                 return JsonGenerationConfig.SourceType.FILE_PIPE;
            case "file-comma": // from user requirement
                 return JsonGenerationConfig.SourceType.FILE_COMMA;
            case "mssql": // from user requirement
                 return JsonGenerationConfig.SourceType.MSSQL;
            case "oracle": // from user requirement
                 return JsonGenerationConfig.SourceType.ORACLE;
            case "hive": // from user requirement
                 return JsonGenerationConfig.SourceType.HIVE;
            case "file": // from csv/pipe examples - map to comma by default if no other info? Or require more specific type?
                 LOG.warn("Generic 'file' sourceType found, assuming Comma. For specific behavior, use 'file-comma' or 'file-pipe'. Input: {}", sorTypeStr);
                 return JsonGenerationConfig.SourceType.FILE_COMMA; // Defaulting generic 'file' to Comma
            default:
                LOG.warn("Unrecognized Source Type string: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.UNKNOWN;
        }
    }

    private JsonGenerationConfig.TargetType parseTargetType(String targetTypeStr) {
        if (targetTypeStr == null) return JsonGenerationConfig.TargetType.UNKNOWN;
        String typeLower = targetTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "hive": return JsonGenerationConfig.TargetType.HIVE;
            case "gcp": return JsonGenerationConfig.TargetType.GCP;
            default:
                LOG.warn("Unrecognized Target Type string: {}", targetTypeStr);
                return JsonGenerationConfig.TargetType.UNKNOWN;
        }
    }

    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) {
        if (item == null || item.size() == 0) return; // Do not add empty or null items
        ArrayNode array = parent.has(arrayName) && parent.get(arrayName).isArray() ?
                (ArrayNode) parent.get(arrayName) : parent.putArray(arrayName);
        array.add(item);
    }
     public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) {
         Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository.findById(requirementId);

         if (existingConnJsonRequirement.isPresent()) {
             return existingConnJsonRequirement.get();
         }
         ObjectNode rootNode = objectMapper.createObjectNode();
         ObjectNode application = objectMapper.createObjectNode();
         ObjectNode email = objectMapper.createObjectNode();
         String sorName;
         try {
             // Assuming requirementId for ConnJson is the same base requirementId (e.g. BZLS-XXXX)
             // And that RequirementRepository holds data keyed by this.
             // This part might need adjustment based on actual data model for RequirementRepository
             String baseRequirementId = requirementId.contains("_") ? requirementId.split("_")[0] : requirementId;
             List<Requirement> reqs = requirementRepository.findByRequirementId(baseRequirementId);
             if (reqs.isEmpty()) throw new ConfigurationException("Requirement not found in RequirementRepository for: " + baseRequirementId);
             sorName = reqs.get(0).getSorName();
         } catch (Exception e) {
             LOG.error("Failed to get SorName for ConnJson requirementId: {}. Error: {}", requirementId, e.getMessage());
             sorName = "UNKNOWN_SOR";
         }
         String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName + " with Job Name $jobName has completed. " +
                           "</p><p> SourceDate =$sourceDate , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
         email.put("body", emailBody);
         ObjectNode job = objectMapper.createObjectNode();
         job.set("Email", email);
         application.set("job", job);
         rootNode.set("application", application);
         Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
         ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
         return connJsonRequirementRepository.save(connJsonRequirement);
    }
}
