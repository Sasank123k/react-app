package com.wellsfargo.utcap.config;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.model.*;
import lombok.Getter;
import lombok.ToString; // For easier debugging
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


import java.util.*;
import java.util.stream.Collectors;

@Getter
@ToString
public class JsonGenerationConfig {
    private static final Logger LOG = LoggerFactory.getLogger(JsonGenerationConfig.class);

    // Constants from Service, needed for descriptions/paths if built here
    private static final String SANITIZED_ZONE = "sanitized";
    private static final String RAW_ZONE = "raw";
    private static final String TABLE_TEXT = " Table ";
    private static final String INGESTION_TO_TEXT = " ingestion to ";
    private static final String DIMC_CHECKS_ENABLED_PLACEHOLDER = "${DIMCCHECK_ENABLED}";


    public enum SourceType {
        TERADATA, FILE_FIXEDWIDTH, FILE_PIPE, FILE_COMMA, MSSQL, ORACLE, HIVE, UNKNOWN
    }

    public enum TargetType {
        HIVE, GCP, UNKNOWN
    }

    // --- Core Input Data Objects (final) ---
    private final JiraStoryIntake jiraStoryIntake;
    private final Sor sor;
    private final ApplicationAuthorizer applicationAuthorizer;
    private final SQLScript sqlScript;
    private final List<Integer> chosenDimcCheckIndices; // Source-specific indices
    private final List<Map<String, Object>> dimcChecksMasterList; // The global master list
    private final ObjectMapper objectMapper;


    // --- Determined Types (final) ---
    private final SourceType sourceType;
    private final TargetType targetType;

    // Constructor: Initializes with raw data and context
    public JsonGenerationConfig(JiraStoryIntake jira, Sor sor, ApplicationAuthorizer auth, SQLScript script,
                                SourceType sourceType, TargetType targetType,
                                List<Integer> chosenDimcCheckIndices,
                                List<Map<String, Object>> dimcChecksMasterList, // Pass the master list
                                ObjectMapper objectMapper) {
        this.jiraStoryIntake = Objects.requireNonNull(jira, "JiraStoryIntake cannot be null");
        this.sor = Objects.requireNonNull(sor, "Sor cannot be null");
        this.applicationAuthorizer = Objects.requireNonNull(auth, "ApplicationAuthorizer cannot be null");
        this.sqlScript = Objects.requireNonNull(script, "SQLScript cannot be null");
        this.sourceType = Objects.requireNonNull(sourceType, "SourceType cannot be null");
        this.targetType = Objects.requireNonNull(targetType, "TargetType cannot be null");
        this.chosenDimcCheckIndices = Objects.requireNonNull(chosenDimcCheckIndices, "chosenDimcCheckIndices cannot be null");
        this.dimcChecksMasterList = Objects.requireNonNull(dimcChecksMasterList, "dimcChecksMasterList cannot be null");
        this.objectMapper = Objects.requireNonNull(objectMapper, "objectMapper cannot be null");

    }

    // --- Section Inclusion Resolvers ---
    public boolean isIncludeIngestions() {
        return this.sourceType != SourceType.HIVE;
    }

    public boolean isIncludeFlows() {
        return true; // Always true
    }

    public boolean isIncludeDataChecks() {
        return true; // Always true
    }

    public boolean isIncludeFileExtracts() {
        return this.targetType == TargetType.GCP;
    }

    private boolean isIncludeDimcChecksInIngestion() {
        return isIncludeIngestions() &&
               (EnumSet.of(SourceType.TERADATA, SourceType.HIVE).contains(this.sourceType) ||
                isCurrentSourceTypeFileBased());
    }

    // --- Field Inclusion Set Resolvers ---
    public Set<String> getJobLevelFieldsToInclude() {
        if (this.targetType != TargetType.GCP) {
            return new HashSet<>(Arrays.asList("app", "appRemedyId", "sorRemedyId", "queryGroup", "zone", "sor"));
        }
        return Collections.emptySet();
    }

    public Set<String> getIngestionFieldsToInclude() {
        if (!isIncludeIngestions()) return Collections.emptySet();

        Set<String> fields = new HashSet<>(Arrays.asList(
            "ingestionId", "ingestionDesc", "isIngestionEnabled", "dataFrameName",
            "isCreateTargetTable", "isCacheEnabled", "isOverWritePartition",
            "abortIfDuplicate", "abortIfEmpty", "isRunEMHReport", "sourceType"
        ));
        if (this.targetType == TargetType.HIVE) {
            fields.add("targetType"); // "parquet"
        }

        if (isCurrentSourceTypeDatabase()) {
            fields.add("ConnectionHeader");
            fields.add("sourceSQLQuery");
            if (this.sourceType == SourceType.ORACLE) {
                fields.add("sourceName");
            }
        } else if (isCurrentSourceTypeFileBased()) {
            fields.addAll(Arrays.asList("isHeaderEnabled", "dataFilePath", "schemaFilePath", "stagingFilePath"));
            if (this.sourceType != SourceType.FILE_FIXEDWIDTH) { // Pipe or Comma
                fields.add("delimiter");
                fields.add("additionalReadConfigs"); // Also for FW now
            }
             if (this.sourceType == SourceType.FILE_FIXEDWIDTH) { //FW also gets additionalReadConfigs now
                fields.add("additionalReadConfigs");
            }
            if (this.sourceType == SourceType.FILE_PIPE || this.sourceType == SourceType.FILE_COMMA || this.sourceType == SourceType.FILE_FIXEDWIDTH) {
                 fields.add("saveOutput");
                 fields.add("targetPartition");
            }
        }
        if (isIncludeDimcChecksInIngestion()) {
            fields.add("dimcChecks");
        }
        return fields;
    }


    // --- Resolved Value Getters for INGESTION ---
    public String getResolvedIngestionSourceTypeValue() {
        switch (this.sourceType) {
            case TERADATA: return "teradb";
            case FILE_FIXEDWIDTH: return "fixedwidthFile";
            case FILE_PIPE: case FILE_COMMA: return "file";
            case MSSQL: return "jdbc";
            case ORACLE: return "oracle";
            case HIVE: return "hive";
            default: return "unknown";
        }
    }

    public String getResolvedIngestionConnectionHeaderValue() {
        switch (this.sourceType) {
            case TERADATA: return "teradbConn";
            case MSSQL: return "jdbcConn";
            case ORACLE: return "oracleConn";
            case HIVE: return "hiveConn";
            default: return null;
        }
    }

    public String getResolvedIngestionDataFrameName() {
        // Standardized to _df for all sources
        return this.jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_df";
    }

    public String getResolvedIngestionSourceSQLQuery() {
        return isCurrentSourceTypeDatabase() ? this.sqlScript.getSrcTableQuery() : null;
    }
    
    public String getResolvedIngestionSourceName() { // Specifically for Oracle
        return (this.sourceType == SourceType.ORACLE) ? "${BDDMCBD_SRC_SCHEMA}." + this.jiraStoryIntake.getTableName() : null;
    }

    public String getResolvedIngestionDataFilePath() {
        return isCurrentSourceTypeFileBased() ? "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}" : null;
    }
    public String getResolvedIngestionSchemaFilePath() {
        return isCurrentSourceTypeFileBased() ? "${DCI_CONFIG_DIR_PATH}/config/" + this.sor.getSorName().toLowerCase() + "/${FEED_NAME}/metadata/" + this.jiraStoryIntake.getTableName().toLowerCase() + ".schema" : null;
    }
    public String getResolvedIngestionStagingFilePath() {
        if (this.sourceType == SourceType.FILE_FIXEDWIDTH && "Hemi".equalsIgnoreCase(this.sor.getSorName()) && "demog_exp_e3_load".equalsIgnoreCase(this.jiraStoryIntake.getTableName())) {
            return "/datalake/utcap/hemi/sanitized";
        }
        return isCurrentSourceTypeFileBased() ? "${EXP_EDL_PATH}" : null;
    }

    public String getResolvedIngestionDelimiter() {
        if (this.sourceType == SourceType.FILE_PIPE) return "|";
        if (this.sourceType == SourceType.FILE_COMMA) return ",";
        return null;
    }
    public String getResolvedIngestionIsHeaderEnabledValue() { // Standardized to "Y" for all files
        return isCurrentSourceTypeFileBased() ? "Y" : null;
    }
     public String getResolvedIngestionSaveOutputFlag() {
        return isCurrentSourceTypeFileBased() ? "Y" : null; // Now for FW, Pipe, Comma
    }
    public String getResolvedIngestionTargetPartition() {
         return isCurrentSourceTypeFileBased() ? "utcap_business_effective_date" : null; // Now for FW, Pipe, Comma
    }
    
    // Standardized boolean flags for ingestion
    public String getIngestionIsCreateTargetTableValue() { return "Y"; }
    public String getIngestionIsCacheEnabledValue() { return "Y"; }
    public String getIngestionIsOverWritePartitionValue() { return "Y"; }


    public ObjectNode getPrebuiltIngestionAdditionalReadConfigsNode() {
        if (isCurrentSourceTypeFileBased()) { // FW, Pipe, Comma
            ObjectNode arNode = this.objectMapper.createObjectNode();
            arNode.put("quote", "\"");
            arNode.put("escape", "\"");
            return arNode;
        }
        return null;
    }

    public ObjectNode getPrebuiltIngestionDimcChecksNode() {
        if (!isIncludeDimcChecksInIngestion()) return null;
        try {
            List<Map<String, Object>> selectedChecks = JsonRequirementService.getChosenDimcChecksListInternal(this.chosenDimcCheckIndices, this.dimcChecksMasterList);
            ObjectNode dimcChecksWrapper = this.objectMapper.createObjectNode();
            dimcChecksWrapper.put("isDimcChecksEnabled", DIMC_CHECKS_ENABLED_PLACEHOLDER);
            ArrayNode dimcCheckArray = this.objectMapper.valueToTree(selectedChecks);
            dimcChecksWrapper.set("dimcCheck", dimcCheckArray);
            return dimcChecksWrapper;
        } catch (Exception e) {
            LOG.error("Error building DIMC checks node", e);
            return null;
        }
    }


    // --- Resolved Value Getters for FLOWS (via List of FlowDetailConfig) ---
    public List<FlowDetailConfig> getFlowDetailsList() {
        List<FlowDetailConfig> flowConfigs = new ArrayList<>();
        int numberOfFlows = (this.sourceType == SourceType.MSSQL || this.sourceType == SourceType.ORACLE) ? 2 : 1;
        String ingestionDfName = getResolvedIngestionDataFrameName(); // Use the now standardized _df suffix

        for (int i = 0; i < numberOfFlows; i++) {
            String flowId = String.valueOf(i + 1);
            boolean isFirstFlow = (i == 0);
            String flowZone = SANITIZED_ZONE;
            if (numberOfFlows == 2 && isFirstFlow) flowZone = RAW_ZONE;

            String querySourceForFlow = ingestionDfName; // Default for most that have ingestion
            if (this.sourceType == SourceType.HIVE) { // Hive as source, ingestion is skipped
                // Placeholder for actual source schema, replace if available in JiraStoryIntake
                String sourceHiveDb = this.jiraStoryIntake.getSourceSchema() != null ? this.jiraStoryIntake.getSourceSchema() : "${SOURCE_HIVE_DATABASE}"; 
                querySourceForFlow = sourceHiveDb + "." + this.jiraStoryIntake.getSourceTableName();
            }
            
            String flowDesc = this.sor.getSorName().toUpperCase(Locale.ROOT) + TABLE_TEXT + this.jiraStoryIntake.getTableName() + INGESTION_TO_TEXT + this.jiraStoryIntake.getApplicationName() + " - " + flowZone;
            String query;
            String registerName = this.jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_" + flowZone;
            String flowDatabaseName = "";
            String storageName = "";
            String targetTablePath = null;
            // Standardized hivePartitionColumn
            String hivePartitionColumn = this.jiraStoryIntake.getApplicationName().toLowerCase(Locale.ROOT) + "_business_effective_date";
            
            // Standardized boolean flags for flow
            String isCreateFlowTargetTable = "N";
            String isOverWritePartition = "Y";
            String isCacheEnabledFlow = "Y";
            String sendEmailFlow = "N";
            String saveOutputFlow = "Y";


            if (this.targetType == TargetType.GCP) {
                query = this.sqlScript.getSrcTableQuery() + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                 if (this.sourceType == SourceType.HIVE) {
                     query = "SELECT * FROM " + querySourceForFlow + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                 }
                flowDesc = "Extraction of " + (this.jiraStoryIntake.getSourceTableName() != null ? this.jiraStoryIntake.getSourceTableName() : this.jiraStoryIntake.getTableName()) + " for CDMP processing.";
                registerName = this.jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_extract_df";
                saveOutputFlow = "N";
            } else { // Hive Target
                if (this.sourceType == SourceType.HIVE) {
                     query = "SELECT * FROM " + querySourceForFlow;
                } else if (querySourceForFlow != null) {
                     String baseSelect = "SELECT *";
                     if (isCurrentSourceTypeFileBased()) {
                        baseSelect = "SELECT df.*"; // Alias for audit columns
                        String auditCols = ", '${UTCAP_CRTE_BY_ID}' as utcap_crte_by_id, '${UTCAP_CRTE_DTTM}' as utcap_crte_dttm, '${UTCAP_DELTA_CD}' as utcap_delta_cd, '${UTCAP_ERR_FLG_IND}' as utcap_err_flg_ind, '${UTCAP_ERR_TXT}' as utcap_err_txt, '${UTCAP_RUN_ID}' as utcap_run_id, '${UTCAP_SOR_CD}' as utcap_sor_cd, '${UTCAP_BUSINESS_EFFECTIVE_DATE}' as utcap_business_effective_date";
                        query = baseSelect + auditCols + " FROM " + querySourceForFlow + " df";
                     } else {
                         query = baseSelect + " FROM " + querySourceForFlow;
                     }
                } else { // Fallback if no ingestionDF and not HIVE source (should be rare)
                    query = this.sqlScript.getSrcDataFrameQuery();
                }

                if (isCurrentSourceTypeFileBased()) {
                    storageName = "${TARGET_TABLE_NAME}";
                    targetTablePath = "${EXP_EDL_PATH}/${TARGET_TABLE_NAME}";
                    flowDatabaseName = "${HIVE_TARGET_DBNAME}";
                } else { // DB sources to Hive
                    Map<String, String> storageNameMap = Map.of("CNAPP", "consumer", "UTCAP", "utcap"); // Example
                    String appSubDir = storageNameMap.getOrDefault(this.jiraStoryIntake.getApplicationName(), this.jiraStoryIntake.getApplicationName().toLowerCase());
                    storageName = "/datalake${EDL_ENV}/" + appSubDir + "/" +
                                  this.sor.getDomainName().toLowerCase(Locale.ROOT) + "/" +
                                  this.sor.getSorName().toLowerCase(Locale.ROOT) + "/" +
                                  flowZone + "/" + this.jiraStoryIntake.getTableName();
                    flowDatabaseName = "${ETL_ENV}" + appSubDir + "_" + this.sor.getSorName().toLowerCase() + "_" + flowZone;
                }
            }

            flowConfigs.add(new FlowDetailConfig(
                    flowId, flowDesc, query, registerName, flowDatabaseName, storageName, targetTablePath,
                    hivePartitionColumn, isCreateFlowTargetTable, isOverWritePartition, isCacheEnabledFlow,
                    sendEmailFlow, "N", saveOutputFlow // abortIfEmpty = "N"
            ));
        }
        return flowConfigs;
    }

    // --- Resolved Value Getters for DATA CHECKS ---
    public String getResolvedDataChecksAcDcDatabase() {
        if (!getCheckDetailsList().isEmpty() && this.targetType != TargetType.GCP) {
            if (this.sourceType == SourceType.MSSQL || this.sourceType == SourceType.ORACLE) return "${ETL_ENV}consumer_audit";
            if (this.sourceType == SourceType.FILE_FIXEDWIDTH && "utcap".equalsIgnoreCase(this.jiraStoryIntake.getApplicationName())) return "utcap_curated_mu_audit";
            if ((this.sourceType == SourceType.FILE_PIPE || this.sourceType == SourceType.FILE_COMMA) &&
                "utcap".equalsIgnoreCase(this.jiraStoryIntake.getApplicationName()) && "corelogic".equalsIgnoreCase(this.sor.getSorName())) return "${UTCAP_AUDIT_DB}";
            return this.applicationAuthorizer.getAuditTable();
        }
        return "${AUDIT_DATABASE}";
    }

    public List<CheckDetailConfig> getCheckDetailsList() {
        List<CheckDetailConfig> checkConfigs = new ArrayList<>();
        List<FlowDetailConfig> flows = getFlowDetailsList(); // Rely on already resolved flow details

        if (this.targetType == TargetType.GCP || flows.isEmpty() || !isIncludeDataChecks()) {
            return checkConfigs;
        }

        for (FlowDetailConfig currentFlow : flows) {
            String checkId = currentFlow.getFlowId();
            String checkSourceQueryDataFrame;

            if (this.sourceType == SourceType.HIVE) {
                 String sourceHiveDb = this.jiraStoryIntake.getSourceSchema() != null ? this.jiraStoryIntake.getSourceSchema() : "${SOURCE_HIVE_DATABASE}";
                checkSourceQueryDataFrame = sourceHiveDb + "." + this.jiraStoryIntake.getSourceTableName();
            } else if (getResolvedIngestionDataFrameName() != null) {
                checkSourceQueryDataFrame = getResolvedIngestionDataFrameName();
            } else {
                LOG.warn("Cannot determine source DataFrame for data check for source: {} and flow: {}", this.sourceType, currentFlow.getFlowId());
                continue;
            }

            String checkSourceQuery = "select count(*) from " + checkSourceQueryDataFrame;
            String checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowDatabaseName() + "." + this.jiraStoryIntake.getTableName() +
                                     " where " + currentFlow.getResolvedFlowHivePartitionColumn() + "='${EAPP_DCT_BUSINESS_EFFECTIVE_DATE}'";
            
            if (this.sourceType == SourceType.FILE_FIXEDWIDTH && "demog_exp_e3_load_sanitized".equals(currentFlow.getResolvedFlowRegisterName())) {
                 checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowRegisterName(); // From FW example
            }

            checkConfigs.add(new CheckDetailConfig(
                    checkId, "HHRCNTCHK", "Data Count Check",
                    checkSourceQuery, checkTargetQuery,
                    "Y", "Y", "N", "N", "N" // isFailJobOnError is "N" always
            ));
        }
        return checkConfigs;
    }
    
    public ObjectNode getPrebuiltDataChecksPreChecksNode() {
        if(!isIncludeDataChecks()) return null;

        ObjectNode preChecksNode = this.objectMapper.createObjectNode();
        boolean overallPreCheckEnabled = this.sourceType != SourceType.FILE_FIXEDWIDTH;

        preChecksNode.put("isPreCheckEnabled", overallPreCheckEnabled ? "Y" : "${PRECHECK_ENABLED}");
        ArrayNode preCheckArray = preChecksNode.putArray("preCheck");

        boolean isDbAsSrc = isCurrentSourceTypeDatabase();
        boolean isFileAsSrc = isCurrentSourceTypeFileBased();

        // Using simplified logic from before, adjust Y/N per specific detailed example if needed
        addPreCheckItemToArray(preCheckArray, "ServiceIDCheck", overallPreCheckEnabled ? "Y" : "N", "Y");
        addPreCheckItemToArray(preCheckArray, "TargetDataBaseAvailabilityCheck", overallPreCheckEnabled && this.targetType == TargetType.HIVE ? "Y" : "N", "Y");
        addPreCheckItemToArray(preCheckArray, "TargetTableAvailabilityCheck", overallPreCheckEnabled && this.targetType == TargetType.HIVE ? "Y" : "N", "Y");
        addPreCheckItemToArray(preCheckArray, "TargetTablePathWritableCheck", overallPreCheckEnabled && this.targetType == TargetType.HIVE ? "Y" : "N", "Y");
        addPreCheckItemToArray(preCheckArray, "FlowSeqDependenciesCheck", "N", "N");
        addPreCheckItemToArray(preCheckArray, "SourceDataBaseAvailabilityCheck", overallPreCheckEnabled && isDbAsSrc ? "Y" : "N", "Y");
        addPreCheckItemToArray(preCheckArray, "SourceTableAvailabilityCheck", overallPreCheckEnabled && isDbAsSrc ? "Y" : "N", "Y");
        addPreCheckItemToArray(preCheckArray, "SourceTableEmptyCheck", "N", "N");
        addPreCheckItemToArray(preCheckArray, "SourcePathAvailabilityCheck", overallPreCheckEnabled && isFileAsSrc ? "Y" : "N", "Y");
        addPreCheckItemToArray(preCheckArray, "SourcePathEmptyCheck", "N", "N");
        return preChecksNode;
    }
     private void addPreCheckItemToArray(ArrayNode array, String name, String enabled, String failOnError) {
        ObjectNode item = this.objectMapper.createObjectNode();
        item.put("preCheckName", name);
        item.put("isPreCheckEnabled", enabled);
        item.put("isFailJobOnPreCheckError", failOnError);
        array.add(item);
    }

    // --- Helper methods for internal logic ---
    private boolean isCurrentSourceTypeDatabase() {
        return EnumSet.of(SourceType.TERADATA, SourceType.MSSQL, SourceType.ORACLE, SourceType.HIVE).contains(this.sourceType);
    }

    private boolean isCurrentSourceTypeFileBased() {
         return EnumSet.of(SourceType.FILE_FIXEDWIDTH, SourceType.FILE_PIPE, SourceType.FILE_COMMA).contains(this.sourceType);
    }
}






















            package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.config.JsonGenerationConfig;
import com.wellsfargo.utcap.exception.ConfigurationException;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(JsonRequirementService.class);
    // Constants moved to JsonGenerationConfig where they are used for descriptions etc.
    private static final List<Map<String, Object>> DIMC_CHECKS_MASTER_LIST = new ArrayList<>();

    private final ApplicationAuthRepository applicationAuthRepository;
    private final RequirementRepository requirementRepository;
    private final JsonRequirementRepository jsonRequirementRepository;
    private final JiraStoryIntakeRepository jiraStoryIntakeRepository;
    private final SorRepository sorRepository;
    private final SQLScriptRepository sqlScriptRepository;
    private final ConnJsonRequirementRepository connJsonRequirementRepository;
    private final ObjectMapper objectMapper;

    @Autowired
    public JsonRequirementService(ApplicationAuthRepository applicationAuthRepository,
                                  RequirementRepository requirementRepository,
                                  JsonRequirementRepository jsonRequirementRepository,
                                  JiraStoryIntakeRepository jiraStoryIntakeRepository,
                                  SorRepository sorRepository,
                                  SQLScriptRepository sqlScriptRepository,
                                  ConnJsonRequirementRepository connJsonRequirementRepository,
                                  ObjectMapper objectMapper) {
        this.applicationAuthRepository = applicationAuthRepository;
        this.requirementRepository = requirementRepository;
        this.jsonRequirementRepository = jsonRequirementRepository;
        this.jiraStoryIntakeRepository = jiraStoryIntakeRepository;
        this.sorRepository = sorRepository;
        this.sqlScriptRepository = sqlScriptRepository;
        this.connJsonRequirementRepository = connJsonRequirementRepository;
        this.objectMapper = objectMapper;
        populateDimcChecksMasterList();
    }

    public JsonRequirement saveJson(String requirementId, Object newJson) {
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId)
                .orElse(new JsonRequirement(requirementId, null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public JsonRequirement getOrCreateJsonRequirement(String requirementId) throws JsonProcessingException, ConfigurationException {
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);
        if (existingJsonRequirement.isPresent()) {
            LOG.debug("Returning existing JSON requirement for ID: {}", requirementId);
            return existingJsonRequirement.get();
        }

        LOG.info("Generating new JSON requirement for ID: {}", requirementId);
        JsonGenerationConfig config = buildJsonGenerationConfig(requirementId); // Simpler call now

        ObjectNode rootNode = objectMapper.createObjectNode();
        rootNode.set("application", createApplicationNode(config));

        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);
        return jsonRequirementRepository.save(jsonRequirement);
    }

    /**
     * Fetches raw data and instantiates the "Smart" JsonGenerationConfig.
     * The config object itself now handles most of the resolution logic.
     */
    private JsonGenerationConfig buildJsonGenerationConfig(String requirementId) throws ConfigurationException {
        String jiraStoryId = requirementId.split("_")[0];

        JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
        Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("Sor not found for name: " + jira.getApplicationSorName()));
        ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
        SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

        JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
        JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

        if (sourceTypeEnum == JsonGenerationConfig.SourceType.UNKNOWN)
            throw new ConfigurationException("Unknown Source Type: " + jira.getSorType());
        if (targetTypeEnum == JsonGenerationConfig.TargetType.UNKNOWN)
            throw new ConfigurationException("Unknown Target Type: " + jira.getTargetType());

        // TODO: Logic to determine source-specific chosenDimcCheckIndices based on sourceTypeEnum
        // For now, using a default list that includes common checks from examples
        // This list should ideally come from Jira/User Input or be mapped based on sourceType
        List<Integer> chosenDimcCheckIndices = determineSourceSpecificDimcIndices(sourceTypeEnum);


        // Instantiate the "Smart Config" - it will do the heavy lifting with its internal resolvers
        return new JsonGenerationConfig(
                jira, sor, auth, sqlScript,
                sourceTypeEnum, targetTypeEnum,
                chosenDimcCheckIndices,
                DIMC_CHECKS_MASTER_LIST, // Pass the master list
                this.objectMapper // Pass objectMapper for node creation within config
        );
    }
    
    // Example helper for source-specific DIMC indices - to be expanded
    private List<Integer> determineSourceSpecificDimcIndices(JsonGenerationConfig.SourceType sourceType) {
        // Default common checks (indices 0-3 are common file checks, 4-5 are common DB checks)
        // This needs to be mapped properly based on your actual DIMC check strategy per source
        switch (sourceType) {
            case FILE_FIXEDWIDTH:
                // FW example had: dataVolumeCompleteness, duplicateFileLoad, dataVolumeConsistency, missingFile, delivery, fileDate, businessDate
                // This implies a more complex mapping than just simple indices if using one master list.
                // For now, returning a common set.
                return Arrays.asList(0,1,2,3); // Example: common file checks
            case FILE_PIPE:
            case FILE_COMMA:
                return Arrays.asList(0,1,2,3); // Example: common file checks
            case TERADATA:
            case HIVE: // Assuming HIVE as source uses similar DB checks
                return Arrays.asList(1,3,4,5); // Example: some DB and some general checks
            // MSSQL and ORACLE do not have DIMC checks as per user rules.
            default:
                return Collections.emptyList();
        }
    }


    // --- Core JSON Node Creation Methods (Generic Assemblers) ---

    private ObjectNode createApplicationNode(JsonGenerationConfig config) {
        ObjectNode applicationNode = objectMapper.createObjectNode();
        applicationNode.put("isParallelProcessing", "N");
        applicationNode.set("jobs", createJobsNode(config));
        return applicationNode;
    }

    private ObjectNode createJobsNode(JsonGenerationConfig config) {
        ObjectNode jobsNode = objectMapper.createObjectNode();
        jobsNode.set("job", createJobNode(config));
        return jobsNode;
    }

    private ObjectNode createJobNode(JsonGenerationConfig config) {
        ObjectNode jobNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake(); // Still needed for some direct, non-varying fields
        Sor sor = config.getSor();
        ApplicationAuthorizer auth = config.getApplicationAuthorizer();

        jobNode.put("JobName", jira.getTableName());

        Set<String> jobFields = config.getJobLevelFieldsToInclude();
        if (jobFields.contains("app")) jobNode.put("app", jira.getApplicationName());
        if (jobFields.contains("appRemedyId")) jobNode.put("appRemedyId", auth.getAppRemedyId());
        if (jobFields.contains("sorRemedyId")) jobNode.put("sorRemedyId", sor.getRemedyId());
        if (jobFields.contains("queryGroup")) jobNode.put("queryGroup", "dimc_check_" + sor.getSorName().toLowerCase(Locale.ROOT));
        if (jobFields.contains("zone")) jobNode.put("zone", JsonGenerationConfig.SANITIZED_ZONE); // Access constant via class
        if (jobFields.contains("sor")) jobNode.put("sor", sor.getSorName().toLowerCase(Locale.ROOT));

        jobNode.put("jobId", "1");
        jobNode.put("jobDescription", determineJobDescription(config)); // Helper can live here or in config
        jobNode.put("isJobEnabled", "Y");
        jobNode.set("Email", createEmailNode(config));

        if (config.isIncludeIngestions()) {
            ObjectNode ingestionsNode = buildGenericIngestionNode(config);
            if (ingestionsNode != null && ingestionsNode.size() > 0) jobNode.set("ingestions", ingestionsNode);
        }
        if (config.isIncludeFlows()) {
            ObjectNode flowsNode = buildGenericFlowsNode(config);
            if (flowsNode != null && flowsNode.size() > 0) jobNode.set("flows", flowsNode);
        }
        if (config.isIncludeDataChecks()) {
            ObjectNode dataChecksNode = buildGenericDataChecksNode(config);
             if (dataChecksNode != null && dataChecksNode.size() > 0) jobNode.set("dataChecks", dataChecksNode);
        }
        if (config.isIncludeFileExtracts()) {
            ObjectNode fileExtractsNode = buildGenericFileExtractsNode(config);
             if (fileExtractsNode != null && fileExtractsNode.size() > 0) jobNode.set("fileExtracts", fileExtractsNode);
        }
        return jobNode;
    }
    
    private String determineJobDescription(JsonGenerationConfig config){
        // This logic could also live inside JsonGenerationConfig as a resolver method
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            return "CDMP Extract for Hive table - " + jira.getTableName().toLowerCase(Locale.ROOT);
        } else {
            // Using specific examples to guide the description
            String appNameToUse = jira.getApplicationName(); // Default
            String sorNameToUse = sor.getSorName();
            String tableNameToUse = jira.getTableName().toLowerCase(Locale.ROOT);
            String ingestionToText = JsonGenerationConfig.INGESTION_TO_TEXT; // Access constant via class
            String tableText = JsonGenerationConfig.TABLE_TEXT;

            if ("BKPF".equalsIgnoreCase(sorNameToUse)) { // MSSQL example
                 return sorNameToUse + " " + tableNameToUse + ingestionToText + appNameToUse;
            } else if ("Hemi".equalsIgnoreCase(sorNameToUse)) { // FW Example
                return sorNameToUse + tableText + tableNameToUse + ingestionToText + "Hive";
            } else if ("corelogic".equalsIgnoreCase(sorNameToUse) && ("utcap vol_lien_load".equalsIgnoreCase(jira.getTableName()) || "mls_load".equalsIgnoreCase(jira.getTableName()))) { 
                 return "utcap " + sorNameToUse + " ingestion for feed " + jira.getTableName(); // Original had feed name in JobName
            } else if ("BDDM".equalsIgnoreCase(sorNameToUse)){ // Oracle Example
                return sorNameToUse + tableText + tableNameToUse + ingestionToText + appNameToUse;
            }
            // Default for Teradata and others
            return sorNameToUse + tableText + tableNameToUse + ingestionToText + appNameToUse;
        }
    }

    private ObjectNode createEmailNode(JsonGenerationConfig config) {
        // This logic is simple enough to remain here, or be a series of calls to config resolvers
        ObjectNode emailNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();

        String fromAddress = "${EMAIL_TO_LIST}";
        String toAddress = "${EMAIL_CC_LIST}";
        String subject = "Data Extract from " + sor.getSorName().toUpperCase(Locale.ROOT) +
                         " to " + jira.getApplicationName() + " - " +
                         jira.getTableName().toLowerCase(Locale.ROOT);

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            fromAddress = "${FROM_EMAIL}"; toAddress = "${TO_EMAIL}";
            subject = "CDMP Extraction - " + jira.getTableName().toLowerCase(Locale.ROOT);
        } else if (config.getSourceType() == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) {
            fromAddress = "${EMAIL_FROM_LIST}"; // From FW example
            subject = "Fixed width UTCAP Demog Feed ingestion " + jira.getTableName() + " hive table";
        } else if (config.getSourceType() == JsonGenerationConfig.SourceType.FILE_PIPE || config.getSourceType() == JsonGenerationConfig.SourceType.FILE_COMMA) {
            if("corelogic".equalsIgnoreCase(sor.getSorName())) { // From CSV/Pipe examples
                 subject = "utcap ingestion for feed " + jira.getTableName();
            }
        }

        emailNode.put("fromAddress", fromAddress);
        emailNode.put("toAddress", toAddress);
        emailNode.put("subject", subject);
        emailNode.put("isPriorityAlert", "Y");
        return emailNode;
    }

    // --- Generic Section Builders (Assemblers) ---
    private ObjectNode buildGenericIngestionNode(JsonGenerationConfig config) {
        Set<String> fields = config.getIngestionFieldsToInclude();
        if (fields.isEmpty()) return null;

        ObjectNode ingestionItemNode = objectMapper.createObjectNode();

        // Static or directly resolved values
        if (fields.contains("ingestionId")) ingestionItemNode.put("ingestionId", "1");
        if (fields.contains("ingestionDesc")) ingestionItemNode.put("ingestionDesc", determineIngestionDescription(config)); // Helper
        if (fields.contains("isIngestionEnabled")) ingestionItemNode.put("isIngestionEnabled", "Y");
        if (fields.contains("sourceType")) ingestionItemNode.put("sourceType", config.getResolvedIngestionSourceTypeValue());
        if (fields.contains("ConnectionHeader") && config.getResolvedIngestionConnectionHeaderValue() != null)
            ingestionItemNode.put("ConnectionHeader", config.getResolvedIngestionConnectionHeaderValue());
        if (fields.contains("delimiter") && config.getResolvedIngestionDelimiter() != null)
            ingestionItemNode.put("delimiter", config.getResolvedIngestionDelimiter());
        if (fields.contains("dataFrameName")) ingestionItemNode.put("dataFrameName", config.getResolvedIngestionDataFrameName());
        
        // Standardized boolean flags
        if (fields.contains("isCreateTargetTable")) ingestionItemNode.put("isCreateTargetTable", config.getIngestionIsCreateTargetTableValue());
        if (fields.contains("isCacheEnabled")) ingestionItemNode.put("isCacheEnabled", config.getIngestionIsCacheEnabledValue());
        if (fields.contains("isOverWritePartition")) ingestionItemNode.put("isOverWritePartition", config.getIngestionIsOverWritePartitionValue());
        
        if (fields.contains("abortIfDuplicate")) ingestionItemNode.put("abortIfDuplicate", "N");
        if (fields.contains("abortIfEmpty")) ingestionItemNode.put("abortIfEmpty", "N");
        if (fields.contains("isRunEMHReport")) ingestionItemNode.put("isRunEMHReport", "N");

        // Source definition fields
        if (fields.contains("sourceSQLQuery") && config.getResolvedIngestionSourceSQLQuery() != null)
            ingestionItemNode.put("sourceSQLQuery", config.getResolvedIngestionSourceSQLQuery());
        if (fields.contains("sourceName") && config.getResolvedIngestionSourceName() != null)
            ingestionItemNode.put("sourceName", config.getResolvedIngestionSourceName());
        
        // File specific fields
        if (fields.contains("dataFilePath") && config.getResolvedIngestionDataFilePath() != null)
            ingestionItemNode.put("dataFilePath", config.getResolvedIngestionDataFilePath());
        if (fields.contains("schemaFilePath") && config.getResolvedIngestionSchemaFilePath() != null)
            ingestionItemNode.put("schemaFilePath", config.getResolvedIngestionSchemaFilePath());
        if (fields.contains("stagingFilePath") && config.getResolvedIngestionStagingFilePath() != null)
            ingestionItemNode.put("stagingFilePath", config.getResolvedIngestionStagingFilePath());
        if (fields.contains("isHeaderEnabled") && config.getResolvedIngestionIsHeaderEnabled() != null)
            ingestionItemNode.put("isHeaderEnabled", config.getResolvedIngestionIsHeaderEnabled());
        if (fields.contains("saveOutput") && config.getResolvedIngestionSaveOutputFlag() != null)
            ingestionItemNode.put("saveOutput", config.getResolvedIngestionSaveOutputFlag());
        if (fields.contains("targetPartition") && config.getResolvedIngestionTargetPartition() != null)
            ingestionItemNode.put("targetPartition", config.getResolvedIngestionTargetPartition());
        if (fields.contains("targetType") && config.getTargetType() == JsonGenerationConfig.TargetType.HIVE)
             ingestionItemNode.put("targetType", "parquet");


        // Pre-built complex nodes
        if (fields.contains("additionalReadConfigs") && config.getPrebuiltIngestionAdditionalReadConfigsNode() != null)
            ingestionItemNode.set("additionalReadConfigs", config.getPrebuiltIngestionAdditionalReadConfigsNode());
        if (fields.contains("dimcChecks") && config.isIncludeDimcChecksInIngestion() && config.getPrebuiltIngestionDimcChecksNode() != null)
            ingestionItemNode.set("dimcChecks", config.getPrebuiltIngestionDimcChecksNode());
        
        ObjectNode finalIngestionsNode = objectMapper.createObjectNode();
        if(ingestionItemNode.size() > 0) {
             addToArray(finalIngestionsNode, "ingestion", ingestionItemNode);
             return finalIngestionsNode;
        }
        return null;
    }
        
    private String determineIngestionDescription(JsonGenerationConfig config){
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();
        String appName = jira.getApplicationName();
        String sorName = sor.getSorName();
        String tableName = jira.getTableName();

        // Default pattern based on Teradata example
        String desc = sorName.toUpperCase() + JsonGenerationConfig.TABLE_TEXT + tableName.toLowerCase() + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
        
        // Example-based overrides
        if ("BKPF".equalsIgnoreCase(sorName)) desc = sorName + " " + tableName + " ingestion"; // MSSQL example
        else if ("HEMI".equalsIgnoreCase(sorName)) desc = sorName.toUpperCase() + JsonGenerationConfig.TABLE_TEXT + tableName + " ingestion to EDL"; // FW example
        else if ("corelogic".equalsIgnoreCase(sorName)) desc = tableName + " ingestion to utcap sanitized"; // CSV/Pipe Example
        else if ("BDDM".equalsIgnoreCase(sorName)) desc = sorName + JsonGenerationConfig.TABLE_TEXT + tableName + JsonGenerationConfig.INGESTION_TO_TEXT + appName; // Oracle example is like default
        
        return desc;
    }


    private ObjectNode buildGenericFlowsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFlows() || config.getFlowDetailsList().isEmpty()) return null;

        ObjectNode flowsNode = objectMapper.createObjectNode();
        ArrayNode flowArray = flowsNode.putArray("flow");

        for (JsonGenerationConfig.FlowDetailConfig flowDetail : config.getFlowDetailsList()) {
            ObjectNode flowItemNode = objectMapper.createObjectNode();
            if(flowDetail.getFlowId() != null) flowItemNode.put("flowId", flowDetail.getFlowId());
            if(flowDetail.getResolvedFlowDescription() != null) flowItemNode.put("flowDesc", flowDetail.getResolvedFlowDescription());
            flowItemNode.put("isFlowEnabled", "Y"); 
            if(flowDetail.getResolvedFlowQuery() != null) flowItemNode.put("query", flowDetail.getResolvedFlowQuery());
            if(flowDetail.getResolvedFlowRegisterName() != null) flowItemNode.put("registerName", flowDetail.getResolvedFlowRegisterName());
            
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) {
                if(flowDetail.getSaveOutputValue() != null) flowItemNode.put("saveOutput", flowDetail.getSaveOutputValue());
                if(flowDetail.getResolvedFlowDatabaseName()!=null) flowItemNode.put("flowDataBaseName", flowDetail.getResolvedFlowDatabaseName());
                if(flowDetail.getResolvedFlowStorageName()!=null) flowItemNode.put("storageName", flowDetail.getResolvedFlowStorageName());
                if(flowDetail.getResolvedFlowTargetTablePath()!=null) flowItemNode.put("targetTablePath", flowDetail.getResolvedFlowTargetTablePath());
                if(flowDetail.getResolvedFlowHivePartitionColumn()!=null) flowItemNode.put("hivePartitionColumn", flowDetail.getResolvedFlowHivePartitionColumn());
                if(flowDetail.getIsCreateFlowTargetTableValue()!=null) flowItemNode.put("isCreateFlowTargetTable", flowDetail.getIsCreateFlowTargetTableValue());
                if(flowDetail.getIsOverWritePartitionValue()!=null) flowItemNode.put("isOverWritePartition", flowDetail.getIsOverWritePartitionValue());
                if(flowDetail.getIsCacheEnabledValue()!=null) flowItemNode.put("isCacheEnabled", flowDetail.getIsCacheEnabledValue());
                if(flowDetail.getSendEmailValue()!=null) flowItemNode.put("sendEmail", flowDetail.getSendEmailValue());
                if(flowDetail.getAbortIfEmptyValue()!=null) flowItemNode.put("abortIfEmpty", flowDetail.getAbortIfEmptyValue());
            }
            flowArray.add(flowItemNode);
        }
        return flowsNode;
    }

    private ObjectNode buildGenericDataChecksNode(JsonGenerationConfig config) {
        if (!config.isIncludeDataChecks()) return null;
        boolean hasCheckDetails = !config.getCheckDetailsList().isEmpty();
        boolean hasPreChecks = config.getPrebuiltDataChecksPreChecksNode() != null && 
                               config.getPrebuiltDataChecksPreChecksNode().has("preCheck") && 
                               config.getPrebuiltDataChecksPreChecksNode().get("preCheck").size() > 0;

        // Only create dataChecks node if there's something to put in it, beyond default placeholders
        if (!hasCheckDetails && !hasPreChecks) {
             // If acDCDatabase is also default placeholder, then skip entire node for GCP
            if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP && 
                "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase())) {
                return null;
            }
             // For Hive, if only default acDCDatabase and isTrendingEnabled would be present, skip.
             if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE && 
                "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase()) ) {
                 return null;
             }
        }

        ObjectNode dataChecksNode = objectMapper.createObjectNode();
        if(config.getResolvedDataChecksAcDcDatabase() != null) {
             dataChecksNode.put("acDCDatabase", config.getResolvedDataChecksAcDcDatabase());
        }
        dataChecksNode.put("isTrendingEnabled", "N"); 

        if (hasCheckDetails) {
            ArrayNode checkArray = dataChecksNode.putArray("check"); 
            for (JsonGenerationConfig.CheckDetailConfig detail : config.getCheckDetailsList()) {
                ObjectNode checkItemNode = objectMapper.createObjectNode();
                // Add keys only if they are expected for this check type or have non-default values
                checkItemNode.put("checkId", detail.getCheckId());
                checkItemNode.put("checkName", detail.getCheckName());
                checkItemNode.put("checkDesc", detail.getCheckDescription());
                checkItemNode.put("isCheckEnabled", detail.getIsCheckEnabledValue());
                checkItemNode.put("isUserSupplyQuery", detail.getIsUserSupplyQueryValue());
                checkItemNode.put("readSourceQueryFromPath", detail.getReadSourceQueryFromPathValue());
                checkItemNode.put("sourceQuery", detail.getResolvedCheckSourceQuery());
                checkItemNode.put("targetQuery", detail.getResolvedCheckTargetQuery());
                checkItemNode.put("readTargetQueryFromPath", detail.getReadTargetQueryFromPathValue());
                checkItemNode.put("isFailJobOnDataCheckError", detail.getIsFailJobOnDataCheckErrorValue());
                checkArray.add(checkItemNode);
            }
        }
        
        if(hasPreChecks) { 
            dataChecksNode.set("preChecks", config.getPrebuiltDataChecksPreChecksNode());
        }
        
        return dataChecksNode.size() > 0 ? dataChecksNode : null;
    }

    private ObjectNode buildGenericFileExtractsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFileExtracts()) return null;
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = fileExtractsNode.putArray("fileExtract");
        JiraStoryIntake jira = config.getJiraStoryIntake();
        ObjectNode fileExtractItemNode = objectMapper.createObjectNode();
        fileExtractItemNode.put("fileExtractId", "1");
        fileExtractItemNode.put("isFileExtractEnabled", "${IS_FILE_EXTRACT_ENABLED}"); // From Hive-GCP example
        fileExtractItemNode.put("fileDescription", "Extracting " + (config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "hive data" : jira.getTableName().toLowerCase(Locale.ROOT)) + " to avro file for CDMP");
        fileExtractItemNode.put("subjectArea", config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "cdmp using FileExtract" : "CDMP");
        fileExtractItemNode.put("zone", RAW_ZONE);
        fileExtractItemNode.put("system", "cdmp");
        fileExtractItemNode.put("fileType", "avro");
        fileExtractItemNode.put("extractQuery", "select * from " + (config.getFlowDetailsList().isEmpty() ? "flow_df" : config.getFlowDetailsList().get(0).getResolvedFlowRegisterName())); // Use flow's registerName
        fileExtractItemNode.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        fileExtractItemNode.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        ObjectNode cdmpParamsNode = objectMapper.createObjectNode();
        cdmpParamsNode.put("outputFormat", "avro");
        cdmpParamsNode.put("cdmpFileName", (config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "UTCAPT_" : "") + jira.getTableName().toLowerCase(Locale.ROOT) + "_cdmp_extract");
        cdmpParamsNode.put("cdmpPatternMode", "daily");
        cdmpParamsNode.put("isCustomPartitionRequired", "true"); // "isCustomPartitionRequired" from H->GCP example
        cdmpParamsNode.put("desiredPartFileSize", "${PART_FILE_SIZE}");
        fileExtractItemNode.set("cdmpAdditionalParams", cdmpParamsNode);
        fileExtractArray.add(fileExtractItemNode);
        return fileExtractsNode;
    }

    private static synchronized void populateDimcChecksMasterList() {
        if (!DIMC_CHECKS_MASTER_LIST.isEmpty()) return;
        LOG.info("Populating DIMC Checks Master List...");
        Map<String, Object> check0 = new HashMap<>(); check0.put("dimcCheckname", "duplicateFileLoadChecks"); check0.put("isDimcCheckEnabled", "${DUPLICATE_FILELOAD_CHECK_ENABLED}"); check0.put("isFailJobOnError", "${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check0));
        Map<String, Object> check1 = new HashMap<>(); check1.put("dimcCheckname", "dataVolumeConsistencyCheck"); check1.put("isDimcCheckEnabled", "${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}"); check1.put("upperThreshold", "${UPPER_THRESHOLD}"); check1.put("lowerThreshold", "${LOWER_THRESHOLD}"); check1.put("numPastDays", "${NUM_PAST_DAYS}"); check1.put("isFailJobOnError", "${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check1));
        Map<String, Object> check2 = new HashMap<>(); check2.put("dimcCheckname", "missingFileCheck"); check2.put("isDimcCheckEnabled", "${MISSING_FILE_CHECK_ENABLED}"); check2.put("isFailJobOnError", "${MISSING_FILE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check2));
        Map<String, Object> check3 = new HashMap<>(); check3.put("dimcCheckname", "deliveryCheck"); check3.put("isDimcCheckEnabled", "${DELIVERY_CHECK_ENABLED}"); check3.put("SLA", "${DELIVERY_CHECK_SLA}"); check3.put("isFailJobOnError", "${DELIVERY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check3));
        Map<String, Object> check4 = new HashMap<>(); check4.put("dimcCheckname", "missingDbObjectCheck"); check4.put("isDimcCheckEnabled", "${MISSING_DB_OBJECT_CHECK_ENABLED}"); check4.put("isFailJobOnError", "${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check4));
        Map<String, Object> check5 = new HashMap<>(); check5.put("dimcCheckname", "emptySourceCheck"); check5.put("isDimcCheckEnabled", "${EMPTY_SOURCE_CHECK_ENABLED}"); check5.put("isFailJobOnError", "${EMPTY_SOURCE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check5));
        // Added checks from FixedWidth Example
        Map<String, Object> check6 = new HashMap<>(); check6.put("dimcCheckname", "dataVolumeCompletenessCheck"); check6.put("controlFileRowFilterByIndex", "/apps/ndm/etlutcap/inbound/experian.ctl"); check6.put("controlFileFieldValueByIndex", "recordCount=3,fileDate=1"); check6.put("isDimcCheckEnabled", "N"); check6.put("isFailJobOnError", "N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check6));
        Map<String, Object> check7 = new HashMap<>(); check7.put("dimcCheckname", "fileDateCheck"); check7.put("isDimcCheckEnabled", "${FILE_DATE_CHECK_ENABLED}"); check7.put("isFailJobOnError", "${FILE_DATE_CHECK_FAILURE_FLAG}"); check7.put("fileDateFromFileNamePattern", ".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check7.put("fileDateFromFileNameFormat", "yyyyMMdd"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check7));
        Map<String, Object> check8 = new HashMap<>(); check8.put("dimcCheckname", "businessDateCheck"); check8.put("businessDateFromFileNamePattern", ".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check8.put("businessDateFromFileNameFormat", "yyyyMMdd"); check8.put("upperThresholdInDays", "20"); check8.put("lowerThresholdInDays", "20"); check8.put("controlFileRowFilterByIndex", "2"); check8.put("controlFileFieldValueByIndex", "fileDate=1"); check8.put("isDimcCheckEnabled", "N"); check8.put("isFailJobOnError", "N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check8));
        LOG.info("DIMC Checks Master List populated with {} items.", DIMC_CHECKS_MASTER_LIST.size());
    }

    // Made this static and internal to JsonRequirementService as it uses the static master list
    public static List<Map<String, Object>> getChosenDimcChecksListInternal(List<Integer> chosenIndices, List<Map<String, Object>> masterList) {
        if (chosenIndices == null || chosenIndices.isEmpty() || masterList == null || masterList.isEmpty()) return Collections.emptyList();
        return chosenIndices.stream()
                .filter(index -> index != null && index >= 0 && index < masterList.size())
                .map(index -> new HashMap<>(masterList.get(index)))
                .collect(Collectors.toList());
    }
    
    private JsonGenerationConfig.SourceType parseSourceType(String sorTypeStr) {
        // (Same as before, ensure all sources are mapped)
        if (sorTypeStr == null) return JsonGenerationConfig.SourceType.UNKNOWN;
        String typeLower = sorTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "teradata": return JsonGenerationConfig.SourceType.TERADATA;
            case "file-fixedwidth": case "fixedwidthfile": return JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
            case "file-pipe": return JsonGenerationConfig.SourceType.FILE_PIPE;
            case "file-comma": return JsonGenerationConfig.SourceType.FILE_COMMA;
            case "mssql": return JsonGenerationConfig.SourceType.MSSQL;
            case "oracle": return JsonGenerationConfig.SourceType.ORACLE;
            case "hive": return JsonGenerationConfig.SourceType.HIVE;
            case "file": 
                 LOG.warn("Generic 'file' sourceType string found. Defaulting to Comma. For specific behavior, ensure input sorType is 'file-comma', 'file-pipe', or 'file-fixedwidth'. Input: {}", sorTypeStr);
                 return JsonGenerationConfig.SourceType.FILE_COMMA;
            default:
                LOG.warn("Unrecognized Source Type string: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.UNKNOWN;
        }
    }

    private JsonGenerationConfig.TargetType parseTargetType(String targetTypeStr) {
        // (Same as before)
        if (targetTypeStr == null) return JsonGenerationConfig.TargetType.UNKNOWN;
        String typeLower = targetTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "hive": return JsonGenerationConfig.TargetType.HIVE;
            case "gcp": return JsonGenerationConfig.TargetType.GCP;
            default:
                LOG.warn("Unrecognized Target Type string: {}", targetTypeStr);
                return JsonGenerationConfig.TargetType.UNKNOWN;
        }
    }

    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) {
        if (item == null || item.size() == 0) return;
        ArrayNode array = parent.has(arrayName) && parent.get(arrayName).isArray() ?
                (ArrayNode) parent.get(arrayName) : parent.putArray(arrayName);
        array.add(item);
    }

     public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) {
         // (Same as before)
         Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository.findById(requirementId);
         if (existingConnJsonRequirement.isPresent()) {
             return existingConnJsonRequirement.get();
         }
         ObjectNode rootNode = objectMapper.createObjectNode();
         ObjectNode application = objectMapper.createObjectNode();
         ObjectNode email = objectMapper.createObjectNode();
         String sorName;
         try {
             String baseRequirementId = requirementId.contains("_") ? requirementId.split("_")[0] : requirementId;
             List<Requirement> reqs = requirementRepository.findByRequirementId(baseRequirementId);
             if (reqs.isEmpty()) throw new ConfigurationException("Requirement not found in RequirementRepository for: " + baseRequirementId);
             sorName = reqs.get(0).getSorName();
         } catch (Exception e) {
             LOG.error("Failed to get SorName for ConnJson requirementId: {}. Error: {}", requirementId, e.getMessage(), e);
             sorName = "UNKNOWN_SOR";
         }
         String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName + " with Job Name $jobName has completed. " +
                           "</p><p> SourceDate =$sourceDate , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
         email.put("body", emailBody);
         ObjectNode job = objectMapper.createObjectNode();
         job.set("Email", email);
         application.set("job", job);
         rootNode.set("application", application);
         Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
         ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
         return connJsonRequirementRepository.save(connJsonRequirement);
    }
}
