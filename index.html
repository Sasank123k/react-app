package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.config.JsonGenerationConfig;
import com.wellsfargo.utcap.exception.ConfigurationException;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(JsonRequirementService.class);
    private static final List<Map<String, Object>> DIMC_CHECKS_MASTER_LIST = new ArrayList<>();

    private final ApplicationAuthRepository applicationAuthRepository;
    private final RequirementRepository requirementRepository;
    private final JsonRequirementRepository jsonRequirementRepository;
    private final JiraStoryIntakeRepository jiraStoryIntakeRepository;
    private final SorRepository sorRepository;
    private final SQLScriptRepository sqlScriptRepository;
    private final ConnJsonRequirementRepository connJsonRequirementRepository;
    private final ObjectMapper objectMapper;

    @Autowired
    public JsonRequirementService(ApplicationAuthRepository applicationAuthRepository,
                                  RequirementRepository requirementRepository,
                                  JsonRequirementRepository jsonRequirementRepository,
                                  JiraStoryIntakeRepository jiraStoryIntakeRepository,
                                  SorRepository sorRepository,
                                  SQLScriptRepository sqlScriptRepository,
                                  ConnJsonRequirementRepository connJsonRequirementRepository,
                                  ObjectMapper objectMapper) {
        this.applicationAuthRepository = applicationAuthRepository;
        this.requirementRepository = requirementRepository;
        this.jsonRequirementRepository = jsonRequirementRepository;
        this.jiraStoryIntakeRepository = jiraStoryIntakeRepository;
        this.sorRepository = sorRepository;
        this.sqlScriptRepository = sqlScriptRepository;
        this.connJsonRequirementRepository = connJsonRequirementRepository;
        this.objectMapper = objectMapper;
        populateDimcChecksMasterList();
    }

    public JsonRequirement saveJson(String requirementId, Object newJson) {
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId)
                .orElse(new JsonRequirement(requirementId, null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public JsonRequirement getOrCreateJsonRequirement(String requirementId) throws JsonProcessingException, ConfigurationException {
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);
        if (existingJsonRequirement.isPresent()) {
            return existingJsonRequirement.get();
        }
        LOG.info("Generating new JSON requirement for ID: {}", requirementId);
        JsonGenerationConfig config = buildJsonGenerationConfig(requirementId); // This is the main orchestrator now
        ObjectNode rootNode = objectMapper.createObjectNode();
        // createApplicationNode is a helper within this service
        rootNode.set("application", createApplicationNode(config)); 
        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);
        return jsonRequirementRepository.save(jsonRequirement);
    }

    // --- Orchestrator for building the "Smart" Config's content ---
    private JsonGenerationConfig buildJsonGenerationConfig(String requirementId) throws ConfigurationException, JsonProcessingException {
        String jiraStoryId = requirementId.split("_")[0];
        JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
        Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("Sor not found for name: " + jira.getApplicationSorName()));
        ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
        SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

        JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
        JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

        if (sourceTypeEnum == JsonGenerationConfig.SourceType.UNKNOWN) throw new ConfigurationException("Unknown Source Type: " + jira.getSorType());
        if (targetTypeEnum == JsonGenerationConfig.TargetType.UNKNOWN) throw new ConfigurationException("Unknown Target Type: " + jira.getTargetType());

        boolean includeIngestions = (sourceTypeEnum != JsonGenerationConfig.SourceType.HIVE);
        boolean includeFlows = true;
        boolean includeDataChecks = true;
        boolean includeFileExtracts = (targetTypeEnum == JsonGenerationConfig.TargetType.GCP);

        Set<String> jobLevelFieldsToInclude = determineJobLevelFieldsToInclude(targetTypeEnum);
        Set<String> ingestionFieldsToInclude = determineIngestionFieldsToInclude(sourceTypeEnum, includeIngestions, targetTypeEnum);

        String resolvedIngestionSourceTypeValue = includeIngestions ? resolveIngestionSourceTypeValue(sourceTypeEnum) : null;
        String resolvedIngestionConnectionHeaderValue = includeIngestions ? resolveIngestionConnectionHeaderValue(sourceTypeEnum) : null;
        String resolvedIngestionDataFrameName = includeIngestions ? resolveIngestionDataFrameName(jira.getTableName()) : null;
        
        String resolvedIngestionSourceSQLQuery = null;
        if (includeIngestions && isDatabaseSource(sourceTypeEnum) && sqlScript != null) {
             resolvedIngestionSourceSQLQuery = sqlScript.getSrcTableQuery();
        }
        String resolvedIngestionSourceName = (includeIngestions && sourceTypeEnum == JsonGenerationConfig.SourceType.ORACLE) ?
                "${BDDMCBD_SRC_SCHEMA}." + jira.getTableName() : null;

        String resolvedIngestionDataFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ? "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}" : null;
        String resolvedIngestionSchemaFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ?
                "${DCI_CONFIG_DIR_PATH}/config/" + sor.getSorName().toLowerCase() + "/${FEED_NAME}/metadata/" + jira.getTableName().toLowerCase() + ".schema" : null;
        String resolvedIngestionStagingFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ?
                determineStagingFilePath(sourceTypeEnum, sor, jira) : null;
        String resolvedIngestionDelimiter = includeIngestions ? resolveIngestionDelimiter(sourceTypeEnum) : null;
        String resolvedIngestionIsHeaderEnabledValue = (includeIngestions && isFileSource(sourceTypeEnum)) ? "Y" : null;
        
        String resolvedIngestionSaveOutputFlag = (includeIngestions && isFileSource(sourceTypeEnum)) ? "Y" : null;
        String resolvedIngestionTargetPartition = (includeIngestions && isFileSource(sourceTypeEnum)) ? "utcap_business_effective_date" : null;
        
        String ingestionIsCreateTargetTableValue = includeIngestions ? "Y" : null;
        String ingestionIsCacheEnabledValue = includeIngestions ? "Y" : null;
        String ingestionIsOverWritePartitionValue = includeIngestions ? "Y" : null;

        ObjectNode prebuiltIngestionAdditionalReadConfigsNode = includeIngestions ?
                buildPrebuiltIngestionAdditionalReadConfigs(sourceTypeEnum, objectMapper) : null;
        
        boolean includeDimcChecksForThisSource = includeIngestions &&
                (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE).contains(sourceTypeEnum) ||
                 isFileSource(sourceTypeEnum));
        List<Integer> chosenDimcCheckIndices = includeDimcChecksForThisSource ? determineSourceSpecificDimcIndices(sourceTypeEnum) : Collections.emptyList();
        ObjectNode prebuiltIngestionDimcChecksNode = includeDimcChecksForThisSource ?
                buildPrebuiltDimcChecksNode(chosenDimcCheckIndices, objectMapper) : null;

        List<JsonGenerationConfig.FlowDetailConfig> flowDetailsList = includeFlows ?
                prepareFlowDetailsList(sourceTypeEnum, targetTypeEnum, jira, sor, auth, sqlScript, resolvedIngestionDataFrameName) : Collections.emptyList();
        
        List<JsonGenerationConfig.CheckDetailConfig> checkDetailsList = includeDataChecks ?
                prepareCheckDetailsList(sourceTypeEnum, targetTypeEnum, jira, auth, flowDetailsList, resolvedIngestionDataFrameName) : Collections.emptyList();
        
        String resolvedDataChecksAcDcDatabase = includeDataChecks ?
                determineDataChecksAcDcDatabase(sourceTypeEnum, targetTypeEnum, auth, sor, jira, !checkDetailsList.isEmpty()) : null;
        ObjectNode prebuiltDataChecksPreChecksNode = includeDataChecks ?
                buildPrebuiltDataChecksPreChecksNode(sourceTypeEnum, objectMapper) : null;

        return new JsonGenerationConfig(
                jira, sor, auth, sqlScript, sourceTypeEnum, targetTypeEnum,
                includeIngestions, includeFlows, includeDataChecks, includeFileExtracts,
                jobLevelFieldsToInclude, ingestionFieldsToInclude,
                resolvedIngestionSourceTypeValue, resolvedIngestionConnectionHeaderValue,
                resolvedIngestionDataFrameName, resolvedIngestionSourceSQLQuery,
                resolvedIngestionSourceName, resolvedIngestionDataFilePath,
                resolvedIngestionSchemaFilePath, resolvedIngestionStagingFilePath,
                resolvedIngestionDelimiter, resolvedIngestionIsHeaderEnabledValue,
                resolvedIngestionSaveOutputFlag, resolvedIngestionTargetPartition,
                ingestionIsCreateTargetTableValue, ingestionIsCacheEnabledValue, ingestionIsOverWritePartitionValue,
                prebuiltIngestionAdditionalReadConfigsNode, prebuiltIngestionDimcChecksNode,
                flowDetailsList,
                resolvedDataChecksAcDcDatabase, checkDetailsList, prebuiltDataChecksPreChecksNode
        );
    }
    
    // --- Helper Methods for buildJsonGenerationConfig (All private within JsonRequirementService) ---
    private boolean isDatabaseSource(JsonGenerationConfig.SourceType sourceType) { return EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.MSSQL, JsonGenerationConfig.SourceType.ORACLE, JsonGenerationConfig.SourceType.HIVE).contains(sourceType); }
    private boolean isFileSource(JsonGenerationConfig.SourceType sourceType) { return EnumSet.of(JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH, JsonGenerationConfig.SourceType.FILE_PIPE, JsonGenerationConfig.SourceType.FILE_COMMA).contains(sourceType); }

    private Set<String> determineJobLevelFieldsToInclude(JsonGenerationConfig.TargetType targetType) {
        if (targetType != JsonGenerationConfig.TargetType.GCP) {
            return new HashSet<>(Arrays.asList("app", "appRemedyId", "sorRemedyId", "queryGroup", "zone", "sor"));
        }
        return Collections.emptySet();
    }
    
    private Set<String> determineIngestionFieldsToInclude(JsonGenerationConfig.SourceType sourceType, boolean includeIngestionsFlag, JsonGenerationConfig.TargetType targetType) {
        if (!includeIngestionsFlag) return Collections.emptySet();
        Set<String> fields = new HashSet<>(Arrays.asList(
            "ingestionId", "ingestionDesc", "isIngestionEnabled", "dataFrameName",
            "isCreateTargetTable", "isCacheEnabled", "isOverWritePartition",
            "abortIfDuplicate", "abortIfEmpty", "isRunEMHReport", "sourceType"
        ));
        if (targetType == JsonGenerationConfig.TargetType.HIVE) fields.add("targetType"); // "parquet"

        if (isDatabaseSource(sourceType)) {
            fields.add("ConnectionHeader"); fields.add("sourceSQLQuery");
            if (sourceType == JsonGenerationConfig.SourceType.ORACLE) fields.add("sourceName");
        } else if (isFileSource(sourceType)) {
            fields.addAll(Arrays.asList("isHeaderEnabled", "dataFilePath", "schemaFilePath", "stagingFilePath", "additionalReadConfigs"));
            if (sourceType != JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) fields.add("delimiter");
            fields.add("saveOutput"); fields.add("targetPartition");
        }
        if (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE).contains(sourceType) || isFileSource(sourceType)) {
            fields.add("dimcChecks");
        }
        return fields;
    }

    private String resolveIngestionSourceTypeValue(JsonGenerationConfig.SourceType sourceType) { /* ... as before ... */ 
        switch (sourceType) {
            case TERADATA: return "teradb"; case FILE_FIXEDWIDTH: return "fixedwidthFile";
            case FILE_PIPE: case FILE_COMMA: return "file"; case MSSQL: return "jdbc";
            case ORACLE: return "oracle"; case HIVE: return "hive"; default: return "unknown";
        }
    }
    private String resolveIngestionConnectionHeaderValue(JsonGenerationConfig.SourceType sourceType) { /* ... as before ... */ 
        switch (sourceType) {
            case TERADATA: return "teradbConn"; case MSSQL: return "jdbcConn";
            case ORACLE: return "oracleConn"; case HIVE: return "hiveConn"; default: return null;
        }
    }
    private String resolveIngestionDataFrameName(String tableNameFromJira) { return tableNameFromJira.toLowerCase(Locale.ROOT) + "_df"; }
    private String determineStagingFilePath(JsonGenerationConfig.SourceType sourceType, Sor sor, JiraStoryIntake jira) { /* ... as before ... */ 
        if (sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "Hemi".equalsIgnoreCase(sor.getSorName()) && "demog_exp_e3_load".equalsIgnoreCase(jira.getTableName())) {
            return "/datalake/utcap/hemi/sanitized";
        }
        return "${EXP_EDL_PATH}";
    }
    private String resolveIngestionDelimiter(JsonGenerationConfig.SourceType sourceType) { /* ... as before ... */ 
        if (sourceType == JsonGenerationConfig.SourceType.FILE_PIPE) return "|";
        if (sourceType == JsonGenerationConfig.SourceType.FILE_COMMA) return ",";
        return null;
    }
    private ObjectNode buildPrebuiltIngestionAdditionalReadConfigs(JsonGenerationConfig.SourceType sourceType, ObjectMapper mapper) { /* ... as before ... */ 
        if (isFileSource(sourceType)) {
            ObjectNode arNode = mapper.createObjectNode();
            arNode.put("quote", "\""); arNode.put("escape", "\""); return arNode;
        }
        return null;
    }
    private ObjectNode buildPrebuiltDimcChecksNode(List<Integer> chosenIndices, ObjectMapper mapper) throws JsonProcessingException { /* ... as before ... */
        List<Map<String, Object>> selectedChecks = getChosenDimcChecksListInternal(chosenIndices);
        ObjectNode dimcChecksWrapper = mapper.createObjectNode();
        dimcChecksWrapper.put("isDimcChecksEnabled", "Y"); // Standardized to "Y"
        ArrayNode dimcCheckArray = mapper.valueToTree(selectedChecks);
        dimcChecksWrapper.set("dimcCheck", dimcCheckArray);
        return dimcChecksWrapper;
    }
    
    private List<Integer> determineSourceSpecificDimcIndices(JsonGenerationConfig.SourceType sourceType) { /* ... as before ... */
        switch (sourceType) {
            case TERADATA: case HIVE: return Arrays.asList(4, 5, 3, 1);
            case FILE_FIXEDWIDTH: return Arrays.asList(6, 0, 1, 2, 3, 7, 8);
            case FILE_PIPE: case FILE_COMMA: return Arrays.asList(0, 1, 2, 3);
            default: return Collections.emptyList();
        }
    }

    private List<JsonGenerationConfig.FlowDetailConfig> prepareFlowDetailsList(
        JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
        JiraStoryIntake jira, Sor sor, ApplicationAuthorizer auth, SQLScript sqlScript, 
        String ingestionDataFrameNameIfAny
    ) { /* ... logic as implemented in previous service ... */ 
        List<JsonGenerationConfig.FlowDetailConfig> flowConfigs = new ArrayList<>();
        int numberOfFlows = (sourceType == JsonGenerationConfig.SourceType.MSSQL || sourceType == JsonGenerationConfig.SourceType.ORACLE) ? 2 : 1;
        String commonHivePartitionColumn = jira.getApplicationName().toLowerCase(Locale.ROOT) + "_business_effective_date";

        for (int i = 0; i < numberOfFlows; i++) {
            String flowId = String.valueOf(i + 1); boolean isFirstFlow = (i == 0);
            String flowZone = JsonGenerationConfig.SANITIZED_ZONE; if (numberOfFlows == 2 && isFirstFlow) flowZone = JsonGenerationConfig.RAW_ZONE;
            String querySourceForFlow = ingestionDataFrameNameIfAny;
            String sourceHiveDb = (jira.getSourceSchema() != null ? jira.getSourceSchema() : "${SOURCE_HIVE_DATABASE}");
            if (sourceType == JsonGenerationConfig.SourceType.HIVE) querySourceForFlow = sourceHiveDb + "." + jira.getSourceTableName();

            String flowDesc = sor.getSorName().toUpperCase(Locale.ROOT) + JsonGenerationConfig.TABLE_TEXT + jira.getTableName() + JsonGenerationConfig.INGESTION_TO_TEXT + jira.getApplicationName() + " - " + flowZone;
            String query; String registerName = jira.getTableName().toLowerCase(Locale.ROOT) + "_" + flowZone;
            String flowDatabaseName = ""; String storageName = ""; String targetTablePath = null;
            String isCreateFlowTargetTable = "N"; String isCacheEnabledFlow = "Y"; String sendEmailFlow = "N"; String saveOutputFlow = "Y"; String isOverWritePartitionFlow = "Y"; String abortIfEmptyFlow = "N";

            if (targetType == JsonGenerationConfig.TargetType.GCP) {
                query = sqlScript.getSrcTableQuery() + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                if (sourceType == JsonGenerationConfig.SourceType.HIVE) query = "SELECT * FROM " + querySourceForFlow + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                flowDesc = "Extraction of " + (jira.getSourceTableName() != null ? jira.getSourceTableName() : jira.getTableName()) + " for CDMP processing.";
                registerName = jira.getTableName().toLowerCase(Locale.ROOT) + "_extract_df";
                saveOutputFlow = "N"; 
            } else { // Hive Target
                if (sourceType == JsonGenerationConfig.SourceType.HIVE) query = "SELECT * FROM " + querySourceForFlow;
                else if (querySourceForFlow != null) {
                     String baseSelect = "SELECT *";
                     if (isFileSource(sourceType)) {
                        baseSelect = "SELECT df.*";
                        String auditCols = ", '${UTCAP_CRTE_BY_ID}' as utcap_crte_by_id, '${UTCAP_CRTE_DTTM}' as utcap_crte_dttm, '${UTCAP_DELTA_CD}' as utcap_delta_cd, '${UTCAP_ERR_FLG_IND}' as utcap_err_flg_ind, '${UTCAP_ERR_TXT}' as utcap_err_txt, '${UTCAP_RUN_ID}' as utcap_run_id, '${UTCAP_SOR_CD}' as utcap_sor_cd, '${UTCAP_BUSINESS_EFFECTIVE_DATE}' as utcap_business_effective_date";
                        query = baseSelect + auditCols + " FROM " + querySourceForFlow + " df";
                     } else query = baseSelect + " FROM " + querySourceForFlow;
                } else query = sqlScript.getSrcDataFrameQuery();

                if (isFileSource(sourceType)) {
                    storageName = "${TARGET_TABLE_NAME}"; targetTablePath = "${EXP_EDL_PATH}/${TARGET_TABLE_NAME}";
                    flowDatabaseName = "${HIVE_TARGET_DBNAME}"; 
                } else { 
                    Map<String, String> storageNameMap = Map.of("CNAPP", "consumer", "UTCAP", "utcap");
                    String appSubDir = storageNameMap.getOrDefault(jira.getApplicationName(), jira.getApplicationName().toLowerCase());
                    storageName = "/datalake${EDL_ENV}/" + appSubDir + "/" + sor.getDomainName().toLowerCase(Locale.ROOT) + "/" + sor.getSorName().toLowerCase(Locale.ROOT) + "/" + flowZone + "/" + jira.getTableName();
                    flowDatabaseName = "${ETL_ENV}" + appSubDir + "_" + sor.getSorName().toLowerCase() + "_" + flowZone;
                }
            }
            flowConfigs.add(new JsonGenerationConfig.FlowDetailConfig(flowId, flowDesc, query, registerName, flowDatabaseName, storageName, targetTablePath, commonHivePartitionColumn, isCreateFlowTargetTable, isOverWritePartitionFlow, isCacheEnabledFlow, sendEmailFlow, abortIfEmptyFlow, saveOutputFlow));
        }
        return flowConfigs;
    }
    private List<JsonGenerationConfig.CheckDetailConfig> prepareCheckDetailsList(
        JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
        JiraStoryIntake jira, ApplicationAuthorizer auth,
        List<JsonGenerationConfig.FlowDetailConfig> flowDetails, String ingestionDataFrameName
    ) { /* ... logic as implemented in previous service ... */ 
        List<JsonGenerationConfig.CheckDetailConfig> checkConfigs = new ArrayList<>();
        if (targetType == JsonGenerationConfig.TargetType.GCP || flowDetails.isEmpty()) return checkConfigs;
        for (JsonGenerationConfig.FlowDetailConfig currentFlow : flowDetails) {
            String checkId = currentFlow.getFlowId(); String checkSourceQueryDataFrame;
            String sourceHiveDb = (jira.getSourceSchema() != null ? jira.getSourceSchema() : "${SOURCE_HIVE_DATABASE}");
            if (sourceType == JsonGenerationConfig.SourceType.HIVE) checkSourceQueryDataFrame = sourceHiveDb + "." + jira.getSourceTableName();
            else if (ingestionDataFrameName != null) checkSourceQueryDataFrame = ingestionDataFrameName;
            else { LOG.warn("Cannot determine source DataFrame for data check for source: {} and flow: {}", sourceType, currentFlow.getFlowId()); continue; }
            String checkSourceQuery = "select count(*) from " + checkSourceQueryDataFrame;
            String checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowDatabaseName() + "." + jira.getTableName() + " where " + currentFlow.getResolvedFlowHivePartitionColumn() + "='${EAPP_DCT_BUSINESS_EFFECTIVE_DATE}'";
            if (sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "demog_exp_e3_load_sanitized".equals(currentFlow.getResolvedFlowRegisterName())) {
                 checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowRegisterName();
            }
            checkConfigs.add(new JsonGenerationConfig.CheckDetailConfig(checkId, "HHRCNTCHK", "Data Count Check", checkSourceQuery, checkTargetQuery, "Y", "Y", "N", "N", "N"));
        }
        return checkConfigs;
    }
    private String determineDataChecksAcDcDatabase(JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType, ApplicationAuthorizer auth, Sor sor, JiraStoryIntake jira, boolean checksArePresent) { /* ... same as before ... */
        if (checksArePresent && targetType != JsonGenerationConfig.TargetType.GCP) {
            if(sourceType == JsonGenerationConfig.SourceType.MSSQL || sourceType == JsonGenerationConfig.SourceType.ORACLE) return "${ETL_ENV}consumer_audit";
            if(sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "utcap".equalsIgnoreCase(jira.getApplicationName())) return "utcap_curated_mu_audit";
            if((EnumSet.of(JsonGenerationConfig.SourceType.FILE_PIPE, JsonGenerationConfig.SourceType.FILE_COMMA).contains(sourceType)) &&
                "utcap".equalsIgnoreCase(jira.getApplicationName()) && "corelogic".equalsIgnoreCase(sor.getSorName())) return "${UTCAP_AUDIT_DB}";
            return auth.getAuditTable();
        }
        return "${AUDIT_DATABASE}";
    }
    private ObjectNode buildPrebuiltDataChecksPreChecksNode(JsonGenerationConfig.SourceType sourceType, ObjectMapper mapper) { /* ... same as before ... */ 
        ObjectNode preChecksNode = mapper.createObjectNode();
        boolean overallPreCheckEnabled = sourceType != JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
        preChecksNode.put("isPreCheckEnabled", overallPreCheckEnabled ? "Y" : "${PRECHECK_ENABLED}");
        ArrayNode preCheckArray = preChecksNode.putArray("preCheck");
        boolean isDbAsSrc = isDatabaseSource(sourceType); boolean isFileAsSrc = isFileSource(sourceType);
        addPreCheckItem(preCheckArray, "ServiceIDCheck", overallPreCheckEnabled ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "TargetDataBaseAvailabilityCheck", overallPreCheckEnabled && !isFileAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "TargetTableAvailabilityCheck", overallPreCheckEnabled && !isFileAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "TargetTablePathWritableCheck", overallPreCheckEnabled ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "FlowSeqDependenciesCheck", "N", "N", mapper);
        addPreCheckItem(preCheckArray, "SourceDataBaseAvailabilityCheck", overallPreCheckEnabled && isDbAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "SourceTableAvailabilityCheck", overallPreCheckEnabled && isDbAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "SourceTableEmptyCheck", "N", "N", mapper);
        addPreCheckItem(preCheckArray, "SourcePathAvailabilityCheck", overallPreCheckEnabled && isFileAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "SourcePathEmptyCheck", "N", "N", mapper);
        return preChecksNode;
    }
     private void addPreCheckItem(ArrayNode array, String name, String enabled, String failOnError, ObjectMapper mapper) { /* ... */ 
        ObjectNode item = mapper.createObjectNode();
        item.put("preCheckName", name); item.put("isPreCheckEnabled", enabled); item.put("isFailJobOnPreCheckError", failOnError);
        array.add(item);
    }

    private String determineJobDescription(JsonGenerationConfig config){ /* ... same as before ... */ 
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();
        String appName = jira.getApplicationName(); String sorName = sor.getSorName();
        String tableName = jira.getTableName(); String jobName = jira.getTableName(); 
        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            return "CDMP Extract for Hive table - " + tableName.toLowerCase(Locale.ROOT);
        } else {
            if ("BKPF".equalsIgnoreCase(sorName))  return sorName + " " + jobName + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
            if ("Hemi".equalsIgnoreCase(sorName)) return sorName + JsonGenerationConfig.TABLE_TEXT + jobName + JsonGenerationConfig.INGESTION_TO_TEXT + "Hive";
            if ("corelogic".equalsIgnoreCase(sorName)) return "utcap " + sorName + " ingestion for feed " + jobName; 
            if ("BDDM".equalsIgnoreCase(sorName)) return sorName + JsonGenerationConfig.TABLE_TEXT + jobName + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
            return sorName.toUpperCase() + JsonGenerationConfig.TABLE_TEXT + tableName.toLowerCase(Locale.ROOT) + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
        }
    }
    private ObjectNode createEmailNode(JsonGenerationConfig config) { /* ... same as before ... */ 
        ObjectNode emailNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();
        String fromAddress = "${EMAIL_TO_LIST}"; String toAddress = "${EMAIL_CC_LIST}";
        String subject = "Data Extract from " + sor.getSorName().toUpperCase(Locale.ROOT) + " to " + jira.getApplicationName() + " - " + jira.getTableName().toLowerCase(Locale.ROOT);
        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            fromAddress = "${FROM_EMAIL}"; toAddress = "${TO_EMAIL}"; subject = "CDMP Extraction - " + jira.getTableName().toLowerCase(Locale.ROOT);
        } else if (config.getSourceType() == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) {
            fromAddress = "${EMAIL_FROM_LIST}"; subject = "Fixed width UTCAP Demog Feed ingestion " + jira.getTableName() + " hive table";
        } else if (EnumSet.of(JsonGenerationConfig.SourceType.FILE_PIPE, JsonGenerationConfig.SourceType.FILE_COMMA).contains(config.getSourceType()) && "corelogic".equalsIgnoreCase(sor.getSorName())) {
            subject = "utcap ingestion for feed " + jira.getTableName();
        } else if(config.getSourceType() == JsonGenerationConfig.SourceType.TERADATA && "bmg".equalsIgnoreCase(sor.getSorName())) {
            fromAddress = "${EMAIL_TO_LIST_FL}"; toAddress = "${EMAIL_CC_LIST_FL}"; subject = "Data Extract from " + sor.getSorName().toUpperCase() + " to " + jira.getApplicationName() + "-" + jira.getTableName().toLowerCase();
        }
        emailNode.put("fromAddress", fromAddress); emailNode.put("toAddress", toAddress); emailNode.put("subject", subject); emailNode.put("isPriorityAlert", "Y");
        return emailNode;
    }
    private ObjectNode buildGenericIngestionNode(JsonGenerationConfig config) { /* ... same as before ... */ 
        Set<String> fields = config.getIngestionFieldsToInclude();
        if (fields.isEmpty()) return null;
        ObjectNode item = objectMapper.createObjectNode();
        if (fields.contains("ingestionId")) item.put("ingestionId", "1");
        if (fields.contains("ingestionDesc")) item.put("ingestionDesc", determineIngestionDescription(config)); // Uses helper
        if (fields.contains("isIngestionEnabled")) item.put("isIngestionEnabled", "Y");
        if (fields.contains("sourceType")) item.put("sourceType", config.getResolvedIngestionSourceTypeValue());
        if (fields.contains("ConnectionHeader") && config.getResolvedIngestionConnectionHeaderValue() != null) item.put("ConnectionHeader", config.getResolvedIngestionConnectionHeaderValue());
        if (fields.contains("delimiter") && config.getResolvedIngestionDelimiter() != null) item.put("delimiter", config.getResolvedIngestionDelimiter());
        if (fields.contains("dataFrameName")) item.put("dataFrameName", config.getResolvedIngestionDataFrameName());
        if (fields.contains("isCreateTargetTable")) item.put("isCreateTargetTable", config.getIngestionIsCreateTargetTableValue());
        if (fields.contains("isCacheEnabled")) item.put("isCacheEnabled", config.getIngestionIsCacheEnabledValue());
        if (fields.contains("isOverWritePartition")) item.put("isOverWritePartition", config.getIngestionIsOverWritePartitionValue());
        if (fields.contains("abortIfDuplicate")) item.put("abortIfDuplicate", "N");
        if (fields.contains("abortIfEmpty")) item.put("abortIfEmpty", "N");
        if (fields.contains("isRunEMHReport")) item.put("isRunEMHReport", "N");
        if (fields.contains("sourceSQLQuery") && config.getResolvedIngestionSourceSQLQuery() != null) item.put("sourceSQLQuery", config.getResolvedIngestionSourceSQLQuery());
        if (fields.contains("sourceName") && config.getResolvedIngestionSourceName() != null) item.put("sourceName", config.getResolvedIngestionSourceName());
        if (fields.contains("dataFilePath") && config.getResolvedIngestionDataFilePath() != null) item.put("dataFilePath", config.getResolvedIngestionDataFilePath());
        if (fields.contains("schemaFilePath") && config.getResolvedIngestionSchemaFilePath() != null) item.put("schemaFilePath", config.getResolvedIngestionSchemaFilePath());
        if (fields.contains("stagingFilePath") && config.getResolvedIngestionStagingFilePath() != null) item.put("stagingFilePath", config.getResolvedIngestionStagingFilePath());
        if (fields.contains("isHeaderEnabled") && config.getResolvedIngestionIsHeaderEnabledValue() != null) item.put("isHeaderEnabled", config.getResolvedIngestionIsHeaderEnabledValue());
        if (fields.contains("saveOutput") && config.getResolvedIngestionSaveOutputFlag() != null) item.put("saveOutput", config.getResolvedIngestionSaveOutputFlag());
        if (fields.contains("targetPartition") && config.getResolvedIngestionTargetPartition() != null) item.put("targetPartition", config.getResolvedIngestionTargetPartition());
        if (fields.contains("targetType") && config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) item.put("targetType", "parquet");
        if (fields.contains("additionalReadConfigs") && config.getPrebuiltIngestionAdditionalReadConfigsNode() != null) item.set("additionalReadConfigs", config.getPrebuiltIngestionAdditionalReadConfigsNode());
        if (fields.contains("dimcChecks") && config.getPrebuiltIngestionDimcChecksNode() != null) item.set("dimcChecks", config.getPrebuiltIngestionDimcChecksNode());
        ObjectNode finalNode = objectMapper.createObjectNode();
        if(item.size() > 0) addToArray(finalNode, "ingestion", item);
        return finalNode.size() > 0 ? finalNode : null;
    }
    private String determineIngestionDescription(JsonGenerationConfig config){ /* ... same as before ... */ 
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();
        String appName = jira.getApplicationName(); String sorName = sor.getSorName(); String tableName = jira.getTableName(); String jobName = jira.getTableName();
        String desc = sorName.toUpperCase() + JsonGenerationConfig.TABLE_TEXT + tableName.toLowerCase() + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
        if ("BKPF".equalsIgnoreCase(sorName)) desc = sorName + " " + jobName + " ingestion"; 
        else if ("Hemi".equalsIgnoreCase(sorName)) desc = sorName.toUpperCase() + JsonGenerationConfig.TABLE_TEXT + jobName + " ingestion to EDL"; 
        else if ("corelogic".equalsIgnoreCase(sorName)) desc = jobName + " ingestion to utcap sanitized"; 
        else if ("BDDM".equalsIgnoreCase(sorName)) desc = sorName + JsonGenerationConfig.TABLE_TEXT + jobName + JsonGenerationConfig.INGESTION_TO_TEXT + appName; 
        return desc;
    }
    private ObjectNode buildGenericFlowsNode(JsonGenerationConfig config) { /* ... same as before ... */ 
        if (!config.isIncludeFlows() || config.getFlowDetailsList().isEmpty()) return null;
        ObjectNode flowsNode = objectMapper.createObjectNode();
        ArrayNode flowArray = flowsNode.putArray("flow");
        for (JsonGenerationConfig.FlowDetailConfig detail : config.getFlowDetailsList()) {
            ObjectNode item = objectMapper.createObjectNode();
            item.put("flowId", detail.getFlowId()); item.put("flowDesc", detail.getResolvedFlowDescription()); item.put("isFlowEnabled", "Y");
            item.put("query", detail.getResolvedFlowQuery()); item.put("registerName", detail.getResolvedFlowRegisterName());
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) {
                item.put("saveOutput", detail.getSaveOutputValue());
                if(detail.getResolvedFlowDatabaseName()!=null) item.put("flowDataBaseName", detail.getResolvedFlowDatabaseName());
                if(detail.getResolvedFlowStorageName()!=null) item.put("storageName", detail.getResolvedFlowStorageName());
                if(detail.getResolvedFlowTargetTablePath()!=null) item.put("targetTablePath", detail.getResolvedFlowTargetTablePath());
                if(detail.getResolvedFlowHivePartitionColumn()!=null) item.put("hivePartitionColumn", detail.getResolvedFlowHivePartitionColumn());
                item.put("isCreateFlowTargetTable", detail.getIsCreateFlowTargetTableValue());
                item.put("isOverWritePartition", detail.getIsOverWritePartitionValue());
                item.put("isCacheEnabled", detail.getIsCacheEnabledValue());
                item.put("sendEmail", detail.getSendEmailValue());
                item.put("abortIfEmpty", detail.getAbortIfEmptyValue());
            }
            flowArray.add(item);
        }
        return flowsNode;
    }
    private ObjectNode buildGenericDataChecksNode(JsonGenerationConfig config) { /* ... same as before ... */ 
        if (!config.isIncludeDataChecks()) return null;
        boolean hasCheckDetails = !config.getCheckDetailsList().isEmpty();
        ObjectNode preChecksNodeFromConfig = config.getPrebuiltDataChecksPreChecksNode();
        boolean hasPreChecks = preChecksNodeFromConfig != null && preChecksNodeFromConfig.has("preCheck") && preChecksNodeFromConfig.get("preCheck").size() > 0 ; 
        if (!hasCheckDetails && !hasPreChecks) {
            if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP && "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase())) return null;
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE && "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase()) ) return null;
        }
        ObjectNode dataChecksNode = objectMapper.createObjectNode();
        if(config.getResolvedDataChecksAcDcDatabase() != null) dataChecksNode.put("acDCDatabase", config.getResolvedDataChecksAcDcDatabase());
        dataChecksNode.put("isTrendingEnabled", "N"); 
        if (hasCheckDetails) {
            ArrayNode checkArray = dataChecksNode.putArray("check"); 
            for (JsonGenerationConfig.CheckDetailConfig detail : config.getCheckDetailsList()) {
                ObjectNode item = objectMapper.createObjectNode();
                item.put("checkId", detail.getCheckId()); item.put("checkName", detail.getCheckName()); item.put("checkDesc", detail.getCheckDescription());
                item.put("isCheckEnabled", detail.getIsCheckEnabledValue()); item.put("isUserSupplyQuery", detail.getIsUserSupplyQueryValue());
                item.put("readSourceQueryFromPath", detail.getReadSourceQueryFromPathValue()); item.put("sourceQuery", detail.getResolvedCheckSourceQuery());
                item.put("targetQuery", detail.getResolvedCheckTargetQuery()); item.put("readTargetQueryFromPath", detail.getReadTargetQueryFromPathValue());
                item.put("isFailJobOnDataCheckError", detail.getIsFailJobOnDataCheckErrorValue());
                checkArray.add(item);
            }
        }
        if(hasPreChecks) dataChecksNode.set("preChecks", preChecksNodeFromConfig);
        return dataChecksNode.size() == 0 || (dataChecksNode.size() <=2 && !hasChecks && !hasPreChecks && dataChecksNode.has("acDCDatabase") && dataChecksNode.has("isTrendingEnabled")) ? null : dataChecksNode;
    }
    private ObjectNode buildGenericFileExtractsNode(JsonGenerationConfig config) { /* ... same as before ... */ 
        if (!config.isIncludeFileExtracts()) return null;
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = fileExtractsNode.putArray("fileExtract");
        JiraStoryIntake jira = config.getJiraStoryIntake();
        ObjectNode item = objectMapper.createObjectNode();
        item.put("fileExtractId", "1");
        item.put("isFileExtractEnabled", "${IS_FILE_EXTRACT_ENABLED}"); 
        String sourceDescriptionPart = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "hive data" : jira.getTableName().toLowerCase(Locale.ROOT);
        item.put("fileDescription", "Extracting " + sourceDescriptionPart + " to avro file for CDMP");
        item.put("subjectArea", config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "cdmp using FileExtract" : "CDMP");
        item.put("zone", JsonGenerationConfig.RAW_ZONE);
        item.put("system", "cdmp");
        item.put("fileType", "avro");
        String flowRegisterNameForExtract = config.getFlowDetailsList().isEmpty() ? "flow_df" : config.getFlowDetailsList().get(0).getResolvedFlowRegisterName();
        item.put("extractQuery", "select * from " + flowRegisterNameForExtract); 
        item.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        item.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        ObjectNode cdmpParams = objectMapper.createObjectNode();
        cdmpParams.put("outputFormat", "avro");
        String cdmpFileNamePrefix = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "UTCAPT_" : "";
        cdmpParams.put("cdmpFileName", cdmpFileNamePrefix + jira.getTableName().toLowerCase(Locale.ROOT) + "_cdmp_extract");
        cdmpParams.put("cdmpPatternMode", "daily");
        cdmpParams.put("isCustomPartitionRequired", "true"); 
        cdmpParams.put("desiredPartFileSize", "${PART_FILE_SIZE}");
        item.set("cdmpAdditionalParams", cdmpParams);
        fileExtractArray.add(item);
        return fileExtractsNode;
    }
    
    private static synchronized void populateDimcChecksMasterList() { /* ... same as before ... */ 
        if (!DIMC_CHECKS_MASTER_LIST.isEmpty()) return;
        LOG.info("Populating DIMC Checks Master List...");
        Map<String, Object> check0=new HashMap<>(); check0.put("dimcCheckname","duplicateFileLoadChecks"); check0.put("isDimcCheckEnabled","${DUPLICATE_FILELOAD_CHECK_ENABLED}"); check0.put("isFailJobOnError","${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check0));
        Map<String, Object> check1=new HashMap<>(); check1.put("dimcCheckname","dataVolumeConsistencyCheck"); check1.put("isDimcCheckEnabled","${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}"); check1.put("upperThreshold","${UPPER_THRESHOLD}"); check1.put("lowerThreshold","${LOWER_THRESHOLD}"); check1.put("numPastDays","${NUM_PAST_DAYS}"); check1.put("isFailJobOnError","${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check1));
        Map<String, Object> check2=new HashMap<>(); check2.put("dimcCheckname","missingFileCheck"); check2.put("isDimcCheckEnabled","${MISSING_FILE_CHECK_ENABLED}"); check2.put("isFailJobOnError","${MISSING_FILE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check2));
        Map<String, Object> check3=new HashMap<>(); check3.put("dimcCheckname","deliveryCheck"); check3.put("isDimcCheckEnabled","${DELIVERY_CHECK_ENABLED}"); check3.put("SLA","${DELIVERY_CHECK_SLA}"); check3.put("isFailJobOnError","${DELIVERY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check3));
        Map<String, Object> check4=new HashMap<>(); check4.put("dimcCheckname","missingDbObjectCheck"); check4.put("isDimcCheckEnabled","${MISSING_DB_OBJECT_CHECK_ENABLED}"); check4.put("isFailJobOnError","${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check4));
        Map<String, Object> check5=new HashMap<>(); check5.put("dimcCheckname","emptySourceCheck"); check5.put("isDimcCheckEnabled","${EMPTY_SOURCE_CHECK_ENABLED}"); check5.put("isFailJobOnError","${EMPTY_SOURCE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check5));
        Map<String, Object> check6=new HashMap<>(); check6.put("dimcCheckname","dataVolumeCompletenessCheck"); check6.put("controlFileRowFilterByIndex","/apps/ndm/etlutcap/inbound/experian.ctl"); check6.put("controlFileFieldValueByIndex","recordCount=3,fileDate=1"); check6.put("isDimcCheckEnabled","N"); check6.put("isFailJobOnError","N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check6));
        Map<String, Object> check7=new HashMap<>(); check7.put("dimcCheckname","fileDateCheck"); check7.put("isDimcCheckEnabled","${FILE_DATE_CHECK_ENABLED}"); check7.put("isFailJobOnError","${FILE_DATE_CHECK_FAILURE_FLAG}"); check7.put("fileDateFromFileNamePattern",".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check7.put("fileDateFromFileNameFormat","yyyyMMdd"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check7));
        Map<String, Object> check8=new HashMap<>(); check8.put("dimcCheckname","businessDateCheck"); check8.put("businessDateFromFileNamePattern",".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check8.put("businessDateFromFileNameFormat","yyyyMMdd"); check8.put("upperThresholdInDays","20"); check8.put("lowerThresholdInDays","20"); check8.put("controlFileRowFilterByIndex","2"); check8.put("controlFileFieldValueByIndex","fileDate=1"); check8.put("isDimcCheckEnabled","N"); check8.put("isFailJobOnError","N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check8));
        LOG.info("DIMC Checks Master List populated with {} items.", DIMC_CHECKS_MASTER_LIST.size());
    }
    // Renamed to avoid conflict if JsonGenerationConfig tries to call a static method on itself
    public static List<Map<String, Object>> getChosenDimcChecksListInternal(List<Integer> chosenIndices) {
        if (chosenIndices == null || chosenIndices.isEmpty()) return Collections.emptyList();
        return chosenIndices.stream()
                .filter(index -> index != null && index >= 0 && index < DIMC_CHECKS_MASTER_LIST.size())
                .map(index -> new HashMap<>(DIMC_CHECKS_MASTER_LIST.get(index)))
                .collect(Collectors.toList());
    }
    private JsonGenerationConfig.SourceType parseSourceType(String sorTypeStr) { /* ... same as before ... */ 
        if (sorTypeStr == null) return JsonGenerationConfig.SourceType.UNKNOWN;
        String typeLower = sorTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "teradata": return JsonGenerationConfig.SourceType.TERADATA;
            case "file-fixedwidth": case "fixedwidthfile": return JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
            case "file-pipe": return JsonGenerationConfig.SourceType.FILE_PIPE;
            case "file-comma": return JsonGenerationConfig.SourceType.FILE_COMMA;
            case "mssql": return JsonGenerationConfig.SourceType.MSSQL;
            case "oracle": return JsonGenerationConfig.SourceType.ORACLE;
            case "hive": return JsonGenerationConfig.SourceType.HIVE;
            case "file": 
                 LOG.warn("Generic 'file' sourceType string found. Defaulting to Comma. For specific behavior, ensure input sorType is 'file-comma', 'file-pipe', or 'file-fixedwidth'. Input: {}", sorTypeStr);
                 return JsonGenerationConfig.SourceType.FILE_COMMA; 
            default:
                LOG.warn("Unrecognized Source Type string: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.UNKNOWN;
        }
    }
    private JsonGenerationConfig.TargetType parseTargetType(String targetTypeStr) { /* ... same as before ... */ 
        if (targetTypeStr == null) return JsonGenerationConfig.TargetType.UNKNOWN;
        String typeLower = targetTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "hive": return JsonGenerationConfig.TargetType.HIVE;
            case "gcp": return JsonGenerationConfig.TargetType.GCP;
            default:
                LOG.warn("Unrecognized Target Type string: {}", targetTypeStr);
                return JsonGenerationConfig.TargetType.UNKNOWN;
        }
    }
    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) { /* ... same as before ... */ 
        if (item == null || item.size() == 0) return;
        ArrayNode array = parent.has(arrayName) && parent.get(arrayName).isArray() ?
                (ArrayNode) parent.get(arrayName) : parent.putArray(arrayName);
        array.add(item);
    }
     public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) { /* ... same as before ... */ 
         Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository.findById(requirementId);
         if (existingConnJsonRequirement.isPresent()) {
             return existingConnJsonRequirement.get();
         }
         ObjectNode rootNode = objectMapper.createObjectNode();
         ObjectNode application = objectMapper.createObjectNode();
         ObjectNode email = objectMapper.createObjectNode();
         String sorName;
         try {
             String baseRequirementId = requirementId.contains("_") ? requirementId.split("_")[0] : requirementId;
             List<Requirement> reqs = requirementRepository.findByRequirementId(baseRequirementId);
             if (reqs.isEmpty()) throw new ConfigurationException("Requirement not found in RequirementRepository for: " + baseRequirementId);
             sorName = reqs.get(0).getSorName();
         } catch (Exception e) {
             LOG.error("Failed to get SorName for ConnJson requirementId: {}. Error: {}", requirementId, e.getMessage(), e);
             sorName = "UNKNOWN_SOR";
         }
         String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName + " with Job Name $jobName has completed. " +
                           "</p><p> SourceDate =$sourceDate , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
         email.put("body", emailBody);
         ObjectNode job = objectMapper.createObjectNode();
         job.set("Email", email);
         application.set("job", job);
         rootNode.set("application", application);
         Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
         ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
         return connJsonRequirementRepository.save(connJsonRequirement);
    }
}
