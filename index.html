package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import java.util.*;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(CDMPAdditionalJson.class);
    private static final String SANITIZED = "sanitized";
    private static final String INGESTION_TO = " ingestion to ";
    private static final String TABLE = " Table ";
    private static final List<Map<Object, Object>> DIMC_CHECKS_LIST = new ArrayList<>();

    @Autowired
    public ApplicationAuthRepository applicationAuthRepository;
    @Autowired
    public RequirementRepository requirementRepository;

    @Autowired
    public JsonRequirementRepository jsonRequirementRepository;

    @Autowired
    public JiraStoryIntakeRepository jiraStoryIntakeRepository;

    @Autowired
    public SorRepository sorRepository;

    @Autowired
    public SQLScriptRepository sqlScriptRepository;

    @Autowired
    public ConnJsonRequirementRepository connJsonRequirementRepository;

    public JsonRequirement saveJson(String requirementId, Object newJson) {
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId).orElse(new JsonRequirement(requirementId, null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public static void populateDimcChecksList() {
        //FILE CHECKS
        Map<Object, Object> duplicateFileLoadChecks = new HashMap<>();
        duplicateFileLoadChecks.put("dimcCheckname", "duplicateFileLoadChecks");
        duplicateFileLoadChecks.put("isDimcCheckEnabled", "${DUPLICATE_FILELOAD_CHECK_ENABLED}");
        duplicateFileLoadChecks.put("isFailJobOnError", "${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(duplicateFileLoadChecks);

        Map<Object, Object> dataVolumeConsistencyCheck = new HashMap<>();
        dataVolumeConsistencyCheck.put("dimcCheckname", "dataVolumeConsistencyCheck");
        dataVolumeConsistencyCheck.put("isDimcCheckEnabled", "${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}");
        dataVolumeConsistencyCheck.put("upperThreshold", "${UPPER_THRESHOLD}");
        dataVolumeConsistencyCheck.put("lowerThreshold", "${LOWER_THRESHOLD}");
        dataVolumeConsistencyCheck.put("numPastDays", "${NUM_PAST_DAYS}");
        dataVolumeConsistencyCheck.put("isFailJobOnError", "${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(dataVolumeConsistencyCheck);

        Map<Object, Object> missingFileCheck = new HashMap<>();
        missingFileCheck.put("dimcCheckname", "missingFileCheck");
        missingFileCheck.put("isDimcCheckEnabled", "${MISSING_FILE_CHECK_ENABLED}");
        missingFileCheck.put("isFailJobOnError", "${MISSING_FILE_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(missingFileCheck);

        Map<Object, Object> deliveryCheck = new HashMap<>();
        deliveryCheck.put("dimcCheckname", "deliveryCheck");
        deliveryCheck.put("isDimcCheckEnabled", "${DELIVERY_CHECK_ENABLED}");
        deliveryCheck.put("SLA", "${DELIVERY_CHECK_SLA}");
        deliveryCheck.put("isFailJobOnError", "${DELIVERY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(deliveryCheck);

        //TABLE_CHECKS
        Map<Object, Object> missingDbObjectCheck = new HashMap<>();
        missingDbObjectCheck.put("dimcCheckname", "missingDbObjectCheck");
        missingDbObjectCheck.put("isDimcCheckEnabled", "${MISSING_DB_OBJECT_CHECK_ENABLED}");
        missingDbObjectCheck.put("isFailJobOnError", "${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(missingDbObjectCheck);

        Map<Object, Object> emptySourceCheck = new HashMap<>();
        emptySourceCheck.put("dimcCheckname", "emptySourceCheck");
        emptySourceCheck.put("isDimcCheckEnabled", "${EMPTY_SOURCE_CHECK_ENABLED}");
        emptySourceCheck.put("isFailJobOnError", "${EMPTY_SOURCE_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(emptySourceCheck);

    }

    static List<Map<Object, Object>> getChosenDimcChecksList(List<Integer> chosenIndices) {
        if (!chosenIndices.isEmpty()) {
            populateDimcChecksList();
        }
        List<Map<Object, Object>> chosenDimcChecksList = new ArrayList<>();
        for (Integer index : chosenIndices) {
            if (index >= 0 && index < DIMC_CHECKS_LIST.size()) {
                Map<Object, Object> dimcCheck = DIMC_CHECKS_LIST.get(index);
                chosenDimcChecksList.add(dimcCheck);
            }
        }
        return chosenDimcChecksList;
    }

    public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) {
        Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository.findById(requirementId);

        if (existingConnJsonRequirement.isPresent()) {
            return existingConnJsonRequirement.get();
        }
        ObjectMapper objectMapper = new ObjectMapper();
        ObjectNode rootNode = objectMapper.createObjectNode();
        ObjectNode application = objectMapper.createObjectNode();

        ObjectNode email = objectMapper.createObjectNode();
        String sorName = requirementRepository.findByRequirementId(requirementId).get(0).getSorName();
        String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName + " with Job Name $jobName has completed. " + "</p><p> SourceData=$sourceData , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
        email.put("body", emailBody);
        ObjectNode job = objectMapper.createObjectNode();
        job.set("Email", email);
        application.set("job", job);
        rootNode.set("application", application);
        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {
        });

        ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
        return connJsonRequirementRepository.save(connJsonRequirement);
    }

    public JsonRequirement getOrCreateJsonRequirement(String requirementId) throws JsonProcessingException {
        String req = requirementId.split("_")[0];
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);

        if (existingJsonRequirement.isPresent()) {
            return existingJsonRequirement.get();
        }

        Optional<JiraStoryIntake> jiraStoryIntake = jiraStoryIntakeRepository.findById(req);
        if (jiraStoryIntake.isEmpty()) {
            return null;
        }

        JiraStoryIntake jiraStoryIntake1 = jiraStoryIntake.get();
        Sor sor = sorRepository.findBySorName(jiraStoryIntake1.getApplicationSorName()).get(0);
        if sor == null) {
            return null;
        }
        ApplicationAuthorizer applicationAuthorizer;
        if (applicationAuthRepository.findByApplicationName(jiraStoryIntake1.getApplicationName()).isEmpty()) {
            return null;
        }
        // HARD-CODED DIMC CHECKS AS OF NOW TO BE REPLACED WITH USER INPUT
        List<Integer> chosenDimcChecks = Arrays.asList(1, 5);
        applicationAuthorizer = applicationAuthRepository.findByApplicationName(jiraStoryIntake1.getApplicationName()).get(0);
        List<SQLScript> sqlScriptList = sqlScriptRepository.findByRequirementId(jiraStoryIntake1.getJiraStoryIntakeId());
        SQLScript sqlScript = sqlScriptList.get(0);
        ObjectMapper objectMapper = new ObjectMapper();
        ObjectNode rootNode = objectMapper.createObjectNode();
        rootNode.set("application", createApplicationNode(objectMapper, jiraStoryIntake1, sor, sqlScript, applicationAuthorizer, chosenDimcChecks));
        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {
        });
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);
        return jsonRequirementRepository.save(jsonRequirement);
    }

    private static ObjectNode createApplicationNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer, List<Integer> chosenDimcChecks) throws JsonProcessingException {
        ObjectNode applicationNode = objectMapper.createObjectNode();
        applicationNode.put("isParallelProcessing", "N");
        applicationNode.set("jobs", createJobsNode(objectMapper, jiraStoryIntake, sor, sqlScript, applicationAuthorizer, chosenDimcChecks));
        return applicationNode;
    }

    private static ObjectNode createJobsNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer, List<Integer> chosenDimcChecks) throws JsonProcessingException {
        ObjectNode jobsNode = objectMapper.createObjectNode();
        jobsNode.set("job", createJobNode(objectMapper, jiraStoryIntake, sor, sqlScript, applicationAuthorizer, chosenDimcChecks));
        return jobsNode;
    }

    private static ObjectNode createJobNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer, List<Integer> chosenDimcChecks) throws JsonProcessingException {
        ObjectNode jobNode = objectMapper.createObjectNode();

        if (!("gcp").equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            jobNode.put("app", jiraStoryIntake.getApplicationName());
            jobNode.put("appRemedyId", applicationAuthorizer.getAppRemedyId());
            jobNode.put("sorRemedyId", sor.getRemedyId());
            jobNode.put("queryGroup", "dimc_check_" + sor.getSorName().toLowerCase(Locale.ROOT));
            jobNode.put("zone", SANITIZED);
            jobNode.put("sor", sor.getSorName().toLowerCase(Locale.ROOT));
        }

        jobNode.put("jobId", "1");
        jobNode.put("jobName", jiraStoryIntake.getTableName());
        jobNode.put("jobDescription", sor.getSorName().toLowerCase(Locale.ROOT) + TABLE + jiraStoryIntake.getTableName().toLowerCase() + INGESTION_TO + applicationAuthorizer.getApplicationName());

        if (("gcp").equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            jobNode.put("jobDecription", "CDMP Extract for Hive table - " + jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT));
        }
        jobNode.put("isJobEnabled", "Y");
        jobNode.set("Email", createEmailNode(objectMapper, jiraStoryIntake, sor));

        if (!("gcp").equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            jobNode.set("ingestions", createIngestionNode(objectMapper, jiraStoryIntake, sor, sqlScript, chosenDimcChecks));
        }

        jobNode.set("flows", createFlowsNode(objectMapper, jiraStoryIntake, sor, sqlScript, applicationAuthorizer));
        jobNode.set("dataChecks", createDataChecksNode(objectMapper, jiraStoryIntake, sor, applicationAuthorizer));

        if (("gcp").equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            jobNode.set("fileExtract", createFileExtractNode(objectMapper, jiraStoryIntake, sqlScript));
        }
        return jobNode;
    }

    private static ObjectNode createEmailNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor) {
        ObjectNode emailNode = objectMapper.createObjectNode();
        emailNode.put("fromAddress", "${EMAIL_TO_LIST}");
        emailNode.put("toAddress", "${EMAIL_CC_LIST}");
        emailNode.put("subject", " Data Extract from " + sor.getSorName().toUpperCase(Locale.ROOT) + " to " + jiraStoryIntake.getApplicationName() + " - " + jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT));

        if (("gcp").equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            emailNode.put("fromAddress", "$(FROM_EMAIL)");
            emailNode.put("toAddress", "${TO_EMAIL}");
            emailNode.put("subject", "COMP Extraction - " + jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT));
        }
        emailNode.put("isPriorityAlert", "Y");
        return emailNode;
    }

    private static ObjectNode createFileExtractNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, SQLScript sqlScript) {
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = objectMapper.createArrayNode();
        ObjectNode fileExtractNode = objectMapper.createObjectNode();
        fileExtractNode.put("fileExtractId", "1");
        fileExtractNode.put("isFileExtractEnabled", "Y");
        fileExtractNode.put("fileDescription", "Extracting " + jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + " to avro file for CDMP");
        fileExtractNode.put("subjectArea", "CDMP");
        fileExtractNode.put("zone", "raw");
        fileExtractNode.put("system", "cdmp");
        fileExtractNode.put("fileType", "avro");
        fileExtractNode.put("extractQuery", "select * from " + jiraStoryIntake.getTableName() + "_extract_df");
        fileExtractNode.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        fileExtractNode.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        fileExtractNode.set("cdmpAdditionalParams", createCdmpAdditionalParamsNode(objectMapper, jiraStoryIntake));
        fileExtractArray.add(fileExtractNode);
        fileExtractsNode.set("fileExtract", fileExtractArray);
        return fileExtractsNode;
    }

    private static ObjectNode createCdmpAdditionalParamsNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake) {
        ObjectNode paramsNode = objectMapper.createObjectNode();
        paramsNode.put("outputFormat", "avro");
        paramsNode.put("cdpFileName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_comp_extract");
        paramsNode.put("cdmpPatternMode", "daily");
        paramsNode.put("isCustomPartitionEnabled", "true");
        paramsNode.put("desiredPartFileSize", "${PART_FILE_SIZE}");

        return paramsNode;
    }

    private static ObjectNode createFlowsNode(ObjectMapper objectMapper,
            JiraStoryIntake jiraStoryIntake,
            Sor sor,
            SQLScript sqlScript,
            ApplicationAuthorizer applicationAuthorizer) {
        ObjectNode flowsNode = objectMapper.createObjectNode();
        String sorType = jiraStoryIntake.getSorType();
        if ("file".equalsIgnoreCase(sorType)) {
            ObjectNode flow1 = createFixedWidthFileFlowNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, sqlScript, applicationAuthorizer);
            addToArray(flowsNode, "flow", flow1);
            return flowsNode;
        } else {
            ObjectNode flow1 = createFlowNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, sqlScript, applicationAuthorizer);
            addToArray(flowsNode, "flow", flow1);
            return flowsNode;
        }
    }

    private static ObjectNode createFlowNode(ObjectMapper objectMapper, String flowId, String zone, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer) {
        Map<String, String> storageNameMap = new HashMap<>();
        storageNameMap.put("CNAPP", "consumer");
        storageNameMap.put("UTCAP", "utcap");

        ObjectNode flowNode = objectMapper.createObjectNode();
        flowNode.put("flowId", flowId);
        flowNode.put("flowDesc", jiraStoryIntake.getApplicationSorName().toUpperCase(Locale.ROOT) + TABLE + jiraStoryIntake.getTableName() + INGESTION_TO + jiraStoryIntake.getApplicationName() + "-" + zone.replaceFirst("\\.", zone.substring(0, 1)).toUpperCase(Locale.ROOT));
        flowNode.put("isFlowEnabled", "Y");
        flowNode.put("query", sqlScript.getSrcDataFrameQuery());
        flowNode.put("registerName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + zone.toLowerCase(Locale.ROOT));

        if ("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            flowNode.put("query", sqlScript.getSrcTableQuery() + " ${EMB_LIMIT} ${LIMIT_COUNT}");
            flowNode.put("flowDesc", "Extraction of " + jiraStoryIntake.getSourceTableName() + " for CDMP processing.");
            flowNode.put("registerName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_extract_df");
        }

        if (!"gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType().toLowerCase(Locale.ROOT))) {
            flowNode.put("saveOutput", "Y");
            flowNode.put("flowDataBaseName", jiraStoryIntake.getSourceTableName() + "_" + zone.toLowerCase(Locale.ROOT) + "_df");
            flowNode.put("storageName", "/datalkgs/${EDL_ENV}/" + storageNameMap.get(jiraStoryIntake.getApplicationName()) + "/" + sor.getDomainName().toLowerCase(Locale.ROOT) + "/" + sor.getSorName().toLowerCase(Locale.ROOT) + "/" + zone.toLowerCase(Locale.ROOT) + "/" + jiraStoryIntake.getTableName());
            flowNode.put("hivePartitionColumn", applicationAuthorizer.getHivePartition());
            flowNode.put("isCreateFlowTargetTable", "Y");
            flowNode.put("isOverWritePartition", "Y");
            flowNode.put("isCacheEnabled", "Y");
            flowNode.put("sendEmail", "N");
            flowNode.put("abortIfEmpty", "N");
        }

        return flowNode;
    }

    private static ObjectNode createIngestionsNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript,
            List<Integer> chosenDimChecks) throws JsonProcessingException {
        ObjectNode ingestionsNode = objectMapper.createObjectNode();
        String sorType = jiraStoryIntake.getSorType();
        if ("file".equalsIgnoreCase(sorType)) {
            ObjectNode ingestion1 = createFixedWidthFileIngestionNode(objectMapper, "1", jiraStoryIntake, sor, sqlScript, chosenDimChecks);
            addToArray(ingestionsNode, "ingestion", ingestion1);
            return ingestionsNode;
        } else {
            ObjectNode ingestion1 = createIngestionNode(objectMapper, "1", jiraStoryIntake, sor, sqlScript, chosenDimChecks);
            addToArray(ingestionsNode, "ingestion", ingestion1);
            return ingestionsNode;
        }
    }

    private static ObjectNode createIngestionNode(ObjectMapper objectMapper, String ingestionId, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, List<Integer> chosenDimChecks) throws JsonProcessingException {
        ObjectNode ingestionNode = objectMapper.createObjectNode();
        Map<String, String> sourceType = new HashMap<>();
        sourceType.putIfAbsent("teradata", "teradb");
        Map<String, String> connectionHeader = new HashMap<>();
        connectionHeader.putIfAbsent("teradata", "teradbConn");

        ingestionNode.set("dimChecks", objectMapper.readTree(objectMapper.writeValueAsString(getChosenDimChecksList(chosenDimChecks))));
        ingestionNode.put("ingestionId", ingestionId);
        ingestionNode.put("ingestionDesc", sor.getSorName().toLowerCase(Locale.ROOT) + TABLE + jiraStoryIntake.getTableName());
        ingestionNode.put("ingestionEnabled", "Y");
        ingestionNode.put("isCreateTargetTable", "Y");
        ingestionNode.put("sourceType", sourceType.get(jiraStoryIntake.getSorType().toLowerCase(Locale.ROOT)));
        ingestionNode.put("connectionHeader", connectionHeader.get(jiraStoryIntake.getSorType().toLowerCase(Locale.ROOT)));
        ingestionNode.put("sourceSQLQuery", sqlScript.getSrcTableQuery());
        ingestionNode.put("dataFrameName", jiraStoryIntake.getTableName() + "_raw_df");
        ingestionNode.put("isRowCountDisabled", "N");
        ingestionNode.put("targetType", "parquet");
        ingestionNode.put("isCacheEnabled", "Y");
        ingestionNode.put("isOverWritePartition", "Y");
        ingestionNode.put("abortIfDuplicate", "N");
        ingestionNode.put("abortIfEmpty", "N");
        ingestionNode.put("isRunEMHReport", "N");

        return ingestionNode;
    }

    private static ObjectNode createDataChecksNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer) {
        ObjectNode dataChecksNode = objectMapper.createObjectNode();
        String sorType = jiraStoryIntake.getSorType();
        dataChecksNode.put("acCDatabase", "${AUDIT_DATABASE}");
        if ("file".equalsIgnoreCase(sorType)) {
            if (!"gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
                ObjectNode check1 = createFixedWidthFileCheckNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, applicationAuthorizer);
                dataChecksNode.put("acCDatabase", applicationAuthorizer.getAuditTable());
                addToArray(dataChecksNode, "checks", check1);
            }
        } else {
            if (!"gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
                ObjectNode check1 = createCheckNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, applicationAuthorizer);
                dataChecksNode.put("acCDatabase", applicationAuthorizer.getAuditTable());
                addToArray(dataChecksNode, "checks", check1);
            }
        }
        if (!"gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            ObjectNode check1 = createCheckNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, applicationAuthorizer);
            dataChecksNode.put("acCDatabase", applicationAuthorizer.getAuditTable());
            addToArray(dataChecksNode, "checks", check1);
        }
        return dataChecksNode;
    }

    private static ObjectNode createCheckNode(ObjectMapper objectMapper, String checkId, String zone, JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer) {
        ObjectNode checkNode = objectMapper.createObjectNode();
        checkNode.put("checkId", checkId);
        checkNode.put("checkName", "HHRNCTCHK");
        checkNode.put("checkDesc", "Data Count Check");
        checkNode.put("isCheckEnabled", "Y");
        checkNode.put("isUserSupplyQuery", "Y");
        checkNode.put("readSourceQueryFromPath", "N");
        checkNode.put("sourceQuery", "select count(*) from " + jiraStoryIntake.getTableName() + "_raw_df");
        checkNode.put("targetQuery", "select count(*) from " + jiraStoryIntake.getTargetSchema() + "." + jiraStoryIntake.getTableName() + " where " + applicationAuthorizer.getHivePartition() + "=${BUSINESS_DATE}");
        checkNode.put("readTargetQueryFromPath", "N");
        checkNode.put("isFailJobOnDataCheckError", "N");
        return checkNode;
    }

    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) {
        ArrayNode array;
        if (parent.has(arrayName) && parent.get(arrayName).isArray()) {
            array = (ArrayNode) parent.get(arrayName);
        } else {
            array = parent.putArray(arrayName);
        }
        array.add(item);
    }

    private static ObjectNode createFixedwidthFileCheckNode(ObjectMapper objectMapper, String checkId, String zone, JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer) { // K097363
        ObjectNode checkNode = objectMapper.createObjectNode();
        checkNode.put("checkId", checkId);
        checkNode.put("checkName", "HORCNTCHK"); // Example check name
        checkNode.put("checkDesc", "Data Count Check");
        checkNode.put("isCheckEnabled", "Y");
        checkNode.put("readSourceQueryFromPath", "N"); // Changed from Y in other method?
        checkNode.put("sourceQuery", "select count(*) from " + jiraStoryIntake.getTableName() + "_raw_df");
        checkNode.put("targetQuery", "select count(*) from " + jiraStoryIntake.getTargetSchema() + "." + jiraStoryIntake.getTableName() + " where " + applicationAuthorizer.getHivePartition() + "='${BUSINESS_DATE}'");
        checkNode.put("isFailJobOnDataCheckError", "N"); // Changed from Y in other method?

        return checkNode;
    }

    private static void addTOArray(ObjectNode parent, String arrayName, ObjectNode item) { // 7 usages new
        ArrayNode array;
        if (parent.has(arrayName) && parent.get(arrayName).isArray()) {
            array = (ArrayNode) parent.get(arrayName);
        } else {
            array = parent.putArray(arrayName);
        }
        array.add(item);
    }

    private static ObjectNode createFixedWidthFileIngestionNode(String ingestionId,
            ObjectMapper objectMapper,
            JiraStoryIntake jiraStoryIntake,
            Sor sor,
            SQLScript sqlScript,
            List<Integer> chosenDimChecks) throws JsonProcessingException {
        ObjectNode ingestionNode = objectMapper.createObjectNode();
        ObjectNode dimChecks = objectMapper.createObjectNode();
        Map<String, String> additionalReadConfigs = new HashMap<>();
        ArrayNode dataArrayNode = objectMapper.createArrayNode();
        List<Map<Object, Object>> listDimChecks = getChosenDimChecksList(chosenDimChecks);
        for (Map<Object, Object> dimCheck : listDimChecks) {
            ObjectNode dimCheckNode = objectMapper.createObjectNode();
            dimCheckNode.put("dimCheckName", dimCheck.get("dimCheckName").toString());
            dimCheckNode.put("isDimCheckEnabled", dimCheck.get("isDimCheckEnabled").toString());
            dataArrayNode.add(dimCheckNode);
        }
        dimChecks.set("dimCheck", dataArrayNode);
        additionalReadConfigs.put("quote", "\"\\\"\"");
        additionalReadConfigs.put("escape", "\\\\");
        ingestionNode.put("ingestionId", ingestionId);
        ingestionNode.set("dimChecks", dimChecks);
        ingestionNode.put("ingestionDesc", sor.getSorName().toLowerCase(Locale.ROOT) + TABLE + jiraStoryIntake.getTableName() + INGESTION_TO + jiraStoryIntake.getApplicationName());
        ingestionNode.put("isIngestionEnabled", "Y");
        ingestionNode.put("sourceType", "fixedWidthFile");
        ingestionNode.put("dataFrameName", jiraStoryIntake.getTableName() + "_df");
        ingestionNode.put("stagingFilePath", "${EXP_EDL_PATH}");
        ingestionNode.put("isCreateTargetTable", "Y");
        ingestionNode.put("dataFilePath", "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}");
        ingestionNode.put("isHeaderEnabled", "Y");
        ingestionNode.put("schemaFilePath", "${DCI_CONFIG_DIR_PATH}/config/" + sor.getSorName() + "/${FEED_NAME}/metadata/" + jiraStoryIntake.getTableName() + ".schema");
        ingestionNode.set("additionalReadConfigs", objectMapper.readTree(objectMapper.writeValueAsString(additionalReadConfigs)));
        ingestionNode.put("isCacheEnabled", "Y");
        ingestionNode.put("isOverWritePartition", "Y");
        ingestionNode.put("abortIfDuplicate", "N");
        ingestionNode.put("abortIfEmpty", "N");
        ingestionNode.put("isRunEMHReport", "N");
        return ingestionNode;
    }

    private static ObjectNode createFixedWidthFileFlowNode(ObjectMapper objectMapper, String flowId, String zone, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer) {
        Map<String, String> storageNameMap = new HashMap<>();
        storageNameMap.put("CNAPP", "consumer");
        storageNameMap.put("UTCAP", "utcap");

        ObjectNode flowNode = objectMapper.createObjectNode();
        flowNode.put("flowId", flowId);
        flowNode.put("flowDesc", jiraStoryIntake.getApplicationSorName().toUpperCase(Locale.ROOT) + TABLE + jiraStoryIntake.getTableName() + INGESTION_TO
                + jiraStoryIntake.getApplicationName() + "-" + zone.replaceFirst("\\.", zone.substring(0, 1)).toUpperCase(Locale.ROOT));
        flowNode.put("isFlowEnabled", "Y");
        flowNode.put("query", sqlScript.getSrcDataFrameQuery());
        flowNode.put("registerName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "-" + zone.toLowerCase(Locale.ROOT));

        if ("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            flowNode.put("query",sqlScript.getSrcTableQuery() + " ${EMB_LIMIT} ${LIMIT_COUNT}");
            flowNode.put("flowDesc","Extraction of " + jiraStoryIntake.getSourceTableName() + " for CDMP processing.");
            flowNode.put("registerName",jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_extract_df");
        }

        if (!"gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType().toLowerCase(Locale.ROOT))) {
            flowNode.put("saveOutput", "Y");
            flowNode.put("flowDataBaseName",jiraStoryIntake.getSourceTableName()+ "-" + zone.toLowerCase(Locale.ROOT) + "_df");
            flowNode.put("storageName","/datalkgs/${EDL_ENV}/"+ storageNameMap.get(jiraStoryIntake.getApplicationName())+ "/" + sor.getDomainName().toLowerCase(Locale.ROOT)+ "/" + sor.getSorName().toLowerCase(Locale.ROOT)+ "/" + zone.toLowerCase(Locale.ROOT)+ "/" + jiraStoryIntake.getTableName());
            flowNode.put("targetTablePath", "${EXP_EDL_PATH}/${TARGET_TABLE_NAME}");
            flowNode.put("hivePartitionColumn", applicationAuthorizer.getHivePartition());
            flowNode.put("isCreateFlowTargetTable", "Y");
            flowNode.put("isOverWritePartition", "Y");
            flowNode.put("isCacheEnabled", "N");
            flowNode.put("sendEmail", "Y");
            flowNode.put("abortIfEmpty", "N");
        }

        return flowNode;
    }

    private static ObjectNode createFixedWidthFileCheckNode(ObjectMapper objectMapper,String checkId,String zone,JiraStoryIntake jiraStoryIntake,Sor sor,ApplicationAuthorizer applicationAuthorizer) {
        ObjectNode checkNode = objectMapper.createObjectNode();
        checkNode.put("checkId", checkId);
        checkNode.put("checkName", "HHRNCTCHK");
        checkNode.put("checkDesc", "Data Count Check");
        checkNode.put("isCheckEnabled", "Y");
        checkNode.put("isUserSupplyQuery", "Y");
        checkNode.put("readSourceQueryFromPath", "N");
        checkNode.put("sourceQuery","select count(*) from " + jiraStoryIntake.getTableName() + "_df");
        checkNode.put("targetQuery","select count(*) from "+ jiraStoryIntake.getTargetSchema() + "."+ jiraStoryIntake.getTableName()+ " where " + applicationAuthorizer.getHivePartition()+ "=${BUSINESS_DATE}");
        checkNode.put("readTargetQueryFromPath", "N");
        checkNode.put("isFailJobOnDataCheckError", "Y");
        return checkNode;
    }

} // End of class JsonRequirementService
