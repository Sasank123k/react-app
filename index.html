package com.wellsfargo.utcap.config;

import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.model.*;
import lombok.Getter;
import lombok.ToString;

import java.util.List;
import java.util.Objects;
import java.util.Set;

@Getter
@ToString
public class JsonGenerationConfig {

    // --- Enums and Constants ---
    public enum SourceType { TERADATA, FILE_FIXEDWIDTH, FILE_PIPE, FILE_COMMA, MSSQL, ORACLE, HIVE, UNKNOWN }
    public enum TargetType { HIVE, GCP, UNKNOWN }

    public static final String SANITIZED_ZONE = "sanitized";
    public static final String RAW_ZONE = "raw";
    public static final String TABLE_TEXT = " Table ";
    public static final String INGESTION_TO_TEXT = " ingestion to ";

    // --- Core Input Data Objects (Stored for reference if needed by generic builders directly) ---
    private final JiraStoryIntake jiraStoryIntake;
    private final Sor sor;
    private final ApplicationAuthorizer applicationAuthorizer;
    private final SQLScript sqlScript;

    // --- Determined Types & Overall Structural Flags ---
    private final SourceType sourceType;
    private final TargetType targetType;
    private final boolean includeIngestions;
    private final boolean includeFlows;
    private final boolean includeDataChecks;
    private final boolean includeFileExtracts;

    // --- Field Inclusion Sets ---
    private final Set<String> jobLevelFieldsToInclude;
    private final Set<String> ingestionFieldsToInclude;

    // --- Pre-Resolved Values / Pre-built Structures for INGESTION ---
    private final String resolvedIngestionSourceTypeValue;
    private final String resolvedIngestionConnectionHeaderValue;
    private final String resolvedIngestionDataFrameName;
    private final String resolvedIngestionSourceSQLQuery;
    private final String resolvedIngestionSourceName; // For Oracle
    private final String resolvedIngestionDataFilePath;
    private final String resolvedIngestionSchemaFilePath;
    private final String resolvedIngestionStagingFilePath;
    private final String resolvedIngestionDelimiter;
    private final String resolvedIngestionIsHeaderEnabledValue; // Standardized to "Y"
    private final String resolvedIngestionSaveOutputFlag;    // Standardized to "Y" for all files
    private final String resolvedIngestionTargetPartition;   // Standardized for all files
    private final String ingestionIsCreateTargetTableValue;    // Standardized to "Y"
    private final String ingestionIsCacheEnabledValue;         // Standardized to "Y"
    private final String ingestionIsOverWritePartitionValue;   // Standardized to "Y"
    private final ObjectNode prebuiltIngestionAdditionalReadConfigsNode;
    private final ObjectNode prebuiltIngestionDimcChecksNode; // Null if not applicable

    // --- Pre-Resolved Values / Pre-built Structures for FLOWS ---
    private final List<FlowDetailConfig> flowDetailsList;

    // --- Pre-Resolved Values / Pre-built Structures for DATA CHECKS ---
    private final String resolvedDataChecksAcDcDatabase;
    private final List<CheckDetailConfig> checkDetailsList;
    private final ObjectNode prebuiltDataChecksPreChecksNode;

    // Constructor - Populated by JsonRequirementService.buildJsonGenerationConfig
    public JsonGenerationConfig(
            JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer, SQLScript sqlScript,
            SourceType sourceType, TargetType targetType,
            boolean includeIngestions, boolean includeFlows, boolean includeDataChecks, boolean includeFileExtracts,
            Set<String> jobLevelFieldsToInclude, Set<String> ingestionFieldsToInclude,
            String resolvedIngestionSourceTypeValue, String resolvedIngestionConnectionHeaderValue,
            String resolvedIngestionDataFrameName, String resolvedIngestionSourceSQLQuery,
            String resolvedIngestionSourceName, String resolvedIngestionDataFilePath,
            String resolvedIngestionSchemaFilePath, String resolvedIngestionStagingFilePath,
            String resolvedIngestionDelimiter, String resolvedIngestionIsHeaderEnabledValue,
            String resolvedIngestionSaveOutputFlag, String resolvedIngestionTargetPartition,
            String ingestionIsCreateTargetTableValue, String ingestionIsCacheEnabledValue, String ingestionIsOverWritePartitionValue,
            ObjectNode prebuiltIngestionAdditionalReadConfigsNode, ObjectNode prebuiltIngestionDimcChecksNode,
            List<FlowDetailConfig> flowDetailsList,
            String resolvedDataChecksAcDcDatabase, List<CheckDetailConfig> checkDetailsList,
            ObjectNode prebuiltDataChecksPreChecksNode) {

        this.jiraStoryIntake = Objects.requireNonNull(jiraStoryIntake);
        this.sor = Objects.requireNonNull(sor);
        this.applicationAuthorizer = Objects.requireNonNull(applicationAuthorizer);
        this.sqlScript = Objects.requireNonNull(sqlScript);
        this.sourceType = Objects.requireNonNull(sourceType);
        this.targetType = Objects.requireNonNull(targetType);
        this.includeIngestions = includeIngestions;
        this.includeFlows = includeFlows;
        this.includeDataChecks = includeDataChecks;
        this.includeFileExtracts = includeFileExtracts;
        this.jobLevelFieldsToInclude = Objects.requireNonNull(jobLevelFieldsToInclude);
        this.ingestionFieldsToInclude = Objects.requireNonNull(ingestionFieldsToInclude);
        this.resolvedIngestionSourceTypeValue = resolvedIngestionSourceTypeValue;
        this.resolvedIngestionConnectionHeaderValue = resolvedIngestionConnectionHeaderValue;
        this.resolvedIngestionDataFrameName = resolvedIngestionDataFrameName;
        this.resolvedIngestionSourceSQLQuery = resolvedIngestionSourceSQLQuery;
        this.resolvedIngestionSourceName = resolvedIngestionSourceName;
        this.resolvedIngestionDataFilePath = resolvedIngestionDataFilePath;
        this.resolvedIngestionSchemaFilePath = resolvedIngestionSchemaFilePath;
        this.resolvedIngestionStagingFilePath = resolvedIngestionStagingFilePath;
        this.resolvedIngestionDelimiter = resolvedIngestionDelimiter;
        this.resolvedIngestionIsHeaderEnabledValue = resolvedIngestionIsHeaderEnabledValue;
        this.resolvedIngestionSaveOutputFlag = resolvedIngestionSaveOutputFlag;
        this.resolvedIngestionTargetPartition = resolvedIngestionTargetPartition;
        this.ingestionIsCreateTargetTableValue = ingestionIsCreateTargetTableValue;
        this.ingestionIsCacheEnabledValue = ingestionIsCacheEnabledValue;
        this.ingestionIsOverWritePartitionValue = ingestionIsOverWritePartitionValue;
        this.prebuiltIngestionAdditionalReadConfigsNode = prebuiltIngestionAdditionalReadConfigsNode;
        this.prebuiltIngestionDimcChecksNode = prebuiltIngestionDimcChecksNode;
        this.flowDetailsList = Objects.requireNonNull(flowDetailsList);
        this.resolvedDataChecksAcDcDatabase = resolvedDataChecksAcDcDatabase;
        this.checkDetailsList = Objects.requireNonNull(checkDetailsList);
        this.prebuiltDataChecksPreChecksNode = prebuiltDataChecksPreChecksNode;
    }

    // --- Inner static classes for Flow and Check details ---
    @Getter @ToString public static class FlowDetailConfig {
        // Fields as defined before (flowId, resolvedFlowDescription, resolvedFlowQuery, etc.)
        // Standardized boolean values will be passed into this constructor directly
        private final String flowId; private final String resolvedFlowDescription; private final String resolvedFlowQuery; private final String resolvedFlowRegisterName; private final String resolvedFlowDatabaseName; private final String resolvedFlowStorageName; private final String resolvedFlowTargetTablePath; private final String resolvedFlowHivePartitionColumn; private final String isCreateFlowTargetTableValue; private final String isOverWritePartitionValue; private final String isCacheEnabledValue; private final String sendEmailValue; private final String abortIfEmptyValue; private final String saveOutputValue;
        public FlowDetailConfig(String flowId, String resolvedFlowDescription, String resolvedFlowQuery, String resolvedFlowRegisterName, String resolvedFlowDatabaseName, String resolvedFlowStorageName, String resolvedFlowTargetTablePath, String resolvedFlowHivePartitionColumn, String isCreateFlowTargetTableValue, String isOverWritePartitionValue, String isCacheEnabledValue, String sendEmailValue, String abortIfEmptyValue, String saveOutputValue) {
            this.flowId = flowId; this.resolvedFlowDescription = resolvedFlowDescription; this.resolvedFlowQuery = resolvedFlowQuery; this.resolvedFlowRegisterName = resolvedFlowRegisterName; this.resolvedFlowDatabaseName = resolvedFlowDatabaseName; this.resolvedFlowStorageName = resolvedFlowStorageName; this.resolvedFlowTargetTablePath = resolvedFlowTargetTablePath; this.resolvedFlowHivePartitionColumn = resolvedFlowHivePartitionColumn; this.isCreateFlowTargetTableValue = isCreateFlowTargetTableValue; this.isOverWritePartitionValue = isOverWritePartitionValue; this.isCacheEnabledValue = isCacheEnabledValue; this.sendEmailValue = sendEmailValue; this.abortIfEmptyValue = abortIfEmptyValue; this.saveOutputValue = saveOutputValue;
        }
    }

    @Getter @ToString public static class CheckDetailConfig {
        // Fields as defined before (checkId, checkName, resolvedCheckSourceQuery, etc.)
        // Standardized boolean values will be passed into this constructor
        private final String checkId; private final String checkName; private final String checkDescription; private final String resolvedCheckSourceQuery; private final String resolvedCheckTargetQuery; private final String isCheckEnabledValue; private final String isUserSupplyQueryValue; private final String readSourceQueryFromPathValue; private final String readTargetQueryFromPathValue; private final String isFailJobOnDataCheckErrorValue;
        public CheckDetailConfig(String checkId, String checkName, String checkDescription, String resolvedCheckSourceQuery, String resolvedCheckTargetQuery, String isCheckEnabledValue, String isUserSupplyQueryValue, String readSourceQueryFromPathValue, String readTargetQueryFromPathValue, String isFailJobOnDataCheckErrorValue) {
            this.checkId = checkId; this.checkName = checkName; this.checkDescription = checkDescription; this.resolvedCheckSourceQuery = resolvedCheckSourceQuery; this.resolvedCheckTargetQuery = resolvedCheckTargetQuery; this.isCheckEnabledValue = isCheckEnabledValue; this.isUserSupplyQueryValue = isUserSupplyQueryValue; this.readSourceQueryFromPathValue = readSourceQueryFromPathValue; this.readTargetQueryFromPathValue = readTargetQueryFromPathValue; this.isFailJobOnDataCheckErrorValue = isFailJobOnDataCheckErrorValue;
        }
    }
}















































package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.config.JsonGenerationConfig;
import com.wellsfargo.utcap.exception.ConfigurationException;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(JsonRequirementService.class);
    private static final List<Map<String, Object>> DIMC_CHECKS_MASTER_LIST = new ArrayList<>();

    private final ApplicationAuthRepository applicationAuthRepository;
    private final RequirementRepository requirementRepository;
    private final JsonRequirementRepository jsonRequirementRepository;
    private final JiraStoryIntakeRepository jiraStoryIntakeRepository;
    private final SorRepository sorRepository;
    private final SQLScriptRepository sqlScriptRepository;
    private final ConnJsonRequirementRepository connJsonRequirementRepository;
    private final ObjectMapper objectMapper;

    @Autowired
    public JsonRequirementService(/*... Repositories ...,*/ ObjectMapper objectMapper) {
        // ... Initialize repositories ...
        this.applicationAuthRepository = applicationAuthRepository;
        this.requirementRepository = requirementRepository;
        this.jsonRequirementRepository = jsonRequirementRepository;
        this.jiraStoryIntakeRepository = jiraStoryIntakeRepository;
        this.sorRepository = sorRepository;
        this.sqlScriptRepository = sqlScriptRepository;
        this.connJsonRequirementRepository = connJsonRequirementRepository;
        this.objectMapper = objectMapper;
        populateDimcChecksMasterList();
    }

    // ... saveJson, getOrCreateJsonRequirement (calls buildJsonGenerationConfig) ...

    private JsonGenerationConfig buildJsonGenerationConfig(String requirementId) throws ConfigurationException, JsonProcessingException {
        String jiraStoryId = requirementId.split("_")[0];
        JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
        Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("Sor not found for name: " + jira.getApplicationSorName()));
        ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
        SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

        JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
        JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

        if (sourceTypeEnum == JsonGenerationConfig.SourceType.UNKNOWN) throw new ConfigurationException("Unknown Source Type: " + jira.getSorType());
        if (targetTypeEnum == JsonGenerationConfig.TargetType.UNKNOWN) throw new ConfigurationException("Unknown Target Type: " + jira.getTargetType());

        // --- Determine Section Inclusion Flags ---
        boolean includeIngestions = (sourceTypeEnum != JsonGenerationConfig.SourceType.HIVE);
        boolean includeFlows = true;
        boolean includeDataChecks = true;
        boolean includeFileExtracts = (targetTypeEnum == JsonGenerationConfig.TargetType.GCP);

        // --- Determine Field Inclusion Sets ---
        Set<String> jobLevelFieldsToInclude = determineJobLevelFieldsToInclude(targetTypeEnum);
        Set<String> ingestionFieldsToInclude = determineIngestionFieldsToInclude(sourceTypeEnum, includeIngestions);

        // --- Resolve/Pre-build Ingestion Data ---
        String resolvedIngestionSourceTypeValue = includeIngestions ? resolveIngestionSourceTypeValue(sourceTypeEnum) : null;
        String resolvedIngestionConnectionHeaderValue = includeIngestions ? resolveIngestionConnectionHeaderValue(sourceTypeEnum) : null;
        String resolvedIngestionDataFrameName = includeIngestions ? resolveIngestionDataFrameName(jira.getTableName()) : null; // Standardized: uses _df
        
        String resolvedIngestionSourceSQLQuery = null;
        if (includeIngestions && isDatabaseSource(sourceTypeEnum)) {
             resolvedIngestionSourceSQLQuery = sqlScript.getSrcTableQuery();
        }
        String resolvedIngestionSourceName = (includeIngestions && sourceTypeEnum == JsonGenerationConfig.SourceType.ORACLE) ?
                "${BDDMCBD_SRC_SCHEMA}." + jira.getTableName() : null;

        String resolvedIngestionDataFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ? "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}" : null;
        String resolvedIngestionSchemaFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ?
                "${DCI_CONFIG_DIR_PATH}/config/" + sor.getSorName().toLowerCase() + "/${FEED_NAME}/metadata/" + jira.getTableName().toLowerCase() + ".schema" : null;
        String resolvedIngestionStagingFilePath = (includeIngestions && isFileSource(sourceTypeEnum)) ?
                determineStagingFilePath(sourceTypeEnum, sor, jira) : null;
        String resolvedIngestionDelimiter = includeIngestions ? resolveIngestionDelimiter(sourceTypeEnum) : null;
        
        String resolvedIngestionIsHeaderEnabledValue = (includeIngestions && isFileSource(sourceTypeEnum)) ? "Y" : null; // Standardized
        String resolvedIngestionSaveOutputFlag = (includeIngestions && isFileSource(sourceTypeEnum)) ? "Y" : null; // Standardized for all files
        String resolvedIngestionTargetPartition = (includeIngestions && isFileSource(sourceTypeEnum)) ? "utcap_business_effective_date" : null; // Standardized for all files
        
        String ingestionIsCreateTargetTableValue = includeIngestions ? "Y" : null; // Standardized
        String ingestionIsCacheEnabledValue = includeIngestions ? "Y" : null;       // Standardized
        String ingestionIsOverWritePartitionValue = includeIngestions ? "Y" : null; // Standardized

        ObjectNode prebuiltIngestionAdditionalReadConfigsNode = includeIngestions ?
                buildPrebuiltIngestionAdditionalReadConfigs(sourceTypeEnum, objectMapper) : null;
        
        boolean includeDimcChecksInIngestionConfig = includeIngestions &&
                (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE).contains(sourceTypeEnum) ||
                 isFileSource(sourceTypeEnum));
        List<Integer> chosenDimcCheckIndices = determineSourceSpecificDimcIndices(sourceTypeEnum);
        ObjectNode prebuiltIngestionDimcChecksNode = includeDimcChecksInIngestionConfig ?
                buildPrebuiltDimcChecksNode(chosenDimcCheckIndices, objectMapper) : null;

        // --- Resolve/Pre-build Flow & Check Data ---
        List<JsonGenerationConfig.FlowDetailConfig> flowDetailsList = includeFlows ?
                prepareFlowDetailsList(sourceTypeEnum, targetTypeEnum, jira, sor, auth, sqlScript, resolvedIngestionDataFrameName, objectMapper) : Collections.emptyList();
        
        List<JsonGenerationConfig.CheckDetailConfig> checkDetailsList = includeDataChecks ?
                prepareCheckDetailsList(sourceTypeEnum, targetTypeEnum, jira, auth, flowDetailsList, resolvedIngestionDataFrameName, objectMapper) : Collections.emptyList();
        
        String resolvedDataChecksAcDcDatabase = includeDataChecks ?
                determineDataChecksAcDcDatabase(sourceTypeEnum, targetTypeEnum, auth, sor, jira, !checkDetailsList.isEmpty()) : null;
        ObjectNode prebuiltDataChecksPreChecksNode = includeDataChecks ?
                buildPrebuiltDataChecksPreChecksNode(sourceTypeEnum, objectMapper) : null; // Removed targetType

        // Instantiate the "Simpler" Config object by passing all pre-resolved/pre-built parts
        return new JsonGenerationConfig(
                jira, sor, auth, sqlScript, sourceTypeEnum, targetTypeEnum,
                includeIngestions, includeFlows, includeDataChecks, includeFileExtracts,
                jobLevelFieldsToInclude, ingestionFieldsToInclude,
                resolvedIngestionSourceTypeValue, resolvedIngestionConnectionHeaderValue,
                resolvedIngestionDataFrameName, resolvedIngestionSourceSQLQuery,
                resolvedIngestionSourceName, resolvedIngestionDataFilePath,
                resolvedIngestionSchemaFilePath, resolvedIngestionStagingFilePath,
                resolvedIngestionDelimiter, resolvedIngestionIsHeaderEnabledValue,
                resolvedIngestionSaveOutputFlag, resolvedIngestionTargetPartition,
                ingestionIsCreateTargetTableValue, ingestionIsCacheEnabledValue, ingestionIsOverWritePartitionValue,
                prebuiltIngestionAdditionalReadConfigsNode,
                // Pass the boolean directly, the node builder will handle it
                prebuiltIngestionDimcChecksNode, // Pass the prebuilt node
                flowDetailsList,
                resolvedDataChecksAcDcDatabase, checkDetailsList, prebuiltDataChecksPreChecksNode
        );
    }

    // --- Helper Methods for buildJsonGenerationConfig (Moved from JsonGenerationConfig) ---
    // These methods now belong to JsonRequirementService and help populate the simple JsonGenerationConfig

    private boolean isDatabaseSource(JsonGenerationConfig.SourceType sourceType) { /* ... */ return EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.MSSQL, JsonGenerationConfig.SourceType.ORACLE, JsonGenerationConfig.SourceType.HIVE).contains(sourceType); }
    private boolean isFileSource(JsonGenerationConfig.SourceType sourceType) { /* ... */ return EnumSet.of(JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH, JsonGenerationConfig.SourceType.FILE_PIPE, JsonGenerationConfig.SourceType.FILE_COMMA).contains(sourceType); }
    private Set<String> determineJobLevelFieldsToInclude(JsonGenerationConfig.TargetType targetType) { /* ... */ 
        if (targetType != JsonGenerationConfig.TargetType.GCP) {
            return new HashSet<>(Arrays.asList("app", "appRemedyId", "sorRemedyId", "queryGroup", "zone", "sor"));
        }
        return Collections.emptySet();
    }
    private Set<String> determineIngestionFieldsToInclude(JsonGenerationConfig.SourceType sourceType, boolean includeIngestionsFlag) { /* ... */ 
        if (!includeIngestionsFlag) return Collections.emptySet();
        Set<String> fields = new HashSet<>(Arrays.asList(
            "ingestionId", "ingestionDesc", "isIngestionEnabled", "dataFrameName",
            "isCreateTargetTable", "isCacheEnabled", "isOverWritePartition",
            "abortIfDuplicate", "abortIfEmpty", "isRunEMHReport", "sourceType"
        ));
        // Assuming targetType is available if needed for "targetType":"parquet"
        // This check should ideally use targetType from the main method's scope
        // For now, hardcoding, or assume this is for Hive target when ingestions are included for non-Hive sources.
        fields.add("targetType"); 


        if (isDatabaseSource(sourceType)) {
            fields.add("ConnectionHeader");
            fields.add("sourceSQLQuery");
            if (sourceType == JsonGenerationConfig.SourceType.ORACLE) fields.add("sourceName");
        } else if (isFileSource(sourceType)) {
            fields.addAll(Arrays.asList("isHeaderEnabled", "dataFilePath", "schemaFilePath", "stagingFilePath", "additionalReadConfigs"));
            if (sourceType != JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) fields.add("delimiter");
            fields.add("saveOutput"); fields.add("targetPartition");
        }
        if (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE).contains(sourceType) || isFileSource(sourceType)) {
            fields.add("dimcChecks");
        }
        return fields;
    }
    private String resolveIngestionSourceTypeValue(JsonGenerationConfig.SourceType sourceType) { /* ... */ 
        switch (sourceType) {
            case TERADATA: return "teradb";
            case FILE_FIXEDWIDTH: return "fixedwidthFile";
            case FILE_PIPE: case FILE_COMMA: return "file";
            case MSSQL: return "jdbc";
            case ORACLE: return "oracle";
            case HIVE: return "hive";
            default: return "unknown";
        }
    }
    private String resolveIngestionConnectionHeaderValue(JsonGenerationConfig.SourceType sourceType) { /* ... */ 
        switch (sourceType) {
            case TERADATA: return "teradbConn";
            case MSSQL: return "jdbcConn";
            case ORACLE: return "oracleConn";
            case HIVE: return "hiveConn"; 
            default: return null;
        }
    }
    private String resolveIngestionDataFrameName(String tableNameFromJira) { return tableNameFromJira.toLowerCase(Locale.ROOT) + "_df"; }
    private String determineStagingFilePath(JsonGenerationConfig.SourceType sourceType, Sor sor, JiraStoryIntake jira) { /* ... */ 
        if (sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "Hemi".equalsIgnoreCase(sor.getSorName()) && "demog_exp_e3_load".equalsIgnoreCase(jira.getTableName())) {
            return "/datalake/utcap/hemi/sanitized";
        }
        return "${EXP_EDL_PATH}";
    }
    private String resolveIngestionDelimiter(JsonGenerationConfig.SourceType sourceType) { /* ... */ 
        if (sourceType == JsonGenerationConfig.SourceType.FILE_PIPE) return "|";
        if (sourceType == JsonGenerationConfig.SourceType.FILE_COMMA) return ",";
        return null;
    }
    private ObjectNode buildPrebuiltIngestionAdditionalReadConfigs(JsonGenerationConfig.SourceType sourceType, ObjectMapper mapper) { /* ... */ 
        if (isFileSource(sourceType)) { // Now for FW, Pipe, Comma
            ObjectNode arNode = mapper.createObjectNode();
            arNode.put("quote", "\"");
            arNode.put("escape", "\"");
            return arNode;
        }
        return null;
    }
    private ObjectNode buildPrebuiltDimcChecksNode(List<Integer> chosenIndices, ObjectMapper mapper) throws JsonProcessingException { /* ... */
        List<Map<String, Object>> selectedChecks = getChosenDimcChecksListInternal(chosenIndices);
        ObjectNode dimcChecksWrapper = mapper.createObjectNode();
        dimcChecksWrapper.put("isDimcChecksEnabled", "Y"); // Standardized to "Y"
        ArrayNode dimcCheckArray = mapper.valueToTree(selectedChecks);
        dimcChecksWrapper.set("dimcCheck", dimcCheckArray);
        return dimcChecksWrapper;
    }

    private List<JsonGenerationConfig.FlowDetailConfig> prepareFlowDetailsList(
        JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
        JiraStoryIntake jira, Sor sor, ApplicationAuthorizer auth, SQLScript sqlScript, 
        String ingestionDataFrameNameIfAny, ObjectMapper mapper
    ) { /* ... complex logic as derived before, using standardized flags and names ... */ 
        List<JsonGenerationConfig.FlowDetailConfig> flowConfigs = new ArrayList<>();
        int numberOfFlows = (sourceType == JsonGenerationConfig.SourceType.MSSQL || sourceType == JsonGenerationConfig.SourceType.ORACLE) ? 2 : 1;
        String commonHivePartitionColumn = jira.getApplicationName().toLowerCase(Locale.ROOT) + "_business_effective_date";

        for (int i = 0; i < numberOfFlows; i++) {
            String flowId = String.valueOf(i + 1); boolean isFirstFlow = (i == 0);
            String flowZone = JsonGenerationConfig.SANITIZED_ZONE; if (numberOfFlows == 2 && isFirstFlow) flowZone = JsonGenerationConfig.RAW_ZONE;
            
            String querySourceForFlow = ingestionDataFrameNameIfAny;
            String sourceHiveDbForDirectQuery = (jira.getSourceSchema() != null ? jira.getSourceSchema() : "${SOURCE_HIVE_DATABASE}");
            if (sourceType == JsonGenerationConfig.SourceType.HIVE) querySourceForFlow = sourceHiveDbForDirectQuery + "." + jira.getSourceTableName();

            String flowDesc = sor.getSorName().toUpperCase(Locale.ROOT) + JsonGenerationConfig.TABLE_TEXT + jira.getTableName() + JsonGenerationConfig.INGESTION_TO_TEXT + jira.getApplicationName() + " - " + flowZone;
            String query;
            String registerName = jira.getTableName().toLowerCase(Locale.ROOT) + "_" + flowZone;
            String flowDatabaseName = ""; String storageName = ""; String targetTablePath = null;
            
            String isCreateFlowTargetTable = "N"; String isCacheEnabledFlow = "Y"; String sendEmailFlow = "N"; String saveOutputFlow = "Y";

            if (targetType == JsonGenerationConfig.TargetType.GCP) {
                query = sqlScript.getSrcTableQuery() + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                if (sourceType == JsonGenerationConfig.SourceType.HIVE) query = "SELECT * FROM " + querySourceForFlow + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                flowDesc = "Extraction of " + (jira.getSourceTableName() != null ? jira.getSourceTableName() : jira.getTableName()) + " for CDMP processing.";
                registerName = jira.getTableName().toLowerCase(Locale.ROOT) + "_extract_df";
                saveOutputFlow = "N"; 
            } else { // Hive Target
                if (sourceType == JsonGenerationConfig.SourceType.HIVE) query = "SELECT * FROM " + querySourceForFlow;
                else if (querySourceForFlow != null) {
                     String baseSelect = "SELECT *";
                     if (isFileSource(sourceType)) {
                        baseSelect = "SELECT df.*";
                        String auditCols = ", '${UTCAP_CRTE_BY_ID}' as utcap_crte_by_id, '${UTCAP_CRTE_DTTM}' as utcap_crte_dttm, '${UTCAP_DELTA_CD}' as utcap_delta_cd, '${UTCAP_ERR_FLG_IND}' as utcap_err_flg_ind, '${UTCAP_ERR_TXT}' as utcap_err_txt, '${UTCAP_RUN_ID}' as utcap_run_id, '${UTCAP_SOR_CD}' as utcap_sor_cd, '${UTCAP_BUSINESS_EFFECTIVE_DATE}' as utcap_business_effective_date";
                        query = baseSelect + auditCols + " FROM " + querySourceForFlow + " df";
                     } else query = baseSelect + " FROM " + querySourceForFlow;
                } else query = sqlScript.getSrcDataFrameQuery();

                if (isFileSource(sourceType)) {
                    storageName = "${TARGET_TABLE_NAME}"; targetTablePath = "${EXP_EDL_PATH}/${TARGET_TABLE_NAME}";
                    flowDatabaseName = "${HIVE_TARGET_DBNAME}"; 
                } else { 
                    Map<String, String> storageNameMap = Map.of("CNAPP", "consumer", "UTCAP", "utcap");
                    String appSubDir = storageNameMap.getOrDefault(jira.getApplicationName(), jira.getApplicationName().toLowerCase());
                    storageName = "/datalake${EDL_ENV}/" + appSubDir + "/" + sor.getDomainName().toLowerCase(Locale.ROOT) + "/" + sor.getSorName().toLowerCase(Locale.ROOT) + "/" + flowZone + "/" + jira.getTableName();
                    flowDatabaseName = "${ETL_ENV}" + appSubDir + "_" + sor.getSorName().toLowerCase() + "_" + flowZone;
                }
            }
            flowConfigs.add(new JsonGenerationConfig.FlowDetailConfig(flowId, flowDesc, query, registerName, flowDatabaseName, storageName, targetTablePath, commonHivePartitionColumn, isCreateFlowTargetTable, "Y", isCacheEnabledFlow, sendEmailFlow, "N", saveOutputFlow));
        }
        return flowConfigs;
    }
    private List<JsonGenerationConfig.CheckDetailConfig> prepareCheckDetailsList(
        JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
        JiraStoryIntake jira, ApplicationAuthorizer auth,
        List<JsonGenerationConfig.FlowDetailConfig> flowDetails, String ingestionDataFrameName, ObjectMapper mapper
    ) { /* ... complex logic as derived before, using standardized flags ... */ 
        List<JsonGenerationConfig.CheckDetailConfig> checkConfigs = new ArrayList<>();
        if (targetType == JsonGenerationConfig.TargetType.GCP || flowDetails.isEmpty()) return checkConfigs;

        for (JsonGenerationConfig.FlowDetailConfig currentFlow : flowDetails) {
            String checkId = currentFlow.getFlowId(); String checkSourceQueryDataFrame;
            String sourceHiveDb = jira.getSourceSchema() != null ? jira.getSourceSchema() : "${SOURCE_HIVE_DATABASE}";
            if (sourceType == JsonGenerationConfig.SourceType.HIVE) checkSourceQueryDataFrame = sourceHiveDb + "." + jira.getSourceTableName();
            else if (ingestionDataFrameName != null) checkSourceQueryDataFrame = ingestionDataFrameName;
            else { LOG.warn("Cannot determine source DataFrame for data check for source: {} and flow: {}", sourceType, currentFlow.getFlowId()); continue; }
            
            String checkSourceQuery = "select count(*) from " + checkSourceQueryDataFrame;
            String checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowDatabaseName() + "." + jira.getTableName() + " where " + currentFlow.getResolvedFlowHivePartitionColumn() + "='${EAPP_DCT_BUSINESS_EFFECTIVE_DATE}'";
            if (sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "demog_exp_e3_load_sanitized".equals(currentFlow.getResolvedFlowRegisterName())) {
                 checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowRegisterName();
            }
            checkConfigs.add(new JsonGenerationConfig.CheckDetailConfig(checkId, "HHRCNTCHK", "Data Count Check", checkSourceQuery, checkTargetQuery, "Y", "Y", "N", "N", "N"));
        }
        return checkConfigs;
    }
    private String determineDataChecksAcDcDatabase(JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType, ApplicationAuthorizer auth, Sor sor, JiraStoryIntake jira, boolean checksArePresent) { /* ... same as before, using passed jira ... */
        if (checksArePresent && targetType != JsonGenerationConfig.TargetType.GCP) {
            if(sourceType == JsonGenerationConfig.SourceType.MSSQL || sourceType == JsonGenerationConfig.SourceType.ORACLE) return "${ETL_ENV}consumer_audit";
            if(sourceType == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH && "utcap".equalsIgnoreCase(jira.getApplicationName())) return "utcap_curated_mu_audit";
            if((sourceType == JsonGenerationConfig.SourceType.FILE_PIPE || sourceType == JsonGenerationConfig.SourceType.FILE_COMMA) &&
                "utcap".equalsIgnoreCase(jira.getApplicationName()) && "corelogic".equalsIgnoreCase(sor.getSorName())) return "${UTCAP_AUDIT_DB}";
            return auth.getAuditTable();
        }
        return "${AUDIT_DATABASE}";
    }
    private ObjectNode buildPrebuiltDataChecksPreChecksNode(JsonGenerationConfig.SourceType sourceType, ObjectMapper mapper) { /* ... same as before ... */ 
        ObjectNode preChecksNode = mapper.createObjectNode();
        boolean overallPreCheckEnabled = sourceType != JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
        preChecksNode.put("isPreCheckEnabled", overallPreCheckEnabled ? "Y" : "${PRECHECK_ENABLED}");
        ArrayNode preCheckArray = preChecksNode.putArray("preCheck");
        boolean isDbAsSrc = isDatabaseSource(sourceType); boolean isFileAsSrc = isFileSource(sourceType);
        addPreCheckItem(preCheckArray, "ServiceIDCheck", overallPreCheckEnabled ? "Y" : "N", "Y", mapper);
        // Add other prechecks, ensure mapper is passed to addPreCheckItem
        addPreCheckItem(preCheckArray, "TargetDataBaseAvailabilityCheck", overallPreCheckEnabled && !isFileAsSrc ? "Y" : "N", "Y", mapper); // Example conditional enabling
        addPreCheckItem(preCheckArray, "TargetTableAvailabilityCheck", overallPreCheckEnabled && !isFileAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "TargetTablePathWritableCheck", overallPreCheckEnabled ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "FlowSeqDependenciesCheck", "N", "N", mapper);
        addPreCheckItem(preCheckArray, "SourceDataBaseAvailabilityCheck", overallPreCheckEnabled && isDbAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "SourceTableAvailabilityCheck", overallPreCheckEnabled && isDbAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "SourceTableEmptyCheck", "N", "N", mapper);
        addPreCheckItem(preCheckArray, "SourcePathAvailabilityCheck", overallPreCheckEnabled && isFileAsSrc ? "Y" : "N", "Y", mapper);
        addPreCheckItem(preCheckArray, "SourcePathEmptyCheck", "N", "N", mapper);
        return preChecksNode;
    }
     private void addPreCheckItem(ArrayNode array, String name, String enabled, String failOnError, ObjectMapper mapper) { /* ... */ 
        ObjectNode item = mapper.createObjectNode();
        item.put("preCheckName", name); item.put("isPreCheckEnabled", enabled); item.put("isFailJobOnPreCheckError", failOnError);
        array.add(item);
    }


    // --- Generic Section Builders (Unchanged from previous, they consume from config) ---
    // buildGenericIngestionNode, buildGenericFlowsNode, buildGenericDataChecksNode, buildGenericFileExtractsNode
    // createEmailNode, determineJobDescription, determineIngestionDescription

    private ObjectNode buildGenericIngestionNode(JsonGenerationConfig config) { /* ... same as previous full code file ... */ 
        Set<String> fields = config.getIngestionFieldsToInclude();
        if (fields.isEmpty()) return null;
        ObjectNode item = objectMapper.createObjectNode();
        if (fields.contains("ingestionId")) item.put("ingestionId", "1");
        if (fields.contains("ingestionDesc")) item.put("ingestionDesc", determineIngestionDescription(config));
        if (fields.contains("isIngestionEnabled")) item.put("isIngestionEnabled", "Y");
        if (fields.contains("sourceType")) item.put("sourceType", config.getResolvedIngestionSourceTypeValue());
        if (fields.contains("ConnectionHeader") && config.getResolvedIngestionConnectionHeaderValue() != null) item.put("ConnectionHeader", config.getResolvedIngestionConnectionHeaderValue());
        if (fields.contains("delimiter") && config.getResolvedIngestionDelimiter() != null) item.put("delimiter", config.getResolvedIngestionDelimiter());
        if (fields.contains("dataFrameName")) item.put("dataFrameName", config.getResolvedIngestionDataFrameName());
        if (fields.contains("isCreateTargetTable")) item.put("isCreateTargetTable", config.getIngestionIsCreateTargetTableValue());
        if (fields.contains("isCacheEnabled")) item.put("isCacheEnabled", config.getIngestionIsCacheEnabledValue());
        if (fields.contains("isOverWritePartition")) item.put("isOverWritePartition", config.getIngestionIsOverWritePartitionValue());
        if (fields.contains("abortIfDuplicate")) item.put("abortIfDuplicate", "N");
        if (fields.contains("abortIfEmpty")) item.put("abortIfEmpty", "N");
        if (fields.contains("isRunEMHReport")) item.put("isRunEMHReport", "N");
        if (fields.contains("sourceSQLQuery") && config.getResolvedIngestionSourceSQLQuery() != null) item.put("sourceSQLQuery", config.getResolvedIngestionSourceSQLQuery());
        if (fields.contains("sourceName") && config.getResolvedIngestionSourceName() != null) item.put("sourceName", config.getResolvedIngestionSourceName());
        if (fields.contains("dataFilePath") && config.getResolvedIngestionDataFilePath() != null) item.put("dataFilePath", config.getResolvedIngestionDataFilePath());
        if (fields.contains("schemaFilePath") && config.getResolvedIngestionSchemaFilePath() != null) item.put("schemaFilePath", config.getResolvedIngestionSchemaFilePath());
        if (fields.contains("stagingFilePath") && config.getResolvedIngestionStagingFilePath() != null) item.put("stagingFilePath", config.getResolvedIngestionStagingFilePath());
        if (fields.contains("isHeaderEnabled") && config.getResolvedIngestionIsHeaderEnabledValue() != null) item.put("isHeaderEnabled", config.getResolvedIngestionIsHeaderEnabledValue());
        if (fields.contains("saveOutput") && config.getResolvedIngestionSaveOutputFlag() != null) item.put("saveOutput", config.getResolvedIngestionSaveOutputFlag());
        if (fields.contains("targetPartition") && config.getResolvedIngestionTargetPartition() != null) item.put("targetPartition", config.getResolvedIngestionTargetPartition());
        if (fields.contains("targetType") && config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) item.put("targetType", "parquet");
        if (fields.contains("additionalReadConfigs") && config.getPrebuiltIngestionAdditionalReadConfigsNode() != null) item.set("additionalReadConfigs", config.getPrebuiltIngestionAdditionalReadConfigsNode());
        if (fields.contains("dimcChecks") && config.getPrebuiltIngestionDimcChecksNode() != null) item.set("dimcChecks", config.getPrebuiltIngestionDimcChecksNode());
        ObjectNode finalNode = objectMapper.createObjectNode();
        if(item.size() > 0) addToArray(finalNode, "ingestion", item);
        return finalNode.size() > 0 ? finalNode : null;
    }
    private ObjectNode buildGenericFlowsNode(JsonGenerationConfig config) { /* ... same as previous full code file ... */ 
        if (!config.isIncludeFlows() || config.getFlowDetailsList().isEmpty()) return null;
        ObjectNode flowsNode = objectMapper.createObjectNode();
        ArrayNode flowArray = flowsNode.putArray("flow");
        for (JsonGenerationConfig.FlowDetailConfig detail : config.getFlowDetailsList()) {
            ObjectNode item = objectMapper.createObjectNode();
            item.put("flowId", detail.getFlowId());
            item.put("flowDesc", detail.getResolvedFlowDescription());
            item.put("isFlowEnabled", "Y");
            item.put("query", detail.getResolvedFlowQuery());
            item.put("registerName", detail.getResolvedFlowRegisterName());
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) {
                item.put("saveOutput", detail.getSaveOutputValue());
                if(detail.getResolvedFlowDatabaseName()!=null) item.put("flowDataBaseName", detail.getResolvedFlowDatabaseName());
                if(detail.getResolvedFlowStorageName()!=null) item.put("storageName", detail.getResolvedFlowStorageName());
                if(detail.getResolvedFlowTargetTablePath()!=null) item.put("targetTablePath", detail.getResolvedFlowTargetTablePath());
                if(detail.getResolvedFlowHivePartitionColumn()!=null) item.put("hivePartitionColumn", detail.getResolvedFlowHivePartitionColumn());
                item.put("isCreateFlowTargetTable", detail.getIsCreateFlowTargetTableValue());
                item.put("isOverWritePartition", detail.getIsOverWritePartitionValue());
                item.put("isCacheEnabled", detail.getIsCacheEnabledValue());
                item.put("sendEmail", detail.getSendEmailValue());
                item.put("abortIfEmpty", detail.getAbortIfEmptyValue());
            }
            flowArray.add(item);
        }
        return flowsNode;
    }
    private ObjectNode buildGenericDataChecksNode(JsonGenerationConfig config) { /* ... same as previous full code file ... */
        if (!config.isIncludeDataChecks()) return null;
        boolean hasCheckDetails = !config.getCheckDetailsList().isEmpty();
        ObjectNode preChecksNodeFromConfig = config.getPrebuiltDataChecksPreChecksNode();
        boolean hasPreChecks = preChecksNodeFromConfig != null && preChecksNodeFromConfig.has("preCheck") && preChecksNodeFromConfig.get("preCheck").size() > 0 ;
        if (!hasCheckDetails && !hasPreChecks) {
            if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP && "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase())) return null;
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE && "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase()) ) return null;
        }
        ObjectNode dataChecksNode = objectMapper.createObjectNode();
        if(config.getResolvedDataChecksAcDcDatabase() != null) dataChecksNode.put("acDCDatabase", config.getResolvedDataChecksAcDcDatabase());
        dataChecksNode.put("isTrendingEnabled", "N"); 
        if (hasCheckDetails) {
            ArrayNode checkArray = dataChecksNode.putArray("check"); 
            for (JsonGenerationConfig.CheckDetailConfig detail : config.getCheckDetailsList()) {
                ObjectNode item = objectMapper.createObjectNode();
                item.put("checkId", detail.getCheckId()); item.put("checkName", detail.getCheckName()); item.put("checkDesc", detail.getCheckDescription());
                item.put("isCheckEnabled", detail.getIsCheckEnabledValue()); item.put("isUserSupplyQuery", detail.getIsUserSupplyQueryValue());
                item.put("readSourceQueryFromPath", detail.getReadSourceQueryFromPathValue()); item.put("sourceQuery", detail.getResolvedCheckSourceQuery());
                item.put("targetQuery", detail.getResolvedCheckTargetQuery()); item.put("readTargetQueryFromPath", detail.getReadTargetQueryFromPathValue());
                item.put("isFailJobOnDataCheckError", detail.getIsFailJobOnDataCheckErrorValue());
                checkArray.add(item);
            }
        }
        if(hasPreChecks) dataChecksNode.set("preChecks", preChecksNodeFromConfig);
        return dataChecksNode.size() == 0 || (dataChecksNode.size() <=2 && !hasChecks && !hasPreChecks && dataChecksNode.has("acDCDatabase") && dataChecksNode.has("isTrendingEnabled")) ? null : dataChecksNode;
    }
    private ObjectNode buildGenericFileExtractsNode(JsonGenerationConfig config) { /* ... same as previous full code file ... */
        if (!config.isIncludeFileExtracts()) return null;
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = fileExtractsNode.putArray("fileExtract");
        JiraStoryIntake jira = config.getJiraStoryIntake();
        ObjectNode item = objectMapper.createObjectNode();
        item.put("fileExtractId", "1");
        item.put("isFileExtractEnabled", "${IS_FILE_EXTRACT_ENABLED}"); 
        String sourceDescriptionPart = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "hive data" : jira.getTableName().toLowerCase(Locale.ROOT);
        item.put("fileDescription", "Extracting " + sourceDescriptionPart + " to avro file for CDMP");
        item.put("subjectArea", config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "cdmp using FileExtract" : "CDMP");
        item.put("zone", JsonGenerationConfig.RAW_ZONE);
        item.put("system", "cdmp");
        item.put("fileType", "avro");
        String flowRegisterNameForExtract = config.getFlowDetailsList().isEmpty() ? "flow_df" : config.getFlowDetailsList().get(0).getResolvedFlowRegisterName();
        item.put("extractQuery", "select * from " + flowRegisterNameForExtract); 
        item.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        item.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        ObjectNode cdmpParams = objectMapper.createObjectNode();
        cdmpParams.put("outputFormat", "avro");
        String cdmpFileNamePrefix = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "UTCAPT_" : "";
        cdmpParams.put("cdmpFileName", cdmpFileNamePrefix + jira.getTableName().toLowerCase(Locale.ROOT) + "_cdmp_extract");
        cdmpParams.put("cdmpPatternMode", "daily");
        cdmpParams.put("isCustomPartitionRequired", "true"); 
        cdmpParams.put("desiredPartFileSize", "${PART_FILE_SIZE}");
        item.set("cdmpAdditionalParams", cdmpParams);
        fileExtractArray.add(item);
        return fileExtractsNode;
    }
    
    // Static helper for DIMC checks, now public to be accessible by JsonGenerationConfig
    public static List<Map<String, Object>> getChosenDimcChecksListInternal(List<Integer> chosenIndices, List<Map<String, Object>> masterList) {
        if (chosenIndices == null || chosenIndices.isEmpty() || masterList == null || masterList.isEmpty()) return Collections.emptyList();
        return chosenIndices.stream()
                .filter(index -> index != null && index >= 0 && index < masterList.size())
                .map(index -> new HashMap<>(masterList.get(index)))
                .collect(Collectors.toList());
    }
    private static synchronized void populateDimcChecksMasterList() { /* ... same as before, ensure HashMap<String, Object> ... */ 
        if (!DIMC_CHECKS_MASTER_LIST.isEmpty()) return;
        LOG.info("Populating DIMC Checks Master List...");
        Map<String, Object> check0=new HashMap<>(); check0.put("dimcCheckname","duplicateFileLoadChecks"); check0.put("isDimcCheckEnabled","${DUPLICATE_FILELOAD_CHECK_ENABLED}"); check0.put("isFailJobOnError","${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check0));
        Map<String, Object> check1=new HashMap<>(); check1.put("dimcCheckname","dataVolumeConsistencyCheck"); check1.put("isDimcCheckEnabled","${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}"); check1.put("upperThreshold","${UPPER_THRESHOLD}"); check1.put("lowerThreshold","${LOWER_THRESHOLD}"); check1.put("numPastDays","${NUM_PAST_DAYS}"); check1.put("isFailJobOnError","${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check1));
        Map<String, Object> check2=new HashMap<>(); check2.put("dimcCheckname","missingFileCheck"); check2.put("isDimcCheckEnabled","${MISSING_FILE_CHECK_ENABLED}"); check2.put("isFailJobOnError","${MISSING_FILE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check2));
        Map<String, Object> check3=new HashMap<>(); check3.put("dimcCheckname","deliveryCheck"); check3.put("isDimcCheckEnabled","${DELIVERY_CHECK_ENABLED}"); check3.put("SLA","${DELIVERY_CHECK_SLA}"); check3.put("isFailJobOnError","${DELIVERY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check3));
        Map<String, Object> check4=new HashMap<>(); check4.put("dimcCheckname","missingDbObjectCheck"); check4.put("isDimcCheckEnabled","${MISSING_DB_OBJECT_CHECK_ENABLED}"); check4.put("isFailJobOnError","${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check4));
        Map<String, Object> check5=new HashMap<>(); check5.put("dimcCheckname","emptySourceCheck"); check5.put("isDimcCheckEnabled","${EMPTY_SOURCE_CHECK_ENABLED}"); check5.put("isFailJobOnError","${EMPTY_SOURCE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check5));
        Map<String, Object> check6=new HashMap<>(); check6.put("dimcCheckname","dataVolumeCompletenessCheck"); check6.put("controlFileRowFilterByIndex","/apps/ndm/etlutcap/inbound/experian.ctl"); check6.put("controlFileFieldValueByIndex","recordCount=3,fileDate=1"); check6.put("isDimcCheckEnabled","N"); check6.put("isFailJobOnError","N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check6));
        Map<String, Object> check7=new HashMap<>(); check7.put("dimcCheckname","fileDateCheck"); check7.put("isDimcCheckEnabled","${FILE_DATE_CHECK_ENABLED}"); check7.put("isFailJobOnError","${FILE_DATE_CHECK_FAILURE_FLAG}"); check7.put("fileDateFromFileNamePattern",".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check7.put("fileDateFromFileNameFormat","yyyyMMdd"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check7));
        Map<String, Object> check8=new HashMap<>(); check8.put("dimcCheckname","businessDateCheck"); check8.put("businessDateFromFileNamePattern",".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check8.put("businessDateFromFileNameFormat","yyyyMMdd"); check8.put("upperThresholdInDays","20"); check8.put("lowerThresholdInDays","20"); check8.put("controlFileRowFilterByIndex","2"); check8.put("controlFileFieldValueByIndex","fileDate=1"); check8.put("isDimcCheckEnabled","N"); check8.put("isFailJobOnError","N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check8));
        LOG.info("DIMC Checks Master List populated with {} items.", DIMC_CHECKS_MASTER_LIST.size());
    }
    private JsonGenerationConfig.SourceType parseSourceType(String sorTypeStr) { /* ... same as before ... */ 
        if (sorTypeStr == null) return JsonGenerationConfig.SourceType.UNKNOWN;
        String typeLower = sorTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "teradata": return JsonGenerationConfig.SourceType.TERADATA;
            case "file-fixedwidth": case "fixedwidthfile": return JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
            case "file-pipe": return JsonGenerationConfig.SourceType.FILE_PIPE;
            case "file-comma": return JsonGenerationConfig.SourceType.FILE_COMMA;
            case "mssql": return JsonGenerationConfig.SourceType.MSSQL;
            case "oracle": return JsonGenerationConfig.SourceType.ORACLE;
            case "hive": return JsonGenerationConfig.SourceType.HIVE;
            case "file": 
                 LOG.warn("Generic 'file' sourceType string found. Defaulting to Comma. For specific behavior, ensure input sorType is 'file-comma', 'file-pipe', or 'file-fixedwidth'. Input: {}", sorTypeStr);
                 return JsonGenerationConfig.SourceType.FILE_COMMA; 
            default:
                LOG.warn("Unrecognized Source Type string: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.UNKNOWN;
        }
    }
    private JsonGenerationConfig.TargetType parseTargetType(String targetTypeStr) { /* ... same as before ... */ 
        if (targetTypeStr == null) return JsonGenerationConfig.TargetType.UNKNOWN;
        String typeLower = targetTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "hive": return JsonGenerationConfig.TargetType.HIVE;
            case "gcp": return JsonGenerationConfig.TargetType.GCP;
            default:
                LOG.warn("Unrecognized Target Type string: {}", targetTypeStr);
                return JsonGenerationConfig.TargetType.UNKNOWN;
        }
    }
    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) { /* ... same as before ... */ 
        if (item == null || item.size() == 0) return;
        ArrayNode array = parent.has(arrayName) && parent.get(arrayName).isArray() ?
                (ArrayNode) parent.get(arrayName) : parent.putArray(arrayName);
        array.add(item);
    }
     public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) { /* ... same as before ... */ 
         Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository.findById(requirementId);
         if (existingConnJsonRequirement.isPresent()) {
             return existingConnJsonRequirement.get();
         }
         ObjectNode rootNode = objectMapper.createObjectNode();
         ObjectNode application = objectMapper.createObjectNode();
         ObjectNode email = objectMapper.createObjectNode();
         String sorName;
         try {
             String baseRequirementId = requirementId.contains("_") ? requirementId.split("_")[0] : requirementId;
             List<Requirement> reqs = requirementRepository.findByRequirementId(baseRequirementId);
             if (reqs.isEmpty()) throw new ConfigurationException("Requirement not found in RequirementRepository for: " + baseRequirementId);
             sorName = reqs.get(0).getSorName();
         } catch (Exception e) {
             LOG.error("Failed to get SorName for ConnJson requirementId: {}. Error: {}", requirementId, e.getMessage(), e);
             sorName = "UNKNOWN_SOR";
         }
         String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName + " with Job Name $jobName has completed. " +
                           "</p><p> SourceDate =$sourceDate , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
         email.put("body", emailBody);
         ObjectNode job = objectMapper.createObjectNode();
         job.set("Email", email);
         application.set("job", job);
         rootNode.set("application", application);
         Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
         ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
         return connJsonRequirementRepository.save(connJsonRequirement);
    }
}
