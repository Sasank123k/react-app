package com.wellsfargo.utcap.service;

import com.wellsfargo.utcap.exception.ResourceNotFoundException;
import com.wellsfargo.utcap.model.DagSql;
import com.wellsfargo.utcap.model.HiveTable;
import com.wellsfargo.utcap.model.JiraStoryIntake;
import com.wellsfargo.utcap.model.SqlDatatype;
import com.wellsfargo.utcap.repository.DagSqlRepository;
import com.wellsfargo.utcap.repository.HiveTableRepository;
import com.wellsfargo.utcap.repository.JiraStoryIntakeRepository;
import com.wellsfargo.utcap.repository.SqlDatatypeRepository;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.*;

@Service
public class DagSqlService {

    private static final String DEFAULT_MAPPING_ID = "defaultMapping";
    private static final String DEFAULT_TYPE       = "STRING";
    private static final String INTEGER_TYPE       = "INTEGER";
    private static final int    PAIR_SIZE          = 2;

    @Autowired
    private DagSqlRepository            dagSqlRepository;

    @Autowired
    private JiraStoryIntakeRepository   jiraStoryIntakeRepository;

    @Autowired
    private HiveTableRepository         hiveTableRepository;

    @Autowired
    private SqlDatatypeRepository       sqlDatatypeRepository;

    /**
     * Fetches an existing DagSql or generates & saves a new one.
     */
    public DagSql getDagSql(String requirementId) throws ResourceNotFoundException {
        Optional<DagSql> existing = dagSqlRepository.findByRequirementId(requirementId);
        return existing.orElseGet(() -> {
            try {
                return generateAndSaveDagSql(requirementId);
            } catch (ResourceNotFoundException e) {
                // wrap checked into unchecked so this lambda compiles
                throw new RuntimeException(e);
            }
        });
    }

    /**
     * Generates the two SQL files and persists a new DagSql.
     */
    public DagSql generateAndSaveDagSql(String requirementId) throws ResourceNotFoundException {
        JiraStoryIntake jiraStory = jiraStoryIntakeRepository
            .findById(requirementId)
            .orElseThrow(() -> new ResourceNotFoundException(
                "JiraStoryIntake not found for requirementId: " + requirementId));

        // source metadata
        String sourceSchema    = jiraStory.getSourceSchema();
        String sourceTableName = jiraStory.getSourceTableName();
        String tableIdentifier = sourceSchema + "." + sourceTableName;

        // target name construction
        String targetSchema    = jiraStory.getTargetSchema();
        String appName         = jiraStory.getApplicationName();
        String targetTableName = jiraStory.getTargetTableName();
        String sqlDbTableName  = targetSchema + "." + appName + targetTableName;

        // fetch HiveTable record
        List<HiveTable> hiveList = hiveTableRepository.findByTableName(tableIdentifier);
        HiveTable hive = Optional.ofNullable(hiveList)
            .filter(l -> !l.isEmpty())
            .map(l -> l.get(0))
            .orElseThrow(() -> new ResourceNotFoundException(
                "HiveTable not found for table: " + tableIdentifier));

        // fetch type mapping
        SqlDatatype sqlDatatype = sqlDatatypeRepository
            .findById(DEFAULT_MAPPING_ID)
            .orElseGet(this::getDefaultSqlDatatype);
        Map<String,String> mapping = sqlDatatype.getMappings();

        // build column definitions
        String cols = generateColumnDefinitions(hive.getFileSetAttr(), mapping);

        // build SQL
        String createSql = "CREATE TABLE " + sqlDbTableName + "(\n" + cols + "\n);";
        String deleteSql = "DROP TABLE "    + sqlDbTableName;

        // persist & return
        DagSql record = new DagSql();
        record.setRequirementId(requirementId);
        record.setCreateSqlContent(createSql);
        record.setDeleteSqlContent(deleteSql);
        record.setCreatedAt(LocalDateTime.now());
        record.setUpdatedAt(LocalDateTime.now());
        return dagSqlRepository.save(record);
    }

    /**
     * Parses fileSetAttr into “name TYPE” lines.
     */
    private String generateColumnDefinitions(String fileSetAttr,
                                             Map<String,String> mapping) {
        String[] tokens = fileSetAttr.split("\\|");
        List<String> clean = new ArrayList<>();
        for (String t : tokens) {
            if (t != null && !t.trim().isEmpty()) {
                clean.add(t.trim());
            }
        }
        if (clean.size() % PAIR_SIZE != 0) {
            throw new IllegalArgumentException(
                "Invalid fileSetAttr format; expected pairs of name|type");
        }

        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < clean.size(); i += PAIR_SIZE) {
            String rawName = clean.get(i);
            String colName = rawName.startsWith("#")
                ? rawName.substring(1)
                : rawName;

            String hiveType = clean.get(i+1).toUpperCase(Locale.ROOT);
            String baseType = hiveType.contains("(")
                ? hiveType.substring(0, hiveType.indexOf('('))
                : hiveType;

            String bqType = mapping.getOrDefault(baseType, DEFAULT_TYPE);
            sb.append("  ").append(colName).append(" ").append(bqType);

            if (i < clean.size() - PAIR_SIZE) {
                sb.append(",\n");
            }
        }
        return sb.toString();
    }

    /**
     * Updates existing DagSql contents.
     */
    public DagSql updateDagSql(String requirementId,
                               String createSqlContent,
                               String deleteSqlContent)
            throws ResourceNotFoundException {
        DagSql existing = dagSqlRepository
            .findByRequirementId(requirementId)
            .orElseThrow(() -> new ResourceNotFoundException(
                "DagSql not found for requirementId: " + requirementId));

        existing.setCreateSqlContent(createSqlContent);
        existing.setDeleteSqlContent(deleteSqlContent);
        existing.setUpdatedAt(LocalDateTime.now());
        return dagSqlRepository.save(existing);
    }

    /**
     * Fallback in‐code mapping if Mongo document is missing.
     */
    private SqlDatatype getDefaultSqlDatatype() {
        Map<String,String> defaults = new HashMap<>();
        defaults.put("STRING",   DEFAULT_TYPE);
        defaults.put("CHAR",     DEFAULT_TYPE);
        defaults.put("VARCHAR",  DEFAULT_TYPE);
        defaults.put("INT",      INTEGER_TYPE);
        defaults.put("INTEGER",  INTEGER_TYPE);
        defaults.put("BIGINT",   INTEGER_TYPE);
        defaults.put("SMALLINT", INTEGER_TYPE);
        defaults.put("TINYINT",  INTEGER_TYPE);
        defaults.put("BOOLEAN",  "BOOLEAN");
        defaults.put("FLOAT",    "FLOAT");
        defaults.put("DOUBLE",   "FLOAT");
        defaults.put("DECIMAL",  "NUMERIC");
        defaults.put("DATE",     "DATE");
        defaults.put("TIMESTAMP","TIMESTAMP");
        defaults.put("BINARY",   "BYTES");

        SqlDatatype dt = new SqlDatatype();
        dt.setId(DEFAULT_MAPPING_ID);
        dt.setMappings(defaults);
        return dt;
    }
}
