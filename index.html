Below are complete sample implementations for your backend files based on your requirements and the package structure (using the base package of `com.wellsfargo.utcap`). These examples assume that you already have models and repositories for the JiraStoryIntake and HiveTable collections.

In summary, you’ll have:

1. A **DagSql** model, repository, service, and controller to generate and update the SQL files.
2. A **SqlDatatype** (or sqlDatatype) model and repository that stores the Hive‑to‑BigQuery type mapping in MongoDB.

---

### 1. DagSql Collection Files

#### 1.1 Model – DagSql.java  
*Location: `com/wellsfargo/utcap/model/DagSql.java`*

```java
package com.wellsfargo.utcap.model;

import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;
import java.time.LocalDateTime;

@Document(collection = "dagSql")
public class DagSql {

    @Id
    private String id;
    private String requirementId;
    private String createSqlContent;
    private String deleteSqlContent;
    private LocalDateTime createdAt;
    private LocalDateTime updatedAt;

    public DagSql() {
    }

    public DagSql(String requirementId, String createSqlContent, String deleteSqlContent) {
        this.requirementId = requirementId;
        this.createSqlContent = createSqlContent;
        this.deleteSqlContent = deleteSqlContent;
        this.createdAt = LocalDateTime.now();
        this.updatedAt = LocalDateTime.now();
    }

    // Getters and Setters

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public String getRequirementId() {
        return requirementId;
    }

    public void setRequirementId(String requirementId) {
        this.requirementId = requirementId;
    }

    public String getCreateSqlContent() {
        return createSqlContent;
    }

    public void setCreateSqlContent(String createSqlContent) {
        this.createSqlContent = createSqlContent;
    }

    public String getDeleteSqlContent() {
        return deleteSqlContent;
    }

    public void setDeleteSqlContent(String deleteSqlContent) {
        this.deleteSqlContent = deleteSqlContent;
    }

    public LocalDateTime getCreatedAt() {
        return createdAt;
    }

    public void setCreatedAt(LocalDateTime createdAt) {
        this.createdAt = createdAt;
    }

    public LocalDateTime getUpdatedAt() {
        return updatedAt;
    }

    public void setUpdatedAt(LocalDateTime updatedAt) {
        this.updatedAt = updatedAt;
    }
}
```

---

#### 1.2 Repository – DagSqlRepository.java  
*Location: `com/wellsfargo/utcap/repository/DagSqlRepository.java`*

```java
package com.wellsfargo.utcap.repository;

import com.wellsfargo.utcap.model.DagSql;
import org.springframework.data.mongodb.repository.MongoRepository;
import java.util.Optional;

public interface DagSqlRepository extends MongoRepository<DagSql, String> {
    Optional<DagSql> findByRequirementId(String requirementId);
}
```

---

#### 1.3 Service – DagSqlService.java  
*Location: `com/wellsfargo/utcap/service/DagSqlService.java`*

> **Note:** This service uses the existing JiraStoryIntake and HiveTable repositories (assumed to be defined already). It also uses the new SqlDatatypeRepository (see next section) to retrieve the Hive-to‑BigQuery mapping.

```java
package com.wellsfargo.utcap.service;

import com.wellsfargo.utcap.model.DagSql;
import com.wellsfargo.utcap.model.HiveTable;          // Assumed existing
import com.wellsfargo.utcap.model.JiraStoryIntake;      // Assumed existing
import com.wellsfargo.utcap.model.SqlDatatype;
import com.wellsfargo.utcap.repository.DagSqlRepository;
import com.wellsfargo.utcap.repository.HiveTableRepository;       // Assumed existing
import com.wellsfargo.utcap.repository.JiraStoryIntakeRepository;   // Assumed existing
import com.wellsfargo.utcap.repository.SqlDatatypeRepository;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import java.time.LocalDateTime;
import java.util.*;

@Service
public class DagSqlService {

    @Autowired
    private DagSqlRepository dagSqlRepository;

    @Autowired
    private JiraStoryIntakeRepository jiraStoryIntakeRepository;

    @Autowired
    private HiveTableRepository hiveTableRepository;

    @Autowired
    private SqlDatatypeRepository sqlDatatypeRepository;

    public DagSql getDagSql(String requirementId) {
        Optional<DagSql> existing = dagSqlRepository.findByRequirementId(requirementId);
        if (existing.isPresent()) {
            return existing.get();
        } else {
            return generateAndSaveDagSql(requirementId);
        }
    }

    public DagSql generateAndSaveDagSql(String requirementId) {
        // Retrieve the Jira story information
        // (Assumes JiraStoryIntake has getTargetSchema() and getTargetTableName() methods)
        var jiraStory = jiraStoryIntakeRepository.findById(requirementId)
                .orElseThrow(() -> new RuntimeException("JiraStoryIntake not found for requirementId: " + requirementId));

        String targetSchema = jiraStory.getTargetSchema();
        String targetTableName = jiraStory.getTargetTableName();
        String tableIdentifier = targetSchema + "." + targetTableName;

        // Retrieve Hive table record using the combined table identifier.
        // (Assumes HiveTableRepository.findByTableName returns a List; we use the first if available)
        List<HiveTable> hiveTableList = hiveTableRepository.findByTableName(tableIdentifier);
        HiveTable hiveTable = Optional.ofNullable(hiveTableList)
                .filter(list -> !list.isEmpty())
                .map(list -> list.get(0))
                .orElseThrow(() -> new RuntimeException("HiveTable record not found for table: " + tableIdentifier));

        // Retrieve the type conversion mapping from the sqlDatatype collection.
        SqlDatatype sqlDatatype = sqlDatatypeRepository.findById("defaultMapping")
                .orElseGet(this::getDefaultSqlDatatype);
        Map<String, String> mapping = sqlDatatype.getMappings();

        // Parse fileSetAttr to generate the columns definition
        String fileSetAttr = hiveTable.getFileSetAttr();
        String columnDefinitions = generateColumnDefinitions(fileSetAttr, mapping);

        // Build the SQL strings
        String createSql = "CREATE TABLE " + tableIdentifier + "(\n" + columnDefinitions + "\n);";
        String deleteSql = "DROP TABLE " + tableIdentifier;

        // Create and save the DagSql record
        DagSql dagSql = new DagSql();
        dagSql.setRequirementId(requirementId);
        dagSql.setCreateSqlContent(createSql);
        dagSql.setDeleteSqlContent(deleteSql);
        dagSql.setCreatedAt(LocalDateTime.now());
        dagSql.setUpdatedAt(LocalDateTime.now());

        return dagSqlRepository.save(dagSql);
    }

    /**
     * Parses the fileSetAttr string into proper column definitions.
     * Expected fileSetAttr format example:
     * "encr_uik|string|#best_ecn|decimal(15,0)|#uik_comb_age_cd|string|..."
     */
    private String generateColumnDefinitions(String fileSetAttr, Map<String, String> mapping) {
        String[] tokens = fileSetAttr.split("\\|");
        List<String> tokenList = new ArrayList<>();
        for (String token : tokens) {
            if (token != null && !token.trim().isEmpty()) {
                tokenList.add(token.trim());
            }
        }

        if (tokenList.size() % 2 != 0) {
            throw new RuntimeException("Invalid fileSetAttr format; expected even number of tokens");
        }

        StringBuilder columnsBuilder = new StringBuilder();
        for (int i = 0; i < tokenList.size(); i += 2) {
            String columnName = tokenList.get(i);
            // Remove the '#' prefix if it exists (for subsequent columns)
            if (columnName.startsWith("#")) {
                columnName = columnName.substring(1);
            }
            String hiveType = tokenList.get(i + 1).toUpperCase();

            // If the type has parameters (e.g., DECIMAL(15,0)), extract the base type
            String baseType = hiveType;
            if (hiveType.contains("(")) {
                baseType = hiveType.substring(0, hiveType.indexOf("("));
            }

            // Convert the Hive type to BigQuery type using the mapping; default to STRING if not found.
            String bigQueryType = mapping.getOrDefault(baseType, "STRING");

            columnsBuilder.append("  ").append(columnName).append(" ").append(bigQueryType);
            if (i < tokenList.size() - 2) {
                columnsBuilder.append(",\n");
            }
        }
        return columnsBuilder.toString();
    }

    public DagSql updateDagSql(String requirementId, String createSqlContent, String deleteSqlContent) {
        DagSql dagSql = dagSqlRepository.findByRequirementId(requirementId)
                .orElseThrow(() -> new RuntimeException("DagSql record not found for requirementId: " + requirementId));
        dagSql.setCreateSqlContent(createSqlContent);
        dagSql.setDeleteSqlContent(deleteSqlContent);
        dagSql.setUpdatedAt(LocalDateTime.now());
        return dagSqlRepository.save(dagSql);
    }

    /**
     * Provides a fallback mapping if no mapping document is found.
     */
    private SqlDatatype getDefaultSqlDatatype() {
        Map<String, String> defaultMapping = new HashMap<>();
        defaultMapping.put("STRING", "STRING");
        defaultMapping.put("CHAR", "STRING");
        defaultMapping.put("VARCHAR", "STRING");
        defaultMapping.put("INT", "INT64");
        defaultMapping.put("INTEGER", "INT64");
        defaultMapping.put("BIGINT", "INT64");
        defaultMapping.put("SMALLINT", "INT64");
        defaultMapping.put("TINYINT", "INT64");
        defaultMapping.put("BOOLEAN", "BOOL");
        defaultMapping.put("FLOAT", "FLOAT64");
        defaultMapping.put("DOUBLE", "FLOAT64");
        defaultMapping.put("DECIMAL", "NUMERIC");
        defaultMapping.put("DATE", "DATE");
        defaultMapping.put("TIMESTAMP", "TIMESTAMP");
        defaultMapping.put("BINARY", "BYTES");

        SqlDatatype sqlDatatype = new SqlDatatype();
        sqlDatatype.setId("defaultMapping");
        sqlDatatype.setMappings(defaultMapping);
        return sqlDatatype;
    }
}
```

---

#### 1.4 Controller – DagSqlController.java  
*Location: `com/wellsfargo/utcap/controller/DagSqlController.java`*

```java
package com.wellsfargo.utcap.controller;

import com.wellsfargo.utcap.model.DagSql;
import com.wellsfargo.utcap.service.DagSqlService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/api/sql")
public class DagSqlController {

    @Autowired
    private DagSqlService dagSqlService;

    @GetMapping("/{requirementId}")
    public ResponseEntity<DagSql> getDagSql(@PathVariable String requirementId) {
        try {
            DagSql dagSql = dagSqlService.getDagSql(requirementId);
            return ResponseEntity.ok(dagSql);
        } catch (Exception e) {
            // Log exception details as needed
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    @PostMapping("/{requirementId}")
    public ResponseEntity<DagSql> updateDagSql(@PathVariable String requirementId, @RequestBody DagSql updatedSql) {
        try {
            DagSql dagSql = dagSqlService.updateDagSql(requirementId, 
                    updatedSql.getCreateSqlContent(), 
                    updatedSql.getDeleteSqlContent());
            return ResponseEntity.ok(dagSql);
        } catch (Exception e) {
            // Log exception details as needed
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }
}
```

---

### 2. sqlDatatype Collection Files

This collection holds your Hive‑to‑BigQuery type mappings.

#### 2.1 Model – SqlDatatype.java  
*Location: `com/wellsfargo/utcap/model/SqlDatatype.java`*

```java
package com.wellsfargo.utcap.model;

import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;
import java.util.Map;

@Document(collection = "sqlDatatype")
public class SqlDatatype {

    @Id
    private String id;  // For example, "defaultMapping"
    private Map<String, String> mappings;

    public SqlDatatype() {
    }

    public SqlDatatype(String id, Map<String, String> mappings) {
        this.id = id;
        this.mappings = mappings;
    }

    // Getters and Setters

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public Map<String, String> getMappings() {
        return mappings;
    }

    public void setMappings(Map<String, String> mappings) {
        this.mappings = mappings;
    }
}
```

---

#### 2.2 Repository – SqlDatatypeRepository.java  
*Location: `com/wellsfargo/utcap/repository/SqlDatatypeRepository.java`*

```java
package com.wellsfargo.utcap.repository;

import com.wellsfargo.utcap.model.SqlDatatype;
import org.springframework.data.mongodb.repository.MongoRepository;
import java.util.Optional;

public interface SqlDatatypeRepository extends MongoRepository<SqlDatatype, String> {
    Optional<SqlDatatype> findById(String id);
}
```

---

### Testing the Endpoints

Using Postman you can test your endpoints as follows:

- **GET Endpoint:**  
  - **URL:**  
    `http://localhost:8080/api/sql/BZLS-4501`  
  - **Method:**  
    GET  
  - This will return the generated DagSql record with both the create and drop SQL strings.

- **POST Endpoint:**  
  - **URL:**  
    `http://localhost:8080/api/sql/BZLS-4501`  
  - **Method:**  
    POST  
  - **Headers:**  
    `Content-Type: application/json`  
  - **Body (raw JSON):**  
    ```json
    {
      "createSqlContent": "CREATE TABLE consumer_bmg_sanitized.acct_pma_rel_hist_cdb (\n  encr_uik STRING,\n  best_ecn NUMERIC\n);",
      "deleteSqlContent": "DROP TABLE consumer_bmg_sanitized.acct_pma_rel_hist_cdb"
    }
    ```
  - This call updates the SQL file contents in the DagSql record.

---

With this complete backend setup, you have:

- A mechanism to generate SQL files dynamically based on data from JiraStoryIntake and HiveTable.
- Parsing of the fileSetAttr string to extract column names and Hive types.
- Conversion of Hive types to BigQuery types driven by a MongoDB‑stored mapping.
- API endpoints to fetch and update the generated SQL files.

Let me know if you need further adjustments or details!






{
  "_id": "defaultMapping",
  "mappings": {
    "STRING": "STRING",
    "CHAR": "STRING",
    "VARCHAR": "STRING",
    "INT": "INT64",
    "INTEGER": "INT64",
    "BIGINT": "INT64",
    "SMALLINT": "INT64",
    "TINYINT": "INT64",
    "BOOLEAN": "BOOL",
    "FLOAT": "FLOAT64",
    "DOUBLE": "FLOAT64",
    "DECIMAL": "NUMERIC",
    "DATE": "DATE",
    "TIMESTAMP": "TIMESTAMP",
    "BINARY": "BYTES"
  }
}
