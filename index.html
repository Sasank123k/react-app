// src/main/java/com/wellsfargo/utcap/service/JsonRequirementService.java
package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.config.JsonGenerationConfig;
import com.wellsfargo.utcap.exception.ConfigurationException;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(JsonRequirementService.class);
    private static final List<Map<String, Object>> DIMC_CHECKS_MASTER_LIST = new ArrayList<>();

    private final ApplicationAuthRepository applicationAuthRepository;
    private final RequirementRepository requirementRepository;
    private final JsonRequirementRepository jsonRequirementRepository;
    private final JiraStoryIntakeRepository jiraStoryIntakeRepository;
    private final SorRepository sorRepository;
    private final SQLScriptRepository sqlScriptRepository;
    private final ConnJsonRequirementRepository connJsonRequirementRepository;
    private final ObjectMapper objectMapper;

    @Autowired
    public JsonRequirementService(ApplicationAuthRepository applicationAuthRepository,
            RequirementRepository requirementRepository,
            JsonRequirementRepository jsonRequirementRepository,
            JiraStoryIntakeRepository jiraStoryIntakeRepository,
            SorRepository sorRepository,
            SQLScriptRepository sqlScriptRepository,
            ConnJsonRequirementRepository connJsonRequirementRepository,
            ObjectMapper objectMapper) {
        this.applicationAuthRepository = applicationAuthRepository;
        this.requirementRepository = requirementRepository;
        this.jsonRequirementRepository = jsonRequirementRepository;
        this.jiraStoryIntakeRepository = jiraStoryIntakeRepository;
        this.sorRepository = sorRepository;
        this.sqlScriptRepository = sqlScriptRepository;
        this.connJsonRequirementRepository = connJsonRequirementRepository;
        this.objectMapper = objectMapper;
        populateDimcChecksMasterList();
    }

    public JsonRequirement saveJson(String requirementId, Object newJson) {
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId)
                .orElse(new JsonRequirement(requirementId, null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public JsonRequirement getOrCreateJsonRequirement(String requirementId)
            throws JsonProcessingException, ConfigurationException {
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);
        if (existingJsonRequirement.isPresent()) {
            LOG.debug("Returning existing JSON requirement for ID: {}", requirementId);
            return existingJsonRequirement.get();
        }

        LOG.info("Generating new JSON requirement for ID: {}", requirementId);
        JsonGenerationConfig config = buildJsonGenerationConfig(requirementId);

        ObjectNode rootNode = objectMapper.createObjectNode();
        rootNode.set("application", createApplicationNode(config));

        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {
        });
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);
        return jsonRequirementRepository.save(jsonRequirement);
    }

    private JsonGenerationConfig buildJsonGenerationConfig(String requirementId)
            throws ConfigurationException, JsonProcessingException {
        String jiraStoryId = requirementId.split("_")[0];

        JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
        Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                .orElseThrow(
                        () -> new ConfigurationException("Sor not found for name: " + jira.getApplicationSorName()));
        ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName()).stream()
                .findFirst()
                .orElseThrow(() -> new ConfigurationException(
                        "ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
        SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                .orElseThrow(
                        () -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

        JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
        JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

        if (sourceTypeEnum == JsonGenerationConfig.SourceType.UNKNOWN)
            throw new ConfigurationException("Unknown Source Type: " + jira.getSorType());
        if (targetTypeEnum == JsonGenerationConfig.TargetType.UNKNOWN)
            throw new ConfigurationException("Unknown Target Type: " + jira.getTargetType());

        String inputTableName = jira.getTableName() != null ? jira.getTableName().toLowerCase(Locale.ROOT) : "";
        String inputSourceSchema = jira.getSourceSchema() != null ? jira.getSourceSchema().toLowerCase(Locale.ROOT)
                : "";

        String resolvedJobNameString;
        if ("bmg".equalsIgnoreCase(sor.getSorName()) && !isFileSource(sourceTypeEnum)) {
            resolvedJobNameString = (!inputSourceSchema.isEmpty() ? inputSourceSchema + "_" : "") + inputTableName;
        } else {
            resolvedJobNameString = inputTableName;
        }

        String jobSpecificAppValue;
        if (auth.getApplicationQualifier() != null && !auth.getApplicationQualifier().isEmpty()) {
            jobSpecificAppValue = auth.getApplicationQualifier();
        } else if (auth.getApplicationName() != null) {
            jobSpecificAppValue = auth.getApplicationName().toLowerCase(Locale.ROOT);
        } else {
            jobSpecificAppValue = jira.getApplicationName() != null ? jira.getApplicationName().toLowerCase(Locale.ROOT)
                    : "default_app_value";
            LOG.warn(
                    "ApplicationQualifier and ApplicationName from ApplicationAuthorizer are missing for JIRA app: {}. Using JIRA app name or default for job 'app' key.",
                    jira.getApplicationName());
        }

        boolean includeIngestions = (sourceTypeEnum != JsonGenerationConfig.SourceType.HIVE);
        boolean includeFlows = (targetTypeEnum != JsonGenerationConfig.TargetType.GCP); // Default logic
        boolean includeDataChecks = true;
        boolean includeFileExtracts = (targetTypeEnum == JsonGenerationConfig.TargetType.GCP);

        Set<String> jobLevelFieldsToInclude = determineJobLevelFieldsToInclude(targetTypeEnum);
        Set<String> ingestionFieldsToInclude = determineIngestionFieldsToInclude(sourceTypeEnum, includeIngestions,
                targetTypeEnum);

        String resolvedIngestionSourceTypeValue = includeIngestions ? resolveIngestionSourceTypeValue(sourceTypeEnum)
                : null;
        String resolvedIngestionConnectionHeaderValue = includeIngestions
                ? resolveIngestionConnectionHeaderValue(sourceTypeEnum)
                : null;

        String resolvedIngestionDataFrameName = includeIngestions ? resolvedJobNameString + "_df" : null;

        String resolvedIngestionSourceSQLQuery = null;
        if (includeIngestions && isDatabaseSource(sourceTypeEnum) && sqlScript != null) {
            resolvedIngestionSourceSQLQuery = sqlScript.getSrcTableQuery();
        }
        String resolvedIngestionSourceName = (includeIngestions
                && sourceTypeEnum == JsonGenerationConfig.SourceType.ORACLE) ? "${BDDMCBD_SRC_SCHEMA}." + inputTableName
                        : null;

        String resolvedIngestionDataFilePath = (includeIngestions && isFileSource(sourceTypeEnum))
                ? "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}"
                : null;
        String resolvedIngestionSchemaFilePath = (includeIngestions && isFileSource(sourceTypeEnum))
                ? "${DCI_CONFIG_DIR_PATH}/config/" + sor.getSorName().toLowerCase() + "/${FEED_NAME}/metadata/"
                        + inputTableName + ".schema"
                : null;
        String resolvedIngestionStagingFilePath = (includeIngestions && isFileSource(sourceTypeEnum))
                ? determineStagingFilePath()
                : null;
        String resolvedIngestionDelimiter = includeIngestions ? resolveIngestionDelimiter(sourceTypeEnum) : null;
        String resolvedIngestionIsHeaderEnabledValue = (includeIngestions && isFileSource(sourceTypeEnum)) ? "Y" : null;
        String ingestionIsCreateTargetTableValue = includeIngestions ? "Y" : null;
        String ingestionIsCacheEnabledValue = includeIngestions ? "Y" : null;
        String ingestionIsOverWritePartitionValue = includeIngestions ? "Y" : null;
        String ingestionIsRowCountDisabledValue = (includeIngestions
                && sourceTypeEnum == JsonGenerationConfig.SourceType.TERADATA) ? "N" : null;

        ObjectNode prebuiltIngestionAdditionalReadConfigsNode = includeIngestions
                ? buildPrebuiltIngestionAdditionalReadConfigs(sourceTypeEnum, this.objectMapper)
                : null;

        boolean includeDimcChecksForThisSource = includeIngestions &&
                (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE)
                        .contains(sourceTypeEnum) ||
                        isFileSource(sourceTypeEnum));
        List<Integer> chosenDimcCheckIndices = includeDimcChecksForThisSource
                ? determineSourceSpecificDimcIndices(sourceTypeEnum)
                : Collections.emptyList();
        ObjectNode prebuiltIngestionDimcChecksNode = (includeDimcChecksForThisSource
                && !chosenDimcCheckIndices.isEmpty())
                        ? buildPrebuiltDimcChecksNode(chosenDimcCheckIndices, this.objectMapper)
                        : null;

        List<JsonGenerationConfig.FlowDetailConfig> flowDetailsList = includeFlows
                ? prepareFlowDetails(sourceTypeEnum, targetTypeEnum, jira, sor, auth, sqlScript,
                        resolvedIngestionDataFrameName, resolvedJobNameString)
                : Collections.emptyList();

        List<JsonGenerationConfig.CheckDetailConfig> checkDetailsList = includeDataChecks
                ? prepareCheckDetails(sourceTypeEnum, targetTypeEnum, jira, auth, flowDetailsList,
                        resolvedIngestionDataFrameName, resolvedJobNameString)
                : Collections.emptyList();

        String resolvedDataChecksAcDcDatabase = includeDataChecks ? determineDataChecksAcDcDatabase() : null;
        ObjectNode prebuiltDataChecksPreChecksNode = includeDataChecks
                ? buildPrebuiltDataChecksPreChecksNode(sourceTypeEnum, this.objectMapper)
                : null;

        return new JsonGenerationConfig(
                jira, sor, auth, sqlScript, sourceTypeEnum, targetTypeEnum, resolvedJobNameString, jobSpecificAppValue,
                includeIngestions, includeFlows, includeDataChecks, includeFileExtracts,
                jobLevelFieldsToInclude, ingestionFieldsToInclude,
                resolvedIngestionSourceTypeValue, resolvedIngestionConnectionHeaderValue,
                resolvedIngestionDataFrameName, resolvedIngestionSourceSQLQuery,
                resolvedIngestionSourceName, resolvedIngestionDataFilePath,
                resolvedIngestionSchemaFilePath, resolvedIngestionStagingFilePath,
                resolvedIngestionDelimiter, resolvedIngestionIsHeaderEnabledValue,
                ingestionIsCreateTargetTableValue, ingestionIsCacheEnabledValue, ingestionIsOverWritePartitionValue,
                ingestionIsRowCountDisabledValue,
                prebuiltIngestionAdditionalReadConfigsNode,
                prebuiltIngestionDimcChecksNode,
                flowDetailsList,
                resolvedDataChecksAcDcDatabase, checkDetailsList, prebuiltDataChecksPreChecksNode);
    }

    public ObjectNode configureFlows(String requirementId, String currentJsonString, boolean addFlows)
            throws JsonProcessingException, ConfigurationException {

        ObjectNode userJsonRootNode = (ObjectNode) objectMapper.readTree(currentJsonString);
        JsonNode jobNodeJson = userJsonRootNode.path("application").path("jobs").path("job");

        if (!jobNodeJson.isObject()) {
            LOG.error("Job node not found or not an object in the provided JSON for requirementId: {}", requirementId);
            throw new ConfigurationException(
                    "Invalid JSON structure: 'application.jobs.job' path not found or not an object.");
        }
        ObjectNode jobNode = (ObjectNode) jobNodeJson;

        if (addFlows) {
            LOG.info("Adding flows section for requirementId: {}", requirementId);
            // Fetch authoritative data
            String jiraStoryId = requirementId.split("_")[0];
            JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                    .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
            Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                    .orElseThrow(() -> new ConfigurationException(
                            "Sor not found for name: " + jira.getApplicationSorName()));
            ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName())
                    .stream().findFirst()
                    .orElseThrow(() -> new ConfigurationException(
                            "ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
            SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                    .orElseThrow(
                            () -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

            // Determine flow generation inputs (Backend Derived)
            JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
            JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

            String inputTableName = jira.getTableName() != null ? jira.getTableName().toLowerCase(Locale.ROOT) : "";
            String inputSourceSchema = jira.getSourceSchema() != null ? jira.getSourceSchema().toLowerCase(Locale.ROOT)
                    : "";

            String resolvedJobNameString;
            if ("bmg".equalsIgnoreCase(sor.getSorName()) && !isFileSource(sourceTypeEnum)) {
                resolvedJobNameString = (!inputSourceSchema.isEmpty() ? inputSourceSchema + "_" : "") + inputTableName;
            } else {
                resolvedJobNameString = inputTableName;
            }

            // For focused config, includeIngestions is false for flow-only generation
            // context
            String resolvedIngestionDataFrameNameForFlowContext = (sourceTypeEnum != JsonGenerationConfig.SourceType.HIVE)
                    ? resolvedJobNameString + "_df"
                    : null;

            List<JsonGenerationConfig.FlowDetailConfig> flowDetailsList = prepareFlowDetails(
                    sourceTypeEnum, targetTypeEnum, jira, sor, auth, sqlScript,
                    resolvedIngestionDataFrameNameForFlowContext, // Use context-specific derivation
                    resolvedJobNameString);

            // Create focused JsonGenerationConfig for flows
            // Determine jobSpecificAppValue for constructor requirement
            String jobSpecificAppValue;
            if (auth.getApplicationQualifier() != null && !auth.getApplicationQualifier().isEmpty()) {
                jobSpecificAppValue = auth.getApplicationQualifier();
            } else if (auth.getApplicationName() != null) {
                jobSpecificAppValue = auth.getApplicationName().toLowerCase(Locale.ROOT);
            } else {
                jobSpecificAppValue = jira.getApplicationName() != null
                        ? jira.getApplicationName().toLowerCase(Locale.ROOT)
                        : "default_app_value";
            }

            JsonGenerationConfig focusedConfig = new JsonGenerationConfig(
                    jira, sor, auth, sqlScript, sourceTypeEnum, targetTypeEnum, resolvedJobNameString,
                    jobSpecificAppValue,
                    false, // includeIngestions
                    true, // includeFlows
                    false, // includeDataChecks
                    false, // includeFileExtracts
                    Collections.emptySet(), // jobLevelFieldsToInclude
                    Collections.emptySet(), // ingestionFieldsToInclude
                    null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, // ingestion
                                                                                                                    // related
                                                                                                                    // fields
                    flowDetailsList, // The generated flow details
                    null, Collections.emptyList(), null // data check related fields
            );

            ObjectNode generatedFlowsSectionNode = buildGenericFlowsNode(focusedConfig);

            if (generatedFlowsSectionNode != null && generatedFlowsSectionNode.size() > 0) {
                jobNode.set("flows", generatedFlowsSectionNode);
            } else {
                // If for some reason flows node is empty (e.g., prepareFlowDetails returned
                // empty list),
                // ensure to remove if it was a NOP or add an empty object based on DCI
                // expectations.
                // For now, if it's null/empty, we can just not add it or explicitly remove any
                // prior.
                jobNode.remove("flows"); // Ensure clean state if new flows are empty
            }
        } else {
            LOG.info("Removing flows section for requirementId: {}", requirementId);
            jobNode.remove("flows");
        }
        return userJsonRootNode;
    }

    private boolean isDatabaseSource(JsonGenerationConfig.SourceType sourceType) {
        return EnumSet
                .of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.MSSQL,
                        JsonGenerationConfig.SourceType.ORACLE, JsonGenerationConfig.SourceType.HIVE)
                .contains(sourceType);
    }

    private boolean isFileSource(JsonGenerationConfig.SourceType sourceType) {
        return EnumSet.of(JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH, JsonGenerationConfig.SourceType.FILE_PIPE,
                JsonGenerationConfig.SourceType.FILE_COMMA).contains(sourceType);
    }

    private Set<String> determineJobLevelFieldsToInclude(JsonGenerationConfig.TargetType targetType) {
        return new HashSet<>(Arrays.asList("app", "appRemedyId", "sorRemedyId", "queryGroup", "zone", "sor"));
    }

    private Set<String> determineIngestionFieldsToInclude(JsonGenerationConfig.SourceType sourceType,
            boolean includeIngestionsFlag, JsonGenerationConfig.TargetType targetType) {
        if (!includeIngestionsFlag)
            return Collections.emptySet();
        Set<String> fields = new HashSet<>(Arrays.asList(
                "ingestionId", "ingestionDesc", "isIngestionEnabled", "dataFrameName",
                "isCreateTargetTable", "isCacheEnabled", "isOverWritePartition",
                "abortIfDuplicate", "abortIfEmpty", "isRunEMHReport", "sourceType"));

        if (targetType == JsonGenerationConfig.TargetType.HIVE
                && sourceType == JsonGenerationConfig.SourceType.TERADATA) {
            fields.add("targetType");
        }

        if (isDatabaseSource(sourceType)) {
            fields.add("ConnectionHeader");
            fields.add("sourceSQLQuery");
            if (sourceType == JsonGenerationConfig.SourceType.ORACLE)
                fields.add("sourceName");
            if (sourceType == JsonGenerationConfig.SourceType.TERADATA)
                fields.add("isRowCountDisabled");
        } else if (isFileSource(sourceType)) {
            fields.addAll(Arrays.asList("isHeaderEnabled", "dataFilePath", "schemaFilePath", "stagingFilePath",
                    "additionalReadConfigs"));
            if (sourceType != JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH)
                fields.add("delimiter");
        }
        if (EnumSet.of(JsonGenerationConfig.SourceType.TERADATA, JsonGenerationConfig.SourceType.HIVE)
                .contains(sourceType) || isFileSource(sourceType)) {
            fields.add("dimcChecks");
        }
        return fields;
    }

    private String resolveIngestionSourceTypeValue(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA:
                return "teradb";
            case FILE_FIXEDWIDTH:
                return "fixedwidthFile";
            case FILE_PIPE:
            case FILE_COMMA:
                return "file";
            case MSSQL:
                return "jdbc";
            case ORACLE:
                return "oracle";
            case HIVE:
                return "hive";
            default:
                return "unknown";
        }
    }

    private String resolveIngestionConnectionHeaderValue(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA:
                return "teradbConn";
            case MSSQL:
                return "jdbcConn";
            case ORACLE:
                return "oracleConn";
            case HIVE:
                return "hiveConn";
            default:
                return null;
        }
    }

    private String determineStagingFilePath() {
        return "${FILE_PATH}";
    }

    private String resolveIngestionDelimiter(JsonGenerationConfig.SourceType sourceType) {
        if (sourceType == JsonGenerationConfig.SourceType.FILE_PIPE)
            return "|";
        if (sourceType == JsonGenerationConfig.SourceType.FILE_COMMA)
            return ",";
        return null;
    }

    private ObjectNode buildPrebuiltIngestionAdditionalReadConfigs(JsonGenerationConfig.SourceType sourceType,
            ObjectMapper mapper) {
        if (isFileSource(sourceType)) {
            ObjectNode arNode = mapper.createObjectNode();
            arNode.put("quote", "\"");
            arNode.put("escape", "\"");
            return arNode;
        }
        return null;
    }

    private List<Integer> determineSourceSpecificDimcIndices(JsonGenerationConfig.SourceType sourceType) {
        switch (sourceType) {
            case TERADATA:
            case HIVE:
                return Arrays.asList(4, 5, 3, 1);
            case FILE_FIXEDWIDTH:
            case FILE_PIPE:
            case FILE_COMMA:
                return Arrays.asList(6, 0, 1, 2, 3, 7, 8);
            default:
                return Collections.emptyList();
        }
    }

    private ObjectNode buildPrebuiltDimcChecksNode(List<Integer> chosenIndices, ObjectMapper mapper)
            throws JsonProcessingException {
        List<Map<String, Object>> selectedChecks = getChosenDimcChecksListInternal(chosenIndices);
        if (selectedChecks.isEmpty())
            return null;

        ObjectNode dimcChecksWrapper = mapper.createObjectNode();
        dimcChecksWrapper.put("isDimcChecksEnabled", "Y");
        ArrayNode dimcCheckArray = mapper.valueToTree(selectedChecks);
        dimcChecksWrapper.set("dimcCheck", dimcCheckArray);
        return dimcChecksWrapper;
    }

    private static List<Map<String, Object>> getChosenDimcChecksListInternal(List<Integer> chosenIndices) {
        if (chosenIndices == null || chosenIndices.isEmpty() || DIMC_CHECKS_MASTER_LIST.isEmpty()) {
            return Collections.emptyList();
        }
        return chosenIndices.stream()
                .filter(index -> index != null && index >= 0 && index < DIMC_CHECKS_MASTER_LIST.size())
                .map(index -> new HashMap<>(DIMC_CHECKS_MASTER_LIST.get(index)))
                .collect(Collectors.toList());
    }

    private List<JsonGenerationConfig.FlowDetailConfig> prepareFlowDetails(
            JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
            JiraStoryIntake jira, Sor sor, ApplicationAuthorizer auth, SQLScript sqlScript,
            String ingestionDataFrameNameIfAny, String currentResolvedJobName) {

        List<JsonGenerationConfig.FlowDetailConfig> flowConfigs = new ArrayList<>();
        int numberOfFlows = (sourceType == JsonGenerationConfig.SourceType.MSSQL
                || sourceType == JsonGenerationConfig.SourceType.ORACLE) ? 2 : 1;

        String appNameForFlowLogic = jira.getApplicationName() != null
                ? jira.getApplicationName().toLowerCase(Locale.ROOT)
                : "defaultapp";
        String commonHivePartitionColumn = appNameForFlowLogic + "_business_effective_date";

        String flowIsCreateTargetTable = "N";
        String flowIsCacheEnabled = "Y";
        String flowSendEmail = "N";
        String flowSaveOutput = "Y";
        String flowIsOverWritePartition = "Y";
        String flowAbortIfEmpty = "N";
        String baseNameForFlow = currentResolvedJobName;

        for (int i = 0; i < numberOfFlows; i++) {
            String flowId = String.valueOf(i + 1);
            boolean isFirstFlow = (i == 0);
            String flowZone = JsonGenerationConfig.SANITIZED_ZONE;
            if (numberOfFlows == 2 && isFirstFlow)
                flowZone = JsonGenerationConfig.RAW_ZONE;

            String appNameForDescription = jira.getApplicationName() != null ? jira.getApplicationName()
                    : "UNKNOWN_APP";
            String flowDesc = sor.getSorName().toUpperCase(Locale.ROOT) +
                    JsonGenerationConfig.TABLE_TEXT + baseNameForFlow +
                    JsonGenerationConfig.INGESTION_TO_TEXT + appNameForDescription + " - " + flowZone;

            String query = (sqlScript != null) ? sqlScript.getSrcDataFrameQuery() : "SELECT * FROM source_placeholder"; // Handle
                                                                                                                        // null
                                                                                                                        // sqlScript
            String registerName = baseNameForFlow + "_" + flowZone;
            String flowDatabaseName = "";
            String storageName = "";
            String targetTablePath = null;

            if (targetType == JsonGenerationConfig.TargetType.HIVE) {
                if (isFileSource(sourceType)) {
                    storageName = "${TARGET_TABLE_NAME}";
                    targetTablePath = "${FILE_PATH}/${TARGET_TABLE_NAME}";
                    flowDatabaseName = "${HIVE_TARGET_DBNAME}";
                } else {
                    Map<String, String> storageNameMap = Map.of("CNAPP", "consumer", "UTCAP", "utcap");
                    String appSubDir = storageNameMap.getOrDefault(jira.getApplicationName(),
                            jira.getApplicationName() != null ? jira.getApplicationName().toLowerCase() : "defaultapp");
                    storageName = "/datalake${EDL_ENV}/" + appSubDir + "/" +
                            sor.getDomainName().toLowerCase(Locale.ROOT) + "/" +
                            sor.getSorName().toLowerCase(Locale.ROOT) + "/" +
                            flowZone + "/" + baseNameForFlow;
                    flowDatabaseName = "${ETL_ENV}" + appSubDir + "_" + sor.getSorName().toLowerCase() + "_" + flowZone;
                }
            }
            flowConfigs.add(new JsonGenerationConfig.FlowDetailConfig(
                    flowId, flowDesc, query, registerName, flowDatabaseName, storageName, targetTablePath,
                    commonHivePartitionColumn, flowIsCreateTargetTable, flowIsOverWritePartition, flowIsCacheEnabled,
                    flowSendEmail, flowAbortIfEmpty, flowSaveOutput));
        }
        return flowConfigs;
    }

    private List<JsonGenerationConfig.CheckDetailConfig> prepareCheckDetails(
            JsonGenerationConfig.SourceType sourceType, JsonGenerationConfig.TargetType targetType,
            JiraStoryIntake jira, ApplicationAuthorizer auth,
            List<JsonGenerationConfig.FlowDetailConfig> flowDetails, String ingestionDataFrameName,
            String currentResolvedJobName) {

        List<JsonGenerationConfig.CheckDetailConfig> checkConfigs = new ArrayList<>();
        if (targetType == JsonGenerationConfig.TargetType.GCP || flowDetails == null || flowDetails.isEmpty()) {
            return checkConfigs;
        }
        String tableNameForCheck = currentResolvedJobName;
        String appNameForPartitionPlaceholder = jira.getApplicationName() != null
                ? jira.getApplicationName().toUpperCase()
                : "DEFAULTAPP";

        for (JsonGenerationConfig.FlowDetailConfig currentFlow : flowDetails) {
            String checkId = currentFlow.getFlowId();
            String checkName = "HHRCNTCHK";
            String checkDesc = "Data Count Check";
            String checkSourceQueryDataFrame;
            String sourceHiveDbForCheck = jira.getSourceSchema() != null ? jira.getSourceSchema()
                    : "${SOURCE_HIVE_DATABASE}";

            if (sourceType == JsonGenerationConfig.SourceType.HIVE) {
                String hiveSourceTableName = "bmg".equalsIgnoreCase(jira.getApplicationSorName())
                        ? currentResolvedJobName
                        : (jira.getSourceTableName() != null ? jira.getSourceTableName() : jira.getTableName());
                checkSourceQueryDataFrame = sourceHiveDbForCheck + "." + hiveSourceTableName;
            } else if (ingestionDataFrameName != null) {
                checkSourceQueryDataFrame = ingestionDataFrameName;
            } else {
                LOG.warn(
                        "Cannot determine source DataFrame for data check for source: {} and flow: {}. Using placeholder.",
                        sourceType, currentFlow.getFlowId());
                checkSourceQueryDataFrame = "placeholder_source_for_check_" + currentFlow.getFlowId(); // Fallback
            }

            String checkSourceQuery = "select count(*) from " + checkSourceQueryDataFrame;
            String partitionValuePlaceholder = "${" + appNameForPartitionPlaceholder + "_BUSINESS_EFFECTIVE_DATE}";
            String checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowDatabaseName() + "."
                    + tableNameForCheck + " where " + currentFlow.getResolvedFlowHivePartitionColumn() + "='"
                    + partitionValuePlaceholder + "'";
            checkConfigs.add(new JsonGenerationConfig.CheckDetailConfig(
                    checkId, checkName, checkDesc, checkSourceQuery, checkTargetQuery, "Y", "Y", "N", "N", "Y"));
        }
        return checkConfigs;
    }

    private String determineDataChecksAcDcDatabase() {
        return "${AUDIT_DATABASE}";
    }

    private String toUpperSnakeCase(String camelOrPascalCase) {
        if (camelOrPascalCase == null || camelOrPascalCase.isEmpty())
            return "";
        Pattern pattern = Pattern.compile("(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])");
        return pattern.matcher(camelOrPascalCase).replaceAll("_").toUpperCase(Locale.ROOT);
    }

    private ObjectNode buildPrebuiltDataChecksPreChecksNode(JsonGenerationConfig.SourceType sourceType,
            ObjectMapper mapper) {
        ObjectNode preChecksNode = mapper.createObjectNode();
        preChecksNode.put("isPreCheckEnabled", "${PRECHECK_ENABLED}");
        ArrayNode preCheckArray = preChecksNode.putArray("preCheck");
        List<String> preCheckNames = Arrays.asList(
                "ServiceIDCheck", "TargetDataBaseAvailabilityCheck", "TargetTableAvailabilityCheck",
                "TargetTablePathWritableCheck", "FlowSeqDependenciesCheck", "SourceDataBaseAvailabilityCheck",
                "SourceTableAvailabilityCheck", "SourceTableEmptyCheck", "SourcePathAvailabilityCheck",
                "SourcePathEmptyCheck");
        for (String checkName : preCheckNames) {
            String upperSnakeCaseName = toUpperSnakeCase(checkName);
            String enabledPlaceholder = "${" + upperSnakeCaseName + "_ENABLED}";
            String failureFlagPlaceholder = "${" + upperSnakeCaseName + "_FAILURE_FLAG}";
            addPreCheckItem(preCheckArray, checkName, enabledPlaceholder, failureFlagPlaceholder, mapper);
        }
        return preChecksNode;
    }

    private void addPreCheckItem(ArrayNode array, String name, String enabledPlaceholder, String failOnErrorPlaceholder,
            ObjectMapper mapper) {
        ObjectNode item = mapper.createObjectNode();
        item.put("preCheckName", name);
        item.put("isPreCheckEnabled", enabledPlaceholder);
        item.put("isFailJobOnPreCheckError", failOnErrorPlaceholder);
        array.add(item);
    }

    private ObjectNode createApplicationNode(JsonGenerationConfig config) {
        ObjectNode applicationNode = objectMapper.createObjectNode();
        applicationNode.put("isParallelProcessing", "N");
        applicationNode.set("jobs", createJobsNode(config));
        return applicationNode;
    }

    private ObjectNode createJobsNode(JsonGenerationConfig config) {
        ObjectNode jobsNode = objectMapper.createObjectNode();
        jobsNode.set("job", createJobNode(config));
        return jobsNode;
    }

    private ObjectNode createJobNode(JsonGenerationConfig config) {
        ObjectNode jobNode = objectMapper.createObjectNode();
        Sor sor = config.getSor();
        ApplicationAuthorizer auth = config.getApplicationAuthorizer();
        jobNode.put("JobName", config.getResolvedJobName());
        Set<String> jobFields = config.getJobLevelFieldsToInclude();
        if (jobFields.contains("app"))
            jobNode.put("app", config.getResolvedJobAppKeySetting());
        if (jobFields.contains("appRemedyId") && auth != null)
            jobNode.put("appRemedyId", auth.getAppRemedyId()); // Null check for auth
        if (jobFields.contains("sorRemedyId") && sor != null)
            jobNode.put("sorRemedyId", sor.getRemedyId()); // Null check for sor
        if (jobFields.contains("queryGroup") && sor != null)
            jobNode.put("queryGroup", "dimc_check_" + sor.getSorName().toLowerCase(Locale.ROOT));
        String zone = JsonGenerationConfig.SANITIZED_ZONE;
        if (jobFields.contains("zone"))
            jobNode.put("zone", zone);
        if (jobFields.contains("sor") && sor != null)
            jobNode.put("sor", sor.getSorName().toLowerCase(Locale.ROOT));
        jobNode.put("jobId", "1");
        jobNode.put("jobDescription", determineJobDescription(config));
        jobNode.put("isJobEnabled", "Y");
        jobNode.set("Email", createEmailNode(config));
        if (config.isIncludeIngestions()) {
            ObjectNode ingestionsNode = buildGenericIngestionNode(config);
            if (ingestionsNode != null && ingestionsNode.size() > 0)
                jobNode.set("ingestions", ingestionsNode);
        }
        if (config.isIncludeFlows()) {
            ObjectNode flowsNode = buildGenericFlowsNode(config);
            if (flowsNode != null && flowsNode.size() > 0)
                jobNode.set("flows", flowsNode);
        }
        if (config.isIncludeDataChecks()) {
            ObjectNode dataChecksNode = buildGenericDataChecksNode(config);
            if (dataChecksNode != null && dataChecksNode.size() > 0)
                jobNode.set("dataChecks", dataChecksNode);
        }
        if (config.isIncludeFileExtracts()) {
            ObjectNode fileExtractsNode = buildGenericFileExtractsNode(config);
            if (fileExtractsNode != null && fileExtractsNode.size() > 0)
                jobNode.set("fileExtracts", fileExtractsNode);
        }
        return jobNode;
    }

    private String determineJobDescription(JsonGenerationConfig config) {
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        String jobNameForDesc = config.getResolvedJobName();
        String appNameForDesc = (jira != null && jira.getApplicationName() != null) ? jira.getApplicationName()
                : "UNKNOWN_APP";
        String sorNameForDesc = (sor != null && sor.getSorName() != null) ? sor.getSorName().toUpperCase(Locale.ROOT)
                : "UNKNOWN_SOR";

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            return "CDMP Ingestion for Hive table - " + jobNameForDesc;
        } else {
            return sorNameForDesc + JsonGenerationConfig.TABLE_TEXT + jobNameForDesc +
                    JsonGenerationConfig.INGESTION_TO_TEXT + appNameForDesc;
        }
    }

    private ObjectNode createEmailNode(JsonGenerationConfig config) {
        ObjectNode emailNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        String fromAddress = "${EMAIL_TO_LIST}";
        String toAddress = "${EMAIL_CC_LIST}";
        String subject;
        String subjectNamePart = config.getResolvedJobName();
        String appNameForSubject = (jira != null && jira.getApplicationName() != null) ? jira.getApplicationName()
                : "UNKNOWN_APP";
        String sorNameForSubject = (sor != null && sor.getSorName() != null) ? sor.getSorName().toUpperCase(Locale.ROOT)
                : "UNKNOWN_SOR";

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            subject = "CDMP Ingestion - " + subjectNamePart;
        } else if (isFileSource(config.getSourceType())) {
            subject = "utcap ingestion for feed " + subjectNamePart;
        } else {
            subject = "Data Extract from " + sorNameForSubject + " to " + appNameForSubject + " - " + subjectNamePart;
        }
        emailNode.put("fromAddress", fromAddress);
        emailNode.put("toAddress", toAddress);
        emailNode.put("subject", subject);
        emailNode.put("isPriorityAlert", "Y");
        return emailNode;
    }

    private ObjectNode buildGenericIngestionNode(JsonGenerationConfig config) {
        Set<String> fields = config.getIngestionFieldsToInclude();
        if (fields.isEmpty())
            return null;
        ObjectNode ingestionItemNode = objectMapper.createObjectNode();
        // ... (rest of the method is assumed to be complete and correct as per previous
        // versions)
        // This is a placeholder for brevity, the actual content would be the full
        // method
        if (fields.contains("ingestionId"))
            ingestionItemNode.put("ingestionId", "1");
        if (fields.contains("ingestionDesc"))
            ingestionItemNode.put("ingestionDesc", determineIngestionDescription(config));
        if (fields.contains("isIngestionEnabled"))
            ingestionItemNode.put("isIngestionEnabled", "Y");
        if (fields.contains("sourceType"))
            ingestionItemNode.put("sourceType", config.getResolvedIngestionSourceTypeValue());
        if (fields.contains("ConnectionHeader") && config.getResolvedIngestionConnectionHeaderValue() != null)
            ingestionItemNode.put("ConnectionHeader", config.getResolvedIngestionConnectionHeaderValue());
        if (fields.contains("delimiter") && config.getResolvedIngestionDelimiter() != null)
            ingestionItemNode.put("delimiter", config.getResolvedIngestionDelimiter());
        if (fields.contains("dataFrameName"))
            ingestionItemNode.put("dataFrameName", config.getResolvedIngestionDataFrameName());
        if (fields.contains("isCreateTargetTable"))
            ingestionItemNode.put("isCreateTargetTable", config.getIngestionIsCreateTargetTableValue());
        if (fields.contains("isCacheEnabled"))
            ingestionItemNode.put("isCacheEnabled", config.getIngestionIsCacheEnabledValue());
        if (fields.contains("isOverWritePartition"))
            ingestionItemNode.put("isOverWritePartition", config.getIngestionIsOverWritePartitionValue());
        if (fields.contains("isRowCountDisabled") && config.getIngestionIsRowCountDisabledValue() != null)
            ingestionItemNode.put("isRowCountDisabled", config.getIngestionIsRowCountDisabledValue());
        if (fields.contains("abortIfDuplicate"))
            ingestionItemNode.put("abortIfDuplicate", "N");
        if (fields.contains("abortIfEmpty"))
            ingestionItemNode.put("abortIfEmpty", "N");
        if (fields.contains("isRunEMHReport"))
            ingestionItemNode.put("isRunEMHReport", "N");
        if (fields.contains("sourceSQLQuery") && config.getResolvedIngestionSourceSQLQuery() != null)
            ingestionItemNode.put("sourceSQLQuery", config.getResolvedIngestionSourceSQLQuery());
        if (fields.contains("sourceName") && config.getResolvedIngestionSourceName() != null)
            ingestionItemNode.put("sourceName", config.getResolvedIngestionSourceName());
        if (fields.contains("dataFilePath") && config.getResolvedIngestionDataFilePath() != null)
            ingestionItemNode.put("dataFilePath", config.getResolvedIngestionDataFilePath());
        if (fields.contains("schemaFilePath") && config.getResolvedIngestionSchemaFilePath() != null)
            ingestionItemNode.put("schemaFilePath", config.getResolvedIngestionSchemaFilePath());
        if (fields.contains("stagingFilePath") && config.getResolvedIngestionStagingFilePath() != null)
            ingestionItemNode.put("stagingFilePath", config.getResolvedIngestionStagingFilePath());
        if (fields.contains("isHeaderEnabled") && config.getResolvedIngestionIsHeaderEnabledValue() != null)
            ingestionItemNode.put("isHeaderEnabled", config.getResolvedIngestionIsHeaderEnabledValue());
        if (fields.contains("targetType") && config.getTargetType() == JsonGenerationConfig.TargetType.HIVE
                && config.getSourceType() == JsonGenerationConfig.SourceType.TERADATA) {
            ingestionItemNode.put("targetType", "parquet");
        }
        if (fields.contains("additionalReadConfigs") && config.getPrebuiltIngestionAdditionalReadConfigsNode() != null)
            ingestionItemNode.set("additionalReadConfigs", config.getPrebuiltIngestionAdditionalReadConfigsNode());
        if (fields.contains("dimcChecks") && config.getPrebuiltIngestionDimcChecksNode() != null)
            ingestionItemNode.set("dimcChecks", config.getPrebuiltIngestionDimcChecksNode());

        ObjectNode finalIngestionsNode = objectMapper.createObjectNode();
        if (ingestionItemNode.size() > 0) {
            addToArray(finalIngestionsNode, "ingestion", ingestionItemNode);
            return finalIngestionsNode;
        }
        return null;
    }

    private String determineIngestionDescription(JsonGenerationConfig config) {
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        String jobNameForDesc = config.getResolvedJobName();
        String appNameForDesc = (jira != null && jira.getApplicationName() != null) ? jira.getApplicationName()
                : "UNKNOWN_APP";
        String sorNameForDesc = (sor != null && sor.getSorName() != null) ? sor.getSorName().toUpperCase(Locale.ROOT)
                : "UNKNOWN_SOR";

        return sorNameForDesc + JsonGenerationConfig.TABLE_TEXT + jobNameForDesc +
                JsonGenerationConfig.INGESTION_TO_TEXT + appNameForDesc;
    }

    private ObjectNode buildGenericFlowsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFlows() || config.getFlowDetailsList() == null || config.getFlowDetailsList().isEmpty()) // Added
                                                                                                                      // null
                                                                                                                      // check
                                                                                                                      // for
                                                                                                                      // safety
            return null;

        ObjectNode flowsNode = objectMapper.createObjectNode();
        ArrayNode flowArray = flowsNode.putArray("flow");

        for (JsonGenerationConfig.FlowDetailConfig flowDetail : config.getFlowDetailsList()) {
            ObjectNode flowItemNode = objectMapper.createObjectNode();
            if (flowDetail.getFlowId() != null)
                flowItemNode.put("flowId", flowDetail.getFlowId());
            if (flowDetail.getResolvedFlowDescription() != null)
                flowItemNode.put("flowDesc", flowDetail.getResolvedFlowDescription());
            flowItemNode.put("isFlowEnabled", "Y"); // Defaulting, ensure this aligns with metadata if it varies
            if (flowDetail.getResolvedFlowQuery() != null)
                flowItemNode.put("query", flowDetail.getResolvedFlowQuery());
            if (flowDetail.getResolvedFlowRegisterName() != null)
                flowItemNode.put("registerName", flowDetail.getResolvedFlowRegisterName());

            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) { // Ensure targetType is available in
                                                                                  // focusedConfig
                if (flowDetail.getSaveOutputValue() != null)
                    flowItemNode.put("saveOutput", flowDetail.getSaveOutputValue());
                if (flowDetail.getResolvedFlowDatabaseName() != null)
                    flowItemNode.put("flowDataBaseName", flowDetail.getResolvedFlowDatabaseName());
                if (flowDetail.getResolvedFlowStorageName() != null)
                    flowItemNode.put("storageName", flowDetail.getResolvedFlowStorageName());
                if (flowDetail.getResolvedFlowTargetTablePath() != null)
                    flowItemNode.put("targetTablePath", flowDetail.getResolvedFlowTargetTablePath());
                if (flowDetail.getResolvedFlowHivePartitionColumn() != null)
                    flowItemNode.put("hivePartitionColumn", flowDetail.getResolvedFlowHivePartitionColumn());
                if (flowDetail.getIsCreateFlowTargetTableValue() != null)
                    flowItemNode.put("isCreateFlowTargetTable", flowDetail.getIsCreateFlowTargetTableValue());
                if (flowDetail.getIsOverWritePartitionValue() != null)
                    flowItemNode.put("isOverWritePartition", flowDetail.getIsOverWritePartitionValue());
                if (flowDetail.getIsCacheEnabledValue() != null)
                    flowItemNode.put("isCacheEnabled", flowDetail.getIsCacheEnabledValue());
                if (flowDetail.getSendEmailValue() != null)
                    flowItemNode.put("sendEmail", flowDetail.getSendEmailValue());
                if (flowDetail.getAbortIfEmptyValue() != null)
                    flowItemNode.put("abortIfEmpty", flowDetail.getAbortIfEmptyValue());
            }
            flowArray.add(flowItemNode);
        }
        return flowsNode.size() > 0 ? flowsNode : null; // Return null if array is effectively empty (though it has
                                                        // "flow" key)
                                                        // More precise: return flowArray.size() > 0 ? flowsNode : null;
    }

    private ObjectNode buildGenericDataChecksNode(JsonGenerationConfig config) {
        if (!config.isIncludeDataChecks()) {
            return null;
        }

        ObjectNode dataChecksNode = objectMapper.createObjectNode();

        if (config.getResolvedDataChecksAcDcDatabase() != null) {
            dataChecksNode.put("acDCDatabase", config.getResolvedDataChecksAcDcDatabase());
        }

        if (config.getCheckDetailsList() != null && !config.getCheckDetailsList().isEmpty()) { // Added null check
            ArrayNode checkArray = dataChecksNode.putArray("check");
            for (JsonGenerationConfig.CheckDetailConfig detail : config.getCheckDetailsList()) {
                ObjectNode checkItemNode = objectMapper.createObjectNode();
                if (detail.getCheckId() != null)
                    checkItemNode.put("checkId", detail.getCheckId());
                if (detail.getCheckName() != null)
                    checkItemNode.put("checkName", detail.getCheckName());
                if (detail.getCheckDescription() != null)
                    checkItemNode.put("checkDesc", detail.getCheckDescription());
                if (detail.getIsCheckEnabledValue() != null)
                    checkItemNode.put("isCheckEnabled", detail.getIsCheckEnabledValue());
                if (detail.getIsUserSupplyQueryValue() != null)
                    checkItemNode.put("isUserSupplyQuery", detail.getIsUserSupplyQueryValue());
                if (detail.getReadSourceQueryFromPathValue() != null)
                    checkItemNode.put("readSourceQueryFromPath", detail.getReadSourceQueryFromPathValue());
                if (detail.getResolvedCheckSourceQuery() != null)
                    checkItemNode.put("sourceQuery", detail.getResolvedCheckSourceQuery());
                if (detail.getResolvedCheckTargetQuery() != null)
                    checkItemNode.put("targetQuery", detail.getResolvedCheckTargetQuery());
                if (detail.getReadTargetQueryFromPathValue() != null)
                    checkItemNode.put("readTargetQueryFromPath", detail.getReadTargetQueryFromPathValue());
                if (detail.getIsFailJobOnDataCheckErrorValue() != null)
                    checkItemNode.put("isFailJobOnDataCheckError", detail.getIsFailJobOnDataCheckErrorValue());
                checkArray.add(checkItemNode);
            }
        }

        ObjectNode preChecksNodeFromConfig = config.getPrebuiltDataChecksPreChecksNode();
        if (preChecksNodeFromConfig != null && preChecksNodeFromConfig.has("preCheck") &&
                preChecksNodeFromConfig.get("preCheck").size() > 0) {
            dataChecksNode.set("preChecks", preChecksNodeFromConfig);
        }

        if (dataChecksNode.isEmpty()) { // if dataChecksNode.size() == 0
            return null;
        }
        if (dataChecksNode.size() == 1 && dataChecksNode.has("acDCDatabase") &&
                "${AUDIT_DATABASE}".equals(dataChecksNode.get("acDCDatabase").asText())) {
            return null;
        }
        return dataChecksNode;
    }

    private ObjectNode buildGenericFileExtractsNode(JsonGenerationConfig config) {
        if (!config.isIncludeFileExtracts())
            return null;
        // ... (rest of the method is assumed to be complete and correct)
        // This is a placeholder for brevity
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = fileExtractsNode.putArray("fileExtract");
        ObjectNode fileExtractItemNode = objectMapper.createObjectNode();
        fileExtractItemNode.put("fileExtractId", "1");
        fileExtractItemNode.put("isFileExtractEnabled", "${IS_FILE_EXTRACT_ENABLED}");
        String sourceDescriptionPart = (config.getSourceType() == JsonGenerationConfig.SourceType.HIVE) ? "hive data"
                : config.getResolvedJobName();
        fileExtractItemNode.put("fileDescription", "Extracting " + sourceDescriptionPart + " to avro file for CDMP");
        fileExtractItemNode.put("subjectArea",
                (config.getSourceType() == JsonGenerationConfig.SourceType.HIVE) ? "cdmp using FileExtract" : "CDMP");
        fileExtractItemNode.put("zone", JsonGenerationConfig.RAW_ZONE);
        fileExtractItemNode.put("system", "cdmp");
        fileExtractItemNode.put("fileType", "avro");
        String flowRegisterNameForExtract = (config.getFlowDetailsList() == null
                || config.getFlowDetailsList().isEmpty()
                || config.getFlowDetailsList().get(0).getResolvedFlowRegisterName() == null)
                        ? config.getResolvedJobName() + "_extract_df"
                        : config.getFlowDetailsList().get(0).getResolvedFlowRegisterName();
        fileExtractItemNode.put("extractQuery", "select * from " + flowRegisterNameForExtract);
        fileExtractItemNode.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        fileExtractItemNode.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        ObjectNode cdmpParamsNode = objectMapper.createObjectNode();
        cdmpParamsNode.put("outputFormat", "avro");
        String cdmpFileNamePrefix = (config.getSourceType() == JsonGenerationConfig.SourceType.HIVE) ? "UTCAPT_" : "";
        cdmpParamsNode.put("cdmpFileName", cdmpFileNamePrefix + config.getResolvedJobName() + "_cdmp_extract");
        cdmpParamsNode.put("cdmpPatternMode", "daily");
        cdmpParamsNode.put("isCustomPartitionRequired", "true");
        cdmpParamsNode.put("desiredPartFileSize", "${PART_FILE_SIZE}");
        fileExtractItemNode.set("cdmpAdditionalParams", cdmpParamsNode);
        fileExtractArray.add(fileExtractItemNode);
        return fileExtractsNode;
    }

    private static synchronized void populateDimcChecksMasterList() {
        if (!DIMC_CHECKS_MASTER_LIST.isEmpty())
            return;
        LOG.info("Populating DIMC Checks Master List (with latest parameterizations)...");
        // ... (DIMC_CHECKS_MASTER_LIST population logic remains the same)
        Map<String, Object> check0 = new HashMap<>();
        check0.put("dimcCheckname", "duplicateFileLoadChecks");
        check0.put("isDimcCheckEnabled", "${DUPLICATE_FILELOAD_CHECK_ENABLED}");
        check0.put("isFailJobOnError", "${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check0));
        Map<String, Object> check1 = new HashMap<>();
        check1.put("dimcCheckname", "dataVolumeConsistencyCheck");
        check1.put("isDimcCheckEnabled", "${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}");
        check1.put("upperThreshold", "${UPPER_THRESHOLD}");
        check1.put("lowerThreshold", "${LOWER_THRESHOLD}");
        check1.put("numPastDays", "${NUM_PAST_DAYS}");
        check1.put("isFailJobOnError", "${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check1));
        Map<String, Object> check2 = new HashMap<>();
        check2.put("dimcCheckname", "missingFileCheck");
        check2.put("isDimcCheckEnabled", "${MISSING_FILE_CHECK_ENABLED}");
        check2.put("isFailJobOnError", "${MISSING_FILE_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check2));
        Map<String, Object> check3 = new HashMap<>();
        check3.put("dimcCheckname", "deliveryCheck");
        check3.put("isDimcCheckEnabled", "${DELIVERY_CHECK_ENABLED}");
        check3.put("SLA", "${DELIVERY_CHECK_SLA}");
        check3.put("isFailJobOnError", "${DELIVERY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check3));
        Map<String, Object> check4 = new HashMap<>();
        check4.put("dimcCheckname", "missingDbObjectCheck");
        check4.put("isDimcCheckEnabled", "${MISSING_DB_OBJECT_CHECK_ENABLED}");
        check4.put("isFailJobOnError", "${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check4));
        Map<String, Object> check5 = new HashMap<>();
        check5.put("dimcCheckname", "emptySourceCheck");
        check5.put("isDimcCheckEnabled", "${EMPTY_SOURCE_CHECK_ENABLED}");
        check5.put("isFailJobOnError", "${EMPTY_SOURCE_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check5));
        Map<String, Object> check6 = new HashMap<>();
        check6.put("dimcCheckname", "dataVolumeCompletenessCheck");
        check6.put("controlFileRowFilterByIndex", "${DVCC_CTL_FILE_ROW_FILTER_SPEC}");
        check6.put("controlFileFieldValueByIndex", "${DVCC_CTL_FILE_FIELD_VALUE_SPEC}");
        check6.put("isDimcCheckEnabled", "${DVCC_ENABLED}");
        check6.put("isFailJobOnError", "${DVCC_FAIL_ON_ERROR}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check6));
        Map<String, Object> check7 = new HashMap<>();
        check7.put("dimcCheckname", "fileDateCheck");
        check7.put("isDimcCheckEnabled", "${FILE_DATE_CHECK_ENABLED}");
        check7.put("isFailJobOnError", "${FILE_DATE_CHECK_FAILURE_FLAG}");
        check7.put("fileDateFromFileNamePattern", "${FDC_FILENAME_PATTERN}");
        check7.put("fileDateFromFileNameFormat", "${FDC_FILENAME_DATE_FORMAT}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check7));
        Map<String, Object> check8 = new HashMap<>();
        check8.put("dimcCheckname", "businessDateCheck");
        check8.put("businessDateFromFileNamePattern", "${BDC_FILENAME_PATTERN}");
        check8.put("businessDateFromFileNameFormat", "${BDC_FILENAME_DATE_FORMAT}");
        check8.put("upperThresholdInDays", "${BDC_UPPER_THRESHOLD_DAYS}");
        check8.put("lowerThresholdInDays", "${BDC_LOWER_THRESHOLD_DAYS}");
        check8.put("controlFileRowFilterByIndex", "${BDC_CTL_FILE_ROW_FILTER_SPEC}");
        check8.put("controlFileFieldValueByIndex", "${BDC_CTL_FILE_FIELD_VALUE_SPEC}");
        check8.put("isDimcCheckEnabled", "${BDC_ENABLED}");
        check8.put("isFailJobOnError", "${BDC_FAIL_ON_ERROR}");
        DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check8));
        LOG.info("DIMC Checks Master List populated with {} items.", DIMC_CHECKS_MASTER_LIST.size());
    }

    private JsonGenerationConfig.SourceType parseSourceType(String sorTypeStr) {
        if (sorTypeStr == null)
            return JsonGenerationConfig.SourceType.UNKNOWN;
        String typeLower = sorTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "teradata":
                return JsonGenerationConfig.SourceType.TERADATA;
            case "file-fixedwidth":
            case "fixedwidthfile":
                return JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
            case "file-pipe":
                return JsonGenerationConfig.SourceType.FILE_PIPE;
            case "file-comma":
                return JsonGenerationConfig.SourceType.FILE_COMMA;
            case "mssql":
                return JsonGenerationConfig.SourceType.MSSQL;
            case "oracle":
                return JsonGenerationConfig.SourceType.ORACLE;
            case "hive":
                return JsonGenerationConfig.SourceType.HIVE;
            case "file":
                LOG.warn("Generic 'file' sourceType string found. Defaulting to Comma. Input: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.FILE_COMMA;
            default:
                LOG.warn("Unrecognized Source Type string: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.UNKNOWN;
        }
    }

    private JsonGenerationConfig.TargetType parseTargetType(String targetTypeStr) {
        if (targetTypeStr == null)
            return JsonGenerationConfig.TargetType.UNKNOWN;
        String typeLower = targetTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "hive":
                return JsonGenerationConfig.TargetType.HIVE;
            case "gcp":
                return JsonGenerationConfig.TargetType.GCP;
            default:
                LOG.warn("Unrecognized Target Type string: {}", targetTypeStr);
                return JsonGenerationConfig.TargetType.UNKNOWN;
        }
    }

    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) {
        if (item == null || item.size() == 0)
            return;
        ArrayNode array = parent.has(arrayName) && parent.get(arrayName).isArray() ? (ArrayNode) parent.get(arrayName)
                : parent.putArray(arrayName);
        array.add(item);
    }

    public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) {
        Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository
                .findById(requirementId);
        if (existingConnJsonRequirement.isPresent())
            return existingConnJsonRequirement.get();
        ObjectNode rootNode = objectMapper.createObjectNode();
        ObjectNode application = objectMapper.createObjectNode();
        ObjectNode email = objectMapper.createObjectNode();
        String sorName;
        try {
            String baseRequirementId = requirementId.contains("_") ? requirementId.split("_")[0] : requirementId;
            List<Requirement> reqs = requirementRepository.findByRequirementId(baseRequirementId);
            if (reqs.isEmpty())
                throw new ConfigurationException("Requirement not found for: " + baseRequirementId);
            sorName = reqs.get(0).getSorName();
        } catch (ConfigurationException e) { // Catch specific exception
            LOG.error("Configuration error while fetching SorName for ConnJson: {}. Error: {}", requirementId,
                    e.getMessage(), e);
            throw e; // Re-throw or handle appropriately
        } catch (Exception e) {
            LOG.error("Failed to get SorName for ConnJson: {}. Error: {}", requirementId, e.getMessage(), e);
            sorName = "UNKNOWN_SOR"; // Fallback or re-throw as critical error
        }
        String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName
                + " with Job Name $jobName has completed. </p><p> SourceDate =$sourceDate , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
        email.put("body", emailBody);
        ObjectNode job = objectMapper.createObjectNode();
        job.set("Email", email);
        application.set("job", job);
        rootNode.set("application", application);
        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {
        });
        ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
        return connJsonRequirementRepository.save(connJsonRequirement);
    }
}
