package com.wellsfargo.utcap.config;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.service.JsonRequirementService; // Needed for getChosenDimcChecksListInternal if moved here
import lombok.Getter;
import lombok.ToString;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.*;
import java.util.stream.Collectors;

@Getter
@ToString
public class JsonGenerationConfig {
    private static final Logger LOG = LoggerFactory.getLogger(JsonGenerationConfig.class);

    // Public constants for use by JsonRequirementService if needed for descriptions/paths
    public static final String SANITIZED_ZONE = "sanitized";
    public static final String RAW_ZONE = "raw";
    public static final String TABLE_TEXT = " Table ";
    public static final String INGESTION_TO_TEXT = " ingestion to ";
    // The placeholder for isDimcChecksEnabled is removed as it's now always "Y"
    // public static final String DIMC_CHECKS_ENABLED_PLACEHOLDER = "${DIMCCHECK_ENABLED}";


    public enum SourceType {
        TERADATA, FILE_FIXEDWIDTH, FILE_PIPE, FILE_COMMA, MSSQL, ORACLE, HIVE, UNKNOWN
    }

    public enum TargetType {
        HIVE, GCP, UNKNOWN
    }

    private final JiraStoryIntake jiraStoryIntake;
    private final Sor sor;
    private final ApplicationAuthorizer applicationAuthorizer;
    private final SQLScript sqlScript;
    private final List<Integer> chosenDimcCheckIndices;
    private final List<Map<String, Object>> dimcChecksMasterList;
    private final ObjectMapper objectMapper;

    private final SourceType sourceType;
    private final TargetType targetType;

    public JsonGenerationConfig(JiraStoryIntake jira, Sor sor, ApplicationAuthorizer auth, SQLScript script,
                                SourceType sourceType, TargetType targetType,
                                List<Integer> chosenDimcCheckIndices,
                                List<Map<String, Object>> dimcChecksMasterList,
                                ObjectMapper objectMapper) {
        this.jiraStoryIntake = Objects.requireNonNull(jira, "JiraStoryIntake cannot be null");
        this.sor = Objects.requireNonNull(sor, "Sor cannot be null");
        this.applicationAuthorizer = Objects.requireNonNull(auth, "ApplicationAuthorizer cannot be null");
        this.sqlScript = Objects.requireNonNull(script, "SQLScript cannot be null");
        this.sourceType = Objects.requireNonNull(sourceType, "SourceType cannot be null");
        this.targetType = Objects.requireNonNull(targetType, "TargetType cannot be null");
        this.chosenDimcCheckIndices = Objects.requireNonNull(chosenDimcCheckIndices, "chosenDimcCheckIndices cannot be null");
        this.dimcChecksMasterList = Objects.requireNonNull(dimcChecksMasterList, "dimcChecksMasterList cannot be null");
        this.objectMapper = Objects.requireNonNull(objectMapper, "objectMapper cannot be null");
    }

    // --- Section Inclusion Resolvers ---
    public boolean isIncludeIngestions() {
        return this.sourceType != SourceType.HIVE;
    }

    public boolean isIncludeFlows() { return true; }
    public boolean isIncludeDataChecks() { return true; }
    public boolean isIncludeFileExtracts() { return this.targetType == TargetType.GCP; }

    private boolean isIncludeDimcChecksInIngestionBlock() { // Renamed for clarity
        return isIncludeIngestions() &&
               (EnumSet.of(SourceType.TERADATA, SourceType.HIVE).contains(this.sourceType) ||
                isCurrentSourceTypeFileBased());
    }

    // --- Field Inclusion Set Resolvers ---
    public Set<String> getJobLevelFieldsToInclude() {
        if (this.targetType != TargetType.GCP) {
            return new HashSet<>(Arrays.asList("app", "appRemedyId", "sorRemedyId", "queryGroup", "zone", "sor"));
        }
        return Collections.emptySet();
    }

    public Set<String> getIngestionFieldsToInclude() {
        if (!isIncludeIngestions()) return Collections.emptySet();
        Set<String> fields = new HashSet<>(Arrays.asList(
            "ingestionId", "ingestionDesc", "isIngestionEnabled", "dataFrameName",
            "isCreateTargetTable", "isCacheEnabled", "isOverWritePartition",
            "abortIfDuplicate", "abortIfEmpty", "isRunEMHReport", "sourceType"
        ));
        if (this.targetType == TargetType.HIVE) {
            fields.add("targetType"); // "parquet"
        }

        if (isCurrentSourceTypeDatabase()) {
            fields.add("ConnectionHeader");
            fields.add("sourceSQLQuery");
            if (this.sourceType == SourceType.ORACLE) fields.add("sourceName");
        } else if (isCurrentSourceTypeFileBased()) {
            fields.addAll(Arrays.asList("isHeaderEnabled", "dataFilePath", "schemaFilePath", "stagingFilePath", "additionalReadConfigs")); // FW now also gets additionalReadConfigs
            if (this.sourceType != SourceType.FILE_FIXEDWIDTH) fields.add("delimiter");
            // saveOutput & targetPartition now for all file types based on last discussion
            fields.add("saveOutput");
            fields.add("targetPartition");
        }
        if (isIncludeDimcChecksInIngestionBlock()) fields.add("dimcChecks");
        return fields;
    }

    // --- Resolved Value Getters for INGESTION ---
    public String getResolvedIngestionSourceTypeValue() { /* ... same as before ... */ 
        switch (this.sourceType) {
            case TERADATA: return "teradb";
            case FILE_FIXEDWIDTH: return "fixedwidthFile";
            case FILE_PIPE: case FILE_COMMA: return "file";
            case MSSQL: return "jdbc";
            case ORACLE: return "oracle";
            case HIVE: return "hive";
            default: return "unknown";
        }
    }
    public String getResolvedIngestionConnectionHeaderValue() { /* ... same as before ... */ 
        switch (this.sourceType) {
            case TERADATA: return "teradbConn";
            case MSSQL: return "jdbcConn";
            case ORACLE: return "oracleConn";
            case HIVE: return "hiveConn"; 
            default: return null;
        }
    }
    public String getResolvedIngestionDataFrameName() {
        return this.jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_df"; // Standardized to _df
    }
    public String getResolvedIngestionSourceSQLQuery() {
        return isCurrentSourceTypeDatabase() ? this.sqlScript.getSrcTableQuery() : null;
    }
    public String getResolvedIngestionSourceName() {
        return (this.sourceType == SourceType.ORACLE) ? "${BDDMCBD_SRC_SCHEMA}." + this.jiraStoryIntake.getTableName() : null;
    }
    public String getResolvedIngestionDataFilePath() {
        return isCurrentSourceTypeFileBased() ? "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}" : null;
    }
    public String getResolvedIngestionSchemaFilePath() {
        return isCurrentSourceTypeFileBased() ? "${DCI_CONFIG_DIR_PATH}/config/" + this.sor.getSorName().toLowerCase() + "/${FEED_NAME}/metadata/" + this.jiraStoryIntake.getTableName().toLowerCase() + ".schema" : null;
    }
    public String getResolvedIngestionStagingFilePath() {
        if (this.sourceType == SourceType.FILE_FIXEDWIDTH && "Hemi".equalsIgnoreCase(this.sor.getSorName()) && "demog_exp_e3_load".equalsIgnoreCase(this.jiraStoryIntake.getTableName())) {
            return "/datalake/utcap/hemi/sanitized";
        }
        return isCurrentSourceTypeFileBased() ? "${EXP_EDL_PATH}" : null;
    }
    public String getResolvedIngestionDelimiter() {
        if (this.sourceType == SourceType.FILE_PIPE) return "|";
        if (this.sourceType == SourceType.FILE_COMMA) return ",";
        return null;
    }
    public String getResolvedIngestionIsHeaderEnabledValue() { return "Y"; } // Standardized to "Y" for all files
    public String getResolvedIngestionSaveOutputFlag() {
        return isCurrentSourceTypeFileBased() ? "Y" : null; // Standardized to "Y" for all files
    }
    public String getResolvedIngestionTargetPartition() {
         return isCurrentSourceTypeFileBased() ? "utcap_business_effective_date" : null; // Standardized for all files
    }
    
    public String getIngestionIsCreateTargetTableValue() { return "Y"; }
    public String getIngestionIsCacheEnabledValue() { return "Y"; }
    public String getIngestionIsOverWritePartitionValue() { return "Y"; }

    public ObjectNode getPrebuiltIngestionAdditionalReadConfigsNode() {
        if (isCurrentSourceTypeFileBased()) { // Now for FW, Pipe, Comma
            ObjectNode arNode = this.objectMapper.createObjectNode();
            arNode.put("quote", "\"");
            arNode.put("escape", "\"");
            return arNode;
        }
        return null;
    }

    public ObjectNode getPrebuiltIngestionDimcChecksNode() {
        if (!isIncludeDimcChecksInIngestionBlock()) return null;
        try {
            List<Map<String, Object>> selectedChecks = JsonRequirementService.getChosenDimcChecksListInternal(this.chosenDimcCheckIndices, this.dimcChecksMasterList);
            ObjectNode dimcChecksWrapper = this.objectMapper.createObjectNode();
            // NEW: isDimcChecksEnabled is always "Y"
            dimcChecksWrapper.put("isDimcChecksEnabled", "Y"); 
            ArrayNode dimcCheckArray = this.objectMapper.valueToTree(selectedChecks);
            dimcChecksWrapper.set("dimcCheck", dimcCheckArray);
            return dimcChecksWrapper;
        } catch (Exception e) {
            LOG.error("Error building DIMC checks node for source {}: {}", this.sourceType, e.getMessage());
            return null;
        }
    }

    // --- FLOWS and CHECKS Logic (FlowDetailConfig, CheckDetailConfig, and their list getters) ---
    // These methods become complex as they encapsulate all source/target specific logic for flows/checks
    // (Similar to the structure in my previous full JsonGenerationConfig where these were implemented)

    public List<FlowDetailConfig> getFlowDetailsList() {
        List<FlowDetailConfig> flowConfigs = new ArrayList<>();
        int numberOfFlows = (this.sourceType == SourceType.MSSQL || this.sourceType == SourceType.ORACLE) ? 2 : 1;
        String ingestionDfNameUsedInFlow = getResolvedIngestionDataFrameName(); // Already includes _df suffix

        for (int i = 0; i < numberOfFlows; i++) {
            String flowId = String.valueOf(i + 1);
            boolean isFirstFlow = (i == 0);
            String flowZone = SANITIZED_ZONE;
            if (numberOfFlows == 2 && isFirstFlow) flowZone = RAW_ZONE;

            String querySourceForFlow = ingestionDfNameUsedInFlow;
            String sourceHiveDbForDirectQuery = this.jiraStoryIntake.getSourceSchema() != null ? this.jiraStoryIntake.getSourceSchema() : "${SOURCE_HIVE_DATABASE}";
            if (this.sourceType == SourceType.HIVE) {
                 querySourceForFlow = sourceHiveDbForDirectQuery + "." + this.jiraStoryIntake.getSourceTableName();
            }

            String flowDesc = this.sor.getSorName().toUpperCase(Locale.ROOT) + TABLE_TEXT + this.jiraStoryIntake.getTableName() + INGESTION_TO_TEXT + this.jiraStoryIntake.getApplicationName() + " - " + flowZone;
            String query;
            String registerName = this.jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_" + flowZone;
            String flowDatabaseName = "";
            String storageName = "";
            String targetTablePath = null;
            String hivePartitionColumn = this.jiraStoryIntake.getApplicationName().toLowerCase(Locale.ROOT) + "_business_effective_date"; // Standardized
            
            String isCreateFlowTargetTable = "N"; // Standardized
            String isOverWritePartition = "Y";   // Common Default
            String isCacheEnabledFlow = "Y";     // Standardized
            String sendEmailFlow = "N";          // Standardized
            String saveOutputFlow = "Y";         // Common Default

            if (this.targetType == TargetType.GCP) {
                query = this.sqlScript.getSrcTableQuery() + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                 if (this.sourceType == SourceType.HIVE) {
                     query = "SELECT * FROM " + querySourceForFlow + " ${ENB_LIMIT} ${LIMIT_COUNT}";
                 }
                flowDesc = "Extraction of " + (this.jiraStoryIntake.getSourceTableName() != null ? this.jiraStoryIntake.getSourceTableName() : this.jiraStoryIntake.getTableName()) + " for CDMP processing.";
                registerName = this.jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT) + "_extract_df";
                saveOutputFlow = "N"; 
            } else { // Hive Target
                if (this.sourceType == SourceType.HIVE) {
                     query = "SELECT * FROM " + querySourceForFlow;
                } else if (querySourceForFlow != null) {
                     String baseSelect = "SELECT *";
                     if (isCurrentSourceTypeFileBased()) {
                        baseSelect = "SELECT df.*";
                        String auditCols = ", '${UTCAP_CRTE_BY_ID}' as utcap_crte_by_id, '${UTCAP_CRTE_DTTM}' as utcap_crte_dttm, '${UTCAP_DELTA_CD}' as utcap_delta_cd, '${UTCAP_ERR_FLG_IND}' as utcap_err_flg_ind, '${UTCAP_ERR_TXT}' as utcap_err_txt, '${UTCAP_RUN_ID}' as utcap_run_id, '${UTCAP_SOR_CD}' as utcap_sor_cd, '${UTCAP_BUSINESS_EFFECTIVE_DATE}' as utcap_business_effective_date";
                        query = baseSelect + auditCols + " FROM " + querySourceForFlow + " df";
                     } else { // DB sources to Hive (TD, MSSQL, Oracle)
                         query = baseSelect + " FROM " + querySourceForFlow;
                     }
                } else { 
                    query = this.sqlScript.getSrcDataFrameQuery();
                }

                if (isCurrentSourceTypeFileBased()) { // Standardized for files
                    storageName = "${TARGET_TABLE_NAME}"; 
                    targetTablePath = "${EXP_EDL_PATH}/${TARGET_TABLE_NAME}";
                    flowDatabaseName = "${HIVE_TARGET_DBNAME}"; 
                } else { // DB sources to Hive
                    Map<String, String> storageNameMap = Map.of("CNAPP", "consumer", "UTCAP", "utcap");
                    String appSubDir = storageNameMap.getOrDefault(this.jiraStoryIntake.getApplicationName(), this.jiraStoryIntake.getApplicationName().toLowerCase());
                    storageName = "/datalake${EDL_ENV}/" + appSubDir + "/" +
                                  this.sor.getDomainName().toLowerCase(Locale.ROOT) + "/" +
                                  this.sor.getSorName().toLowerCase(Locale.ROOT) + "/" +
                                  flowZone + "/" + this.jiraStoryIntake.getTableName();
                    flowDatabaseName = "${ETL_ENV}" + appSubDir + "_" + this.sor.getSorName().toLowerCase() + "_" + flowZone;
                }
            }
            flowConfigs.add(new FlowDetailConfig(
                    flowId, flowDesc, query, registerName, flowDatabaseName, storageName, targetTablePath,
                    hivePartitionColumn, isCreateFlowTargetTable, isOverWritePartition, isCacheEnabledFlow,
                    sendEmailFlow, "N", saveOutputFlow
            ));
        }
        return flowConfigs;
    }

    public String getResolvedDataChecksAcDcDatabase() {
        // Pass necessary fields directly if `this.getCheckDetailsList()` would cause recursion or premature calculation.
        // Or, this method simply uses the fields of `this` (Jira, Sor, Auth, SourceType, TargetType).
        boolean hasChecks = !getCheckDetailsListInternal(getFlowDetailsList()).isEmpty(); // Calculate if checks would be present

        if (hasChecks && this.targetType != TargetType.GCP) {
            if (this.sourceType == SourceType.MSSQL || this.sourceType == SourceType.ORACLE) return "${ETL_ENV}consumer_audit";
            if (this.sourceType == SourceType.FILE_FIXEDWIDTH && "utcap".equalsIgnoreCase(this.jiraStoryIntake.getApplicationName())) return "utcap_curated_mu_audit";
            if ((this.sourceType == SourceType.FILE_PIPE || this.sourceType == SourceType.FILE_COMMA) &&
                "utcap".equalsIgnoreCase(this.jiraStoryIntake.getApplicationName()) && "corelogic".equalsIgnoreCase(this.sor.getSorName())) return "${UTCAP_AUDIT_DB}";
            return this.applicationAuthorizer.getAuditTable();
        }
        return "${AUDIT_DATABASE}";
    }
    
    // Internal helper to avoid recursion if getCheckDetailsList calls getResolvedDataChecksAcDcDatabase
    private List<CheckDetailConfig> getCheckDetailsListInternal(List<FlowDetailConfig> flowDetailsForChecks) {
        List<CheckDetailConfig> checkConfigs = new ArrayList<>();
        if (this.targetType == TargetType.GCP || flowDetailsForChecks.isEmpty() || !isIncludeDataChecks()) {
            return checkConfigs;
        }
        for (FlowDetailConfig currentFlow : flowDetailsForChecks) {
            String checkId = currentFlow.getFlowId();
            String checkSourceQueryDataFrame;
            String sourceHiveDb = this.jiraStoryIntake.getSourceSchema() != null ? this.jiraStoryIntake.getSourceSchema() : "${SOURCE_HIVE_DATABASE}";

            if (this.sourceType == SourceType.HIVE) {
                checkSourceQueryDataFrame = sourceHiveDb + "." + this.jiraStoryIntake.getSourceTableName();
            } else if (getResolvedIngestionDataFrameName() != null) {
                checkSourceQueryDataFrame = getResolvedIngestionDataFrameName();
            } else {
                LOG.warn("Cannot determine source DataFrame for data check for source: {} and flow: {}", this.sourceType, currentFlow.getFlowId());
                continue;
            }
            String checkSourceQuery = "select count(*) from " + checkSourceQueryDataFrame;
            String checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowDatabaseName() + "." + this.jiraStoryIntake.getTableName() +
                                     " where " + currentFlow.getResolvedFlowHivePartitionColumn() + "='${EAPP_DCT_BUSINESS_EFFECTIVE_DATE}'";
             if (this.sourceType == SourceType.FILE_FIXEDWIDTH && "demog_exp_e3_load_sanitized".equals(currentFlow.getResolvedFlowRegisterName())) {
                 checkTargetQuery = "select count(*) from " + currentFlow.getResolvedFlowRegisterName();
            }

            checkConfigs.add(new CheckDetailConfig(
                    checkId, "HHRCNTCHK", "Data Count Check",
                    checkSourceQuery, checkTargetQuery,
                    "Y", "Y", "N", "N", "N" // isFailJobOnDataCheckError is "N" always
            ));
        }
        return checkConfigs;
    }

    public List<CheckDetailConfig> getCheckDetailsList() {
        return getCheckDetailsListInternal(getFlowDetailsList());
    }


    public ObjectNode getPrebuiltDataChecksPreChecksNode() {
        if (!isIncludeDataChecks()) return null;
        // Logic from JsonRequirementService to build this node, simplified
        ObjectNode preChecksNode = this.objectMapper.createObjectNode();
        boolean overallPreCheckEnabled = this.sourceType != SourceType.FILE_FIXEDWIDTH;
        preChecksNode.put("isPreCheckEnabled", overallPreCheckEnabled ? "Y" : "${PRECHECK_ENABLED}");
        ArrayNode preCheckArray = preChecksNode.putArray("preCheck");
        // Simplified, actual flags might need more context from JiraStoryIntake or ApplicationAuthorizer if they vary
        addPreCheckItemToArray(preCheckArray, "ServiceIDCheck", "Y", "Y");
        addPreCheckItemToArray(preCheckArray, "TargetDataBaseAvailabilityCheck", this.targetType == TargetType.HIVE ? "Y" : "N", "Y");
        // ... add all other preCheckItems similarly, using this.sourceType, this.targetType for conditions
        return preChecksNode;
    }

    private void addPreCheckItemToArray(ArrayNode array, String name, String enabled, String failOnError) {
        ObjectNode item = this.objectMapper.createObjectNode();
        item.put("preCheckName", name);
        item.put("isPreCheckEnabled", enabled);
        item.put("isFailJobOnPreCheckError", failOnError);
        array.add(item);
    }

    // --- Helper methods for internal logic ---
    private boolean isCurrentSourceTypeDatabase() {
        return EnumSet.of(SourceType.TERADATA, SourceType.MSSQL, SourceType.ORACLE, SourceType.HIVE).contains(this.sourceType);
    }
    private boolean isCurrentSourceTypeFileBased() {
         return EnumSet.of(SourceType.FILE_FIXEDWIDTH, SourceType.FILE_PIPE, SourceType.FILE_COMMA).contains(this.sourceType);
    }

    // Nested static classes for FlowDetailConfig and CheckDetailConfig should be here
    // ... (as defined in previous versions of JsonGenerationConfig.java) ...
}









package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.config.JsonGenerationConfig;
import com.wellsfargo.utcap.exception.ConfigurationException;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;

@Service
public class JsonRequirementService {

    private static final Logger LOG = LoggerFactory.getLogger(JsonRequirementService.class);
    // DIMC Master List and helper method to get chosen checks
    private static final List<Map<String, Object>> DIMC_CHECKS_MASTER_LIST = new ArrayList<>();


    private final ApplicationAuthRepository applicationAuthRepository;
    private final RequirementRepository requirementRepository;
    private final JsonRequirementRepository jsonRequirementRepository;
    private final JiraStoryIntakeRepository jiraStoryIntakeRepository;
    private final SorRepository sorRepository;
    private final SQLScriptRepository sqlScriptRepository;
    private final ConnJsonRequirementRepository connJsonRequirementRepository;
    private final ObjectMapper objectMapper;

    @Autowired
    public JsonRequirementService(ApplicationAuthRepository applicationAuthRepository,
                                  RequirementRepository requirementRepository,
                                  JsonRequirementRepository jsonRequirementRepository,
                                  JiraStoryIntakeRepository jiraStoryIntakeRepository,
                                  SorRepository sorRepository,
                                  SQLScriptRepository sqlScriptRepository,
                                  ConnJsonRequirementRepository connJsonRequirementRepository,
                                  ObjectMapper objectMapper) {
        this.applicationAuthRepository = applicationAuthRepository;
        this.requirementRepository = requirementRepository;
        this.jsonRequirementRepository = jsonRequirementRepository;
        this.jiraStoryIntakeRepository = jiraStoryIntakeRepository;
        this.sorRepository = sorRepository;
        this.sqlScriptRepository = sqlScriptRepository;
        this.connJsonRequirementRepository = connJsonRequirementRepository;
        this.objectMapper = objectMapper;
        populateDimcChecksMasterList(); // Initialize master list
    }

    public JsonRequirement saveJson(String requirementId, Object newJson) {
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId)
                .orElse(new JsonRequirement(requirementId, null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public JsonRequirement getOrCreateJsonRequirement(String requirementId) throws JsonProcessingException, ConfigurationException {
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);
        if (existingJsonRequirement.isPresent()) {
            return existingJsonRequirement.get();
        }
        LOG.info("Generating new JSON requirement for ID: {}", requirementId);
        JsonGenerationConfig config = buildJsonGenerationConfig(requirementId); // Simpler call
        ObjectNode rootNode = objectMapper.createObjectNode();
        rootNode.set("application", createApplicationNode(config));
        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);
        return jsonRequirementRepository.save(jsonRequirement);
    }

    private JsonGenerationConfig buildJsonGenerationConfig(String requirementId) throws ConfigurationException {
        String jiraStoryId = requirementId.split("_")[0];
        JiraStoryIntake jira = jiraStoryIntakeRepository.findById(jiraStoryId)
                .orElseThrow(() -> new ConfigurationException("JiraStoryIntake not found for ID: " + jiraStoryId));
        Sor sor = sorRepository.findBySorName(jira.getApplicationSorName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("Sor not found for name: " + jira.getApplicationSorName()));
        ApplicationAuthorizer auth = applicationAuthRepository.findByApplicationName(jira.getApplicationName()).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("ApplicationAuthorizer not found for name: " + jira.getApplicationName()));
        SQLScript sqlScript = sqlScriptRepository.findByRequirementId(jiraStoryId).stream().findFirst()
                .orElseThrow(() -> new ConfigurationException("SQLScript not found for requirement ID: " + jiraStoryId));

        JsonGenerationConfig.SourceType sourceTypeEnum = parseSourceType(jira.getSorType());
        JsonGenerationConfig.TargetType targetTypeEnum = parseTargetType(jira.getTargetType());

        if (sourceTypeEnum == JsonGenerationConfig.SourceType.UNKNOWN)
            throw new ConfigurationException("Unknown Source Type: " + jira.getSorType());
        if (targetTypeEnum == JsonGenerationConfig.TargetType.UNKNOWN)
            throw new ConfigurationException("Unknown Target Type: " + jira.getTargetType());

        // Determine source-specific DIMC check indices
        List<Integer> chosenDimcCheckIndices = determineSourceSpecificDimcIndices(sourceTypeEnum);

        // Instantiate the "Smart Config" - it will do the heavy lifting with its internal resolvers
        return new JsonGenerationConfig(
                jira, sor, auth, sqlScript,
                sourceTypeEnum, targetTypeEnum,
                chosenDimcCheckIndices,
                DIMC_CHECKS_MASTER_LIST, // Pass the master list
                this.objectMapper
        );
    }
    
    // This method now becomes crucial for defining which DIMC checks are default for each source
    private List<Integer> determineSourceSpecificDimcIndices(JsonGenerationConfig.SourceType sourceType) {
        // Indices refer to DIMC_CHECKS_MASTER_LIST (0-based)
        // Master List Order (example):
        // 0: duplicateFileLoadChecks
        // 1: dataVolumeConsistencyCheck
        // 2: missingFileCheck
        // 3: deliveryCheck
        // 4: missingDbObjectCheck
        // 5: emptySourceCheck
        // 6: dataVolumeCompletenessCheck (FW specific)
        // 7: fileDateCheck (FW specific)
        // 8: businessDateCheck (FW specific)
        switch (sourceType) {
            case TERADATA:
            case HIVE: // Assuming Hive as source uses similar DB checks
                return Arrays.asList(4, 5, 3, 1); // missingDbObject, emptySource, delivery, dataVolumeConsistency
            case FILE_FIXEDWIDTH:
                // FW example had: dataVolumeCompleteness(6), duplicateFileLoad(0), dataVolumeConsistency(1), missingFile(2), delivery(3), fileDate(7), businessDate(8)
                // Note: Some of these might be enabled/disabled by placeholders. This list just makes them available.
                return Arrays.asList(6, 0, 1, 2, 3, 7, 8);
            case FILE_PIPE:
            case FILE_COMMA:
                return Arrays.asList(0, 1, 2, 3); // duplicateFileLoad, dataVolumeConsistency, missingFile, delivery
            // MSSQL and ORACLE do not have DIMC checks as per user rules.
            case MSSQL:
            case ORACLE:
            default:
                return Collections.emptyList();
        }
    }

    private ObjectNode createApplicationNode(JsonGenerationConfig config) {
        ObjectNode applicationNode = objectMapper.createObjectNode();
        applicationNode.put("isParallelProcessing", "N");
        applicationNode.set("jobs", createJobsNode(config));
        return applicationNode;
    }

    private ObjectNode createJobsNode(JsonGenerationConfig config) {
        ObjectNode jobsNode = objectMapper.createObjectNode();
        jobsNode.set("job", createJobNode(config));
        return jobsNode;
    }

    private ObjectNode createJobNode(JsonGenerationConfig config) {
        ObjectNode jobNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake();
        Sor sor = config.getSor();
        ApplicationAuthorizer auth = config.getApplicationAuthorizer();

        jobNode.put("JobName", jira.getTableName()); // Capital J

        Set<String> jobFields = config.getJobLevelFieldsToInclude();
        if (jobFields.contains("app")) jobNode.put("app", jira.getApplicationName());
        if (jobFields.contains("appRemedyId")) jobNode.put("appRemedyId", auth.getAppRemedyId());
        if (jobFields.contains("sorRemedyId")) jobNode.put("sorRemedyId", sor.getRemedyId());
        if (jobFields.contains("queryGroup")) jobNode.put("queryGroup", "dimc_check_" + sor.getSorName().toLowerCase(Locale.ROOT));
        if (jobFields.contains("zone")) jobNode.put("zone", JsonGenerationConfig.SANITIZED_ZONE);
        if (jobFields.contains("sor")) jobNode.put("sor", sor.getSorName().toLowerCase(Locale.ROOT));

        jobNode.put("jobId", "1");
        jobNode.put("jobDescription", determineJobDescription(config));
        jobNode.put("isJobEnabled", "Y");
        jobNode.set("Email", createEmailNode(config));

        if (config.isIncludeIngestions()) {
            ObjectNode ingestionsNode = buildGenericIngestionNode(config);
            if (ingestionsNode != null && ingestionsNode.size() > 0) jobNode.set("ingestions", ingestionsNode);
        }
        if (config.isIncludeFlows()) {
            ObjectNode flowsNode = buildGenericFlowsNode(config);
            if (flowsNode != null && flowsNode.size() > 0) jobNode.set("flows", flowsNode);
        }
        if (config.isIncludeDataChecks()) {
            ObjectNode dataChecksNode = buildGenericDataChecksNode(config);
             if (dataChecksNode != null && dataChecksNode.size() > 0) jobNode.set("dataChecks", dataChecksNode);
        }
        if (config.isIncludeFileExtracts()) {
            ObjectNode fileExtractsNode = buildGenericFileExtractsNode(config);
             if (fileExtractsNode != null && fileExtractsNode.size() > 0) jobNode.set("fileExtracts", fileExtractsNode);
        }
        return jobNode;
    }
    
    private String determineJobDescription(JsonGenerationConfig config){
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();
        String appName = jira.getApplicationName(); String sorName = sor.getSorName();
        String tableName = jira.getTableName();

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            return "CDMP Extract for Hive table - " + tableName.toLowerCase(Locale.ROOT);
        } else {
            if ("BKPF".equalsIgnoreCase(sorName))  return sorName + " " + tableName + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
            if ("Hemi".equalsIgnoreCase(sorName)) return sorName + JsonGenerationConfig.TABLE_TEXT + tableName + JsonGenerationConfig.INGESTION_TO_TEXT + "Hive";
            if ("corelogic".equalsIgnoreCase(sorName)) return "utcap " + sorName + " ingestion for feed " + tableName;
            if ("BDDM".equalsIgnoreCase(sorName)) return sorName + JsonGenerationConfig.TABLE_TEXT + tableName + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
            // Default (e.g. Teradata from first example)
            return sorName.toUpperCase() + JsonGenerationConfig.TABLE_TEXT + tableName.toLowerCase(Locale.ROOT) + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
        }
    }

    private ObjectNode createEmailNode(JsonGenerationConfig config) {
        ObjectNode emailNode = objectMapper.createObjectNode();
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();
        String fromAddress = "${EMAIL_TO_LIST}"; String toAddress = "${EMAIL_CC_LIST}";
        String subject = "Data Extract from " + sor.getSorName().toUpperCase(Locale.ROOT) +
                         " to " + jira.getApplicationName() + " - " + jira.getTableName().toLowerCase(Locale.ROOT);

        if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP) {
            fromAddress = "${FROM_EMAIL}"; toAddress = "${TO_EMAIL}";
            subject = "CDMP Extraction - " + jira.getTableName().toLowerCase(Locale.ROOT);
        } else if (config.getSourceType() == JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH) {
            fromAddress = "${EMAIL_FROM_LIST}";
            subject = "Fixed width UTCAP Demog Feed ingestion " + jira.getTableName() + " hive table";
        } else if (EnumSet.of(JsonGenerationConfig.SourceType.FILE_PIPE, JsonGenerationConfig.SourceType.FILE_COMMA).contains(config.getSourceType())
                   && "corelogic".equalsIgnoreCase(sor.getSorName())) {
            subject = "utcap ingestion for feed " + jira.getTableName();
        } else if(config.getSourceType() == JsonGenerationConfig.SourceType.TERADATA && "bmg".equalsIgnoreCase(sor.getSorName())) {
            // Specific from Teradata example
            fromAddress = "${EMAIL_TO_LIST_FL}";
            toAddress = "${EMAIL_CC_LIST_FL}";
            subject = "Data Extract from " + sor.getSorName().toUpperCase() + " to " + jira.getApplicationName() + "-" + jira.getTableName().toLowerCase();
        }

        emailNode.put("fromAddress", fromAddress);
        emailNode.put("toAddress", toAddress);
        emailNode.put("subject", subject);
        emailNode.put("isPriorityAlert", "Y");
        return emailNode;
    }

    private ObjectNode buildGenericIngestionNode(JsonGenerationConfig config) {
        Set<String> fields = config.getIngestionFieldsToInclude();
        if (fields.isEmpty()) return null;
        ObjectNode item = objectMapper.createObjectNode();

        if (fields.contains("ingestionId")) item.put("ingestionId", "1");
        if (fields.contains("ingestionDesc")) item.put("ingestionDesc", determineIngestionDescription(config));
        if (fields.contains("isIngestionEnabled")) item.put("isIngestionEnabled", "Y");
        if (fields.contains("sourceType")) item.put("sourceType", config.getResolvedIngestionSourceTypeValue());
        if (fields.contains("ConnectionHeader") && config.getResolvedIngestionConnectionHeaderValue() != null)
            item.put("ConnectionHeader", config.getResolvedIngestionConnectionHeaderValue());
        if (fields.contains("delimiter") && config.getResolvedIngestionDelimiter() != null)
            item.put("delimiter", config.getResolvedIngestionDelimiter());
        if (fields.contains("dataFrameName")) item.put("dataFrameName", config.getResolvedIngestionDataFrameName());
        if (fields.contains("isCreateTargetTable")) item.put("isCreateTargetTable", config.getIngestionIsCreateTargetTableValue());
        if (fields.contains("isCacheEnabled")) item.put("isCacheEnabled", config.getIngestionIsCacheEnabledValue());
        if (fields.contains("isOverWritePartition")) item.put("isOverWritePartition", config.getIngestionIsOverWritePartitionValue());
        if (fields.contains("abortIfDuplicate")) item.put("abortIfDuplicate", "N");
        if (fields.contains("abortIfEmpty")) item.put("abortIfEmpty", "N");
        if (fields.contains("isRunEMHReport")) item.put("isRunEMHReport", "N");
        if (fields.contains("sourceSQLQuery") && config.getResolvedIngestionSourceSQLQuery() != null)
            item.put("sourceSQLQuery", config.getResolvedIngestionSourceSQLQuery());
        if (fields.contains("sourceName") && config.getResolvedIngestionSourceName() != null)
            item.put("sourceName", config.getResolvedIngestionSourceName());
        if (fields.contains("dataFilePath") && config.getResolvedIngestionDataFilePath() != null)
            item.put("dataFilePath", config.getResolvedIngestionDataFilePath());
        if (fields.contains("schemaFilePath") && config.getResolvedIngestionSchemaFilePath() != null)
            item.put("schemaFilePath", config.getResolvedIngestionSchemaFilePath());
        if (fields.contains("stagingFilePath") && config.getResolvedIngestionStagingFilePath() != null)
            item.put("stagingFilePath", config.getResolvedIngestionStagingFilePath());
        if (fields.contains("isHeaderEnabled") && config.getResolvedIngestionIsHeaderEnabled() != null)
            item.put("isHeaderEnabled", config.getResolvedIngestionIsHeaderEnabled());
        if (fields.contains("saveOutput") && config.getResolvedIngestionSaveOutputFlag() != null)
            item.put("saveOutput", config.getResolvedIngestionSaveOutputFlag());
        if (fields.contains("targetPartition") && config.getResolvedIngestionTargetPartition() != null)
            item.put("targetPartition", config.getResolvedIngestionTargetPartition());
        if (fields.contains("targetType") && config.getTargetType() == JsonGenerationConfig.TargetType.HIVE)
             item.put("targetType", "parquet");
        if (fields.contains("additionalReadConfigs") && config.getPrebuiltIngestionAdditionalReadConfigsNode() != null)
            item.set("additionalReadConfigs", config.getPrebuiltIngestionAdditionalReadConfigsNode());
        if (fields.contains("dimcChecks") && config.getPrebuiltIngestionDimcChecksNode() != null) // isIncludeDimcChecksInIngestion already checked by config
            item.set("dimcChecks", config.getPrebuiltIngestionDimcChecksNode());
        
        ObjectNode finalNode = objectMapper.createObjectNode();
        if(item.size() > 0) addToArray(finalNode, "ingestion", item);
        return finalNode.size() > 0 ? finalNode : null;
    }
    
    private String determineIngestionDescription(JsonGenerationConfig config){
        // Simplified further, relies on consistent naming from Jira/SOR for description pieces
        JiraStoryIntake jira = config.getJiraStoryIntake(); Sor sor = config.getSor();
        String appName = jira.getApplicationName(); String sorName = sor.getSorName(); String tableName = jira.getTableName();
        return sorName.toUpperCase() + JsonGenerationConfig.TABLE_TEXT + tableName.toLowerCase() + JsonGenerationConfig.INGESTION_TO_TEXT + appName;
    }

    private ObjectNode buildGenericFlowsNode(JsonGenerationConfig config) {
        // (Similar to previous version: uses config.getFlowDetailsList() and assembles)
        if (!config.isIncludeFlows() || config.getFlowDetailsList().isEmpty()) return null;
        ObjectNode flowsNode = objectMapper.createObjectNode();
        ArrayNode flowArray = flowsNode.putArray("flow");
        for (JsonGenerationConfig.FlowDetailConfig detail : config.getFlowDetailsList()) {
            ObjectNode item = objectMapper.createObjectNode();
            item.put("flowId", detail.getFlowId());
            item.put("flowDesc", detail.getResolvedFlowDescription());
            item.put("isFlowEnabled", "Y");
            item.put("query", detail.getResolvedFlowQuery());
            item.put("registerName", detail.getResolvedFlowRegisterName());
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE) {
                item.put("saveOutput", detail.getSaveOutputValue());
                if(detail.getResolvedFlowDatabaseName()!=null) item.put("flowDataBaseName", detail.getResolvedFlowDatabaseName());
                if(detail.getResolvedFlowStorageName()!=null) item.put("storageName", detail.getResolvedFlowStorageName());
                if(detail.getResolvedFlowTargetTablePath()!=null) item.put("targetTablePath", detail.getResolvedFlowTargetTablePath());
                if(detail.getResolvedFlowHivePartitionColumn()!=null) item.put("hivePartitionColumn", detail.getResolvedFlowHivePartitionColumn());
                item.put("isCreateFlowTargetTable", detail.getIsCreateFlowTargetTableValue());
                item.put("isOverWritePartition", detail.getIsOverWritePartitionValue());
                item.put("isCacheEnabled", detail.getIsCacheEnabledValue());
                item.put("sendEmail", detail.getSendEmailValue());
                item.put("abortIfEmpty", detail.getAbortIfEmptyValue());
            }
            flowArray.add(item);
        }
        return flowsNode;
    }

    private ObjectNode buildGenericDataChecksNode(JsonGenerationConfig config) {
        // (Similar to previous version: uses config.getCheckDetailsList() and assembles)
        if (!config.isIncludeDataChecks()) return null;
        boolean hasChecks = !config.getCheckDetailsList().isEmpty();
        ObjectNode preChecksNodeFromConfig = config.getPrebuiltDataChecksPreChecksNode();
        boolean hasPreChecks = preChecksNodeFromConfig != null && preChecksNodeFromConfig.has("preCheck") && preChecksNodeFromConfig.get("preCheck").size() > 0 ;

        if (!hasChecks && !hasPreChecks) {
            if (config.getTargetType() == JsonGenerationConfig.TargetType.GCP && "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase())) return null;
            if (config.getTargetType() == JsonGenerationConfig.TargetType.HIVE && "${AUDIT_DATABASE}".equals(config.getResolvedDataChecksAcDcDatabase()) ) return null;
        }
        ObjectNode dataChecksNode = objectMapper.createObjectNode();
        if(config.getResolvedDataChecksAcDcDatabase() != null) dataChecksNode.put("acDCDatabase", config.getResolvedDataChecksAcDcDatabase());
        dataChecksNode.put("isTrendingEnabled", "N"); 
        if (hasChecks) {
            ArrayNode checkArray = dataChecksNode.putArray("check"); 
            for (JsonGenerationConfig.CheckDetailConfig detail : config.getCheckDetailsList()) {
                ObjectNode item = objectMapper.createObjectNode();
                item.put("checkId", detail.getCheckId());
                item.put("checkName", detail.getCheckName());
                item.put("checkDesc", detail.getCheckDescription());
                item.put("isCheckEnabled", detail.getIsCheckEnabledValue());
                item.put("isUserSupplyQuery", detail.getIsUserSupplyQueryValue());
                item.put("readSourceQueryFromPath", detail.getReadSourceQueryFromPathValue());
                item.put("sourceQuery", detail.getResolvedCheckSourceQuery());
                item.put("targetQuery", detail.getResolvedCheckTargetQuery());
                item.put("readTargetQueryFromPath", detail.getReadTargetQueryFromPathValue());
                item.put("isFailJobOnDataCheckError", detail.getIsFailJobOnDataCheckErrorValue());
                checkArray.add(item);
            }
        }
        if(hasPreChecks) dataChecksNode.set("preChecks", preChecksNodeFromConfig);
        return dataChecksNode.size() == 0 || (dataChecksNode.size() <=2 && !hasChecks && !hasPreChecks) ? null : dataChecksNode;
    }

    private ObjectNode buildGenericFileExtractsNode(JsonGenerationConfig config) {
        // (Same as before, relies on config.getJiraStoryIntake(), config.getFlowDetailsList() for registerName)
        if (!config.isIncludeFileExtracts()) return null;
        ObjectNode fileExtractsNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = fileExtractsNode.putArray("fileExtract");
        JiraStoryIntake jira = config.getJiraStoryIntake();
        ObjectNode item = objectMapper.createObjectNode();
        item.put("fileExtractId", "1");
        item.put("isFileExtractEnabled", "${IS_FILE_EXTRACT_ENABLED}"); 
        String sourceDescriptionPart = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "hive data" : jira.getTableName().toLowerCase(Locale.ROOT);
        item.put("fileDescription", "Extracting " + sourceDescriptionPart + " to avro file for CDMP");
        item.put("subjectArea", config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "cdmp using FileExtract" : "CDMP");
        item.put("zone", JsonGenerationConfig.RAW_ZONE);
        item.put("system", "cdmp");
        item.put("fileType", "avro");
        String flowRegisterNameForExtract = config.getFlowDetailsList().isEmpty() ? "flow_df" : config.getFlowDetailsList().get(0).getResolvedFlowRegisterName();
        item.put("extractQuery", "select * from " + flowRegisterNameForExtract); 
        item.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        item.put("externalFilePath", "${TGT_TBL_NDM_OUT_DIR_PATH}");
        ObjectNode cdmpParams = objectMapper.createObjectNode();
        cdmpParams.put("outputFormat", "avro");
        String cdmpFileNamePrefix = config.getSourceType() == JsonGenerationConfig.SourceType.HIVE ? "UTCAPT_" : "";
        cdmpParams.put("cdmpFileName", cdmpFileNamePrefix + jira.getTableName().toLowerCase(Locale.ROOT) + "_cdmp_extract");
        cdmpParams.put("cdmpPatternMode", "daily");
        cdmpParams.put("isCustomPartitionRequired", "true"); 
        cdmpParams.put("desiredPartFileSize", "${PART_FILE_SIZE}");
        item.set("cdmpAdditionalParams", cdmpParams);
        fileExtractArray.add(item);
        return fileExtractsNode;
    }
    
    // Corrected: Moved DIMC helper to be static within service, taking masterList as param
    public static List<Map<String, Object>> getChosenDimcChecksListInternal(List<Integer> chosenIndices, List<Map<String, Object>> masterList) {
        if (chosenIndices == null || chosenIndices.isEmpty() || masterList == null || masterList.isEmpty()) return Collections.emptyList();
        return chosenIndices.stream()
                .filter(index -> index != null && index >= 0 && index < masterList.size())
                .map(index -> new HashMap<>(masterList.get(index)))
                .collect(Collectors.toList());
    }

    private static synchronized void populateDimcChecksMasterList() {
        // (Same definition as previous, ensuring HashMap<String, Object> is used)
        if (!DIMC_CHECKS_MASTER_LIST.isEmpty()) return;
        LOG.info("Populating DIMC Checks Master List...");
        Map<String, Object> check0=new HashMap<>(); check0.put("dimcCheckname","duplicateFileLoadChecks"); check0.put("isDimcCheckEnabled","${DUPLICATE_FILELOAD_CHECK_ENABLED}"); check0.put("isFailJobOnError","${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check0));
        Map<String, Object> check1=new HashMap<>(); check1.put("dimcCheckname","dataVolumeConsistencyCheck"); check1.put("isDimcCheckEnabled","${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}"); check1.put("upperThreshold","${UPPER_THRESHOLD}"); check1.put("lowerThreshold","${LOWER_THRESHOLD}"); check1.put("numPastDays","${NUM_PAST_DAYS}"); check1.put("isFailJobOnError","${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check1));
        Map<String, Object> check2=new HashMap<>(); check2.put("dimcCheckname","missingFileCheck"); check2.put("isDimcCheckEnabled","${MISSING_FILE_CHECK_ENABLED}"); check2.put("isFailJobOnError","${MISSING_FILE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check2));
        Map<String, Object> check3=new HashMap<>(); check3.put("dimcCheckname","deliveryCheck"); check3.put("isDimcCheckEnabled","${DELIVERY_CHECK_ENABLED}"); check3.put("SLA","${DELIVERY_CHECK_SLA}"); check3.put("isFailJobOnError","${DELIVERY_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check3));
        Map<String, Object> check4=new HashMap<>(); check4.put("dimcCheckname","missingDbObjectCheck"); check4.put("isDimcCheckEnabled","${MISSING_DB_OBJECT_CHECK_ENABLED}"); check4.put("isFailJobOnError","${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check4));
        Map<String, Object> check5=new HashMap<>(); check5.put("dimcCheckname","emptySourceCheck"); check5.put("isDimcCheckEnabled","${EMPTY_SOURCE_CHECK_ENABLED}"); check5.put("isFailJobOnError","${EMPTY_SOURCE_CHECK_FAILURE_FLAG}"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check5));
        Map<String, Object> check6=new HashMap<>(); check6.put("dimcCheckname","dataVolumeCompletenessCheck"); check6.put("controlFileRowFilterByIndex","/apps/ndm/etlutcap/inbound/experian.ctl"); check6.put("controlFileFieldValueByIndex","recordCount=3,fileDate=1"); check6.put("isDimcCheckEnabled","N"); check6.put("isFailJobOnError","N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check6));
        Map<String, Object> check7=new HashMap<>(); check7.put("dimcCheckname","fileDateCheck"); check7.put("isDimcCheckEnabled","${FILE_DATE_CHECK_ENABLED}"); check7.put("isFailJobOnError","${FILE_DATE_CHECK_FAILURE_FLAG}"); check7.put("fileDateFromFileNamePattern",".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check7.put("fileDateFromFileNameFormat","yyyyMMdd"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check7));
        Map<String, Object> check8=new HashMap<>(); check8.put("dimcCheckname","businessDateCheck"); check8.put("businessDateFromFileNamePattern",".*([0-9]{4}[0-9]{2}[0-9]{2}).*"); check8.put("businessDateFromFileNameFormat","yyyyMMdd"); check8.put("upperThresholdInDays","20"); check8.put("lowerThresholdInDays","20"); check8.put("controlFileRowFilterByIndex","2"); check8.put("controlFileFieldValueByIndex","fileDate=1"); check8.put("isDimcCheckEnabled","N"); check8.put("isFailJobOnError","N"); DIMC_CHECKS_MASTER_LIST.add(Collections.unmodifiableMap(check8));
        LOG.info("DIMC Checks Master List populated with {} items.", DIMC_CHECKS_MASTER_LIST.size());
    }
    
    private JsonGenerationConfig.SourceType parseSourceType(String sorTypeStr) { /* ... same as before ... */ 
        if (sorTypeStr == null) return JsonGenerationConfig.SourceType.UNKNOWN;
        String typeLower = sorTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "teradata": return JsonGenerationConfig.SourceType.TERADATA;
            case "file-fixedwidth": case "fixedwidthfile": return JsonGenerationConfig.SourceType.FILE_FIXEDWIDTH;
            case "file-pipe": return JsonGenerationConfig.SourceType.FILE_PIPE;
            case "file-comma": return JsonGenerationConfig.SourceType.FILE_COMMA;
            case "mssql": return JsonGenerationConfig.SourceType.MSSQL;
            case "oracle": return JsonGenerationConfig.SourceType.ORACLE;
            case "hive": return JsonGenerationConfig.SourceType.HIVE;
            case "file": 
                 LOG.warn("Generic 'file' sourceType string found. Defaulting to Comma. For specific behavior, ensure input sorType is 'file-comma', 'file-pipe', or 'file-fixedwidth'. Input: {}", sorTypeStr);
                 return JsonGenerationConfig.SourceType.FILE_COMMA; 
            default:
                LOG.warn("Unrecognized Source Type string: {}", sorTypeStr);
                return JsonGenerationConfig.SourceType.UNKNOWN;
        }
    }
    private JsonGenerationConfig.TargetType parseTargetType(String targetTypeStr) { /* ... same as before ... */ 
        if (targetTypeStr == null) return JsonGenerationConfig.TargetType.UNKNOWN;
        String typeLower = targetTypeStr.toLowerCase(Locale.ROOT);
        switch (typeLower) {
            case "hive": return JsonGenerationConfig.TargetType.HIVE;
            case "gcp": return JsonGenerationConfig.TargetType.GCP;
            default:
                LOG.warn("Unrecognized Target Type string: {}", targetTypeStr);
                return JsonGenerationConfig.TargetType.UNKNOWN;
        }
    }
    private static void addToArray(ObjectNode parent, String arrayName, ObjectNode item) { /* ... same as before ... */ 
        if (item == null || item.size() == 0) return;
        ArrayNode array = parent.has(arrayName) && parent.get(arrayName).isArray() ?
                (ArrayNode) parent.get(arrayName) : parent.putArray(arrayName);
        array.add(item);
    }
     public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) { /* ... same as before ... */ 
         Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository.findById(requirementId);
         if (existingConnJsonRequirement.isPresent()) {
             return existingConnJsonRequirement.get();
         }
         ObjectNode rootNode = objectMapper.createObjectNode();
         ObjectNode application = objectMapper.createObjectNode();
         ObjectNode email = objectMapper.createObjectNode();
         String sorName;
         try {
             String baseRequirementId = requirementId.contains("_") ? requirementId.split("_")[0] : requirementId;
             List<Requirement> reqs = requirementRepository.findByRequirementId(baseRequirementId);
             if (reqs.isEmpty()) throw new ConfigurationException("Requirement not found in RequirementRepository for: " + baseRequirementId);
             sorName = reqs.get(0).getSorName();
         } catch (Exception e) {
             LOG.error("Failed to get SorName for ConnJson requirementId: {}. Error: {}", requirementId, e.getMessage(), e);
             sorName = "UNKNOWN_SOR";
         }
         String emailBody = "<p>Hello All,</p><p> Ingestion for " + sorName + " with Job Name $jobName has completed. " +
                           "</p><p> SourceDate =$sourceDate , TargetDate = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p><html>";
         email.put("body", emailBody);
         ObjectNode job = objectMapper.createObjectNode();
         job.set("Email", email);
         application.set("job", job);
         rootNode.set("application", application);
         Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
         ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, json);
         return connJsonRequirementRepository.save(connJsonRequirement);
    }
}
