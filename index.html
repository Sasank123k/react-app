package com.wellsfargo.utcap.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import com.wellsfargo.utcap.model.*;
import com.wellsfargo.utcap.repository.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import java.util.*;

@Service // K007363 +1
public class JsonRequirementService {
    private static final Logger LOG = LoggerFactory.getLogger(COMPAdditionalJson.class); // no usages
    private static final String SANITIZED = "sanitized"; // 6 usages
    private static final String INGESTION_ID = "ingestionId"; // 5 usages
    private static final String TABLE = "Table"; // 5 usages
    private static final List<Map<Object, Object>> DIMC_CHECKS_LIST = new ArrayList<>(); // 8 usages

    @Autowired
    Public ApplicationAuthRepository applicationAuthRepository;

    @Autowired
    public RequirementRepository requirementRepository;

    @Autowired
    public JsonRequirementRepository jsonRequirementRepository;

    @Autowired
    public JiraStoryIntakeRepository jiraStoryIntakeRepository;

    @Autowired
    public SorRepository sorRepository;

    @Autowired
    public SQLScriptRepository sqlScriptRepository;

    @Autowired // 2 usages
    public ConnJsonRequirementRepository connJsonRequirementRepository;


    public JsonRequirement saveJson(String requirementId, ObjectNode newJson) { // K007363
        JsonRequirement entity = jsonRequirementRepository.findById(requirementId).orElse(new JsonRequirement(requirementId, newJson: null));
        entity.setJson(newJson);
        return jsonRequirementRepository.save(entity);
    }

    public static void populateDimcChecksList() { // 1 usage K097363
        //FILE CHECKS
        Map<Object, Object> duplicateFileLoadCheck = new HashMap<>();
        duplicateFileLoadCheck.put("dimcCheckName", "duplicateFileLoadCheck");
        duplicateFileLoadCheck.put("isDimcCheckEnabled", "${DUPLICATE_FILELOAD_CHECK_ENABLED}");
        duplicateFileLoadCheck.put("dimcCheckFailureFlag", "${DUPLICATE_FILELOAD_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(duplicateFileLoadCheck);

        Map<Object, Object> dataVolumeConsistencyCheck = new HashMap<>();
        dataVolumeConsistencyCheck.put("dimcCheckName", "dataVolumeConsistencyCheck");
        dataVolumeConsistencyCheck.put("isDimcCheckEnabled", "${DATA_VOLUME_CONSISTENCY_CHECK_ENABLED}");
        dataVolumeConsistencyCheck.put("dimcCheckFailureFlag", "${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}");
        dataVolumeConsistencyCheck.put("lowerThreshold", "${LOWER_THRESHOLD}");
        dataVolumeConsistencyCheck.put("pastNumDays", "${NUM_PAST_DAYS}");
        dataVolumeConsistencyCheck.put("isFailJobOnDataError", "${DATA_VOLUME_CONSISTENCY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(dataVolumeConsistencyCheck);

        Map<Object, Object> missingFileCheck = new HashMap<>();
        missingFileCheck.put("dimcCheckName", "missingFileCheck");
        missingFileCheck.put("isDimcCheckEnabled", "${MISSING_FILE_CHECK_ENABLED}");
        missingFileCheck.put("isFailJobOnErrer", "${MISSING_FILE_CHECK_FAILURE_FLAG}"); // Mispelled "Error" in original image
        DIMC_CHECKS_LIST.add(missingFileCheck);

        Map<Object, Object> deliveryCheck = new HashMap<>();
        deliveryCheck.put("dimcCheckName", "deliveryCheck");
        deliveryCheck.put("isDimcCheckEnabled", "${DELIVERY_CHECK_ENABLED}");
        deliveryCheck.put("deliveryCheckSLA", "${DELIVERY_CHECK_SLA}");
        deliveryCheck.put("dimcCheckFailureFlag", "${DELIVERY_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(deliveryCheck);

        //TABLE_CHECKS
        Map<Object, Object> missingDbObjectCheck = new HashMap<>();
        missingDbObjectCheck.put("dimcCheckName", "missingDbObjectCheck");
        missingDbObjectCheck.put("isDimcCheckEnabled", "${MISSING_DB_OBJECT_CHECK_ENABLED}");
        missingDbObjectCheck.put("dimcCheckFailureFlag", "${MISSING_DB_OBJECT_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(missingDbObjectCheck);

        Map<Object, Object> emptySourceCheck = new HashMap<>();
        emptySourceCheck.put("dimcCheckName", "emptySourceCheck");
        emptySourceCheck.put("isDimcCheckEnabled", "${EMPTY_SOURCE_CHECK_ENABLED}");
        emptySourceCheck.put("dimcCheckFailureFlag", "${EMPTY_SOURCE_CHECK_FAILURE_FLAG}");
        DIMC_CHECKS_LIST.add(emptySourceCheck);

    }

    public static List<Map<Object, Object>> getChosenDimcCheckList(List<Integer> chosenIndices) { // 2 usages K097363
        populateDimcChecksList();

        List<Map<Object, Object>> chosenDimcChecksList = new ArrayList<>();
        for (Integer index : chosenIndices) {
            if (index < DIMC_CHECKS_LIST.size()) {
                 Map<Object, Object> dimcCheck = DIMC_CHECKS_LIST.get(index);
                 chosenDimcChecksList.add(dimcCheck);
            }
        }
        return chosenDimcChecksList;
    }

    public ConnJsonRequirement getOrCreateConnectionJsonReq(String requirementId) { // K097363
        Optional<ConnJsonRequirement> existingConnJsonRequirement = connJsonRequirementRepository.findById(requirementId);

        if(existingConnJsonRequirement.isPresent()) {
            return existingConnJsonRequirement.get();
        }
        ObjectMapper objectMapper = new ObjectMapper();
        ObjectNode rootNode = objectMapper.createObjectNode();
        ObjectNode job = objectMapper.createObjectNode();
        ObjectNode wait = objectMapper.createObjectNode();
        String sorName=connJsonRequirement.get().getSorName(); // This line seems problematic as connJsonRequirement might not exist yet. Copied as is.
        String applicationName=connJsonRequirement.get().getApplicationName(); // Same potential issue as above.
        String emailBody = "<p>Hi Team,</p><p> << "+sorName+" / "+applicationName+" Job Name: $yarnApplicationId >> Job has completed. " +
                "</p><p> Job Status: $jobStatus </p><p> Target Date = $targetDate </p><p> Yarn Application ID = $yarnApplicationId</p></html>";

        job.put("emailBody", emailBody);
        job.set("wait", wait);
        ObjectNode email = objectMapper.createObjectNode();
        job.set("email", email);
        rootNode.set("application", job);
        Map<String, Object> rootMap = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});

        ConnJsonRequirement connJsonRequirement = new ConnJsonRequirement(requirementId, rootMap);
        return connJsonRequirementRepository.save(connJsonRequirement);
    }


    public JsonRequirement getOrCreateJsonRequirement(String requirementId) throws JsonProcessingException { // K007363
        String req = requirementId.split(":")[0];
        Optional<JsonRequirement> existingJsonRequirement = jsonRequirementRepository.findById(requirementId);

        if(existingJsonRequirement.isPresent()) {
            return existingJsonRequirement.get();
        }

        Optional<JiraStoryIntake> jiraStoryIntake = jiraStoryIntakeRepository.findById(req);
        if(!jiraStoryIntake.isPresent()) {
             return null;
        }

        Sor sor = sorRepository.findByApplicationName(jiraStoryIntake.get().getApplicationName()).get(0);
        if(sor == null){
             return null;
        }
        ApplicationAuthorizer applicationAuthorizer = applicationAuthRepository.findByApplicationName(jiraStoryIntake.get().getApplicationName()).isEmpty() ?
                null : applicationAuthRepository.findByApplicationName(jiraStoryIntake.get().getApplicationName()).get(0);
        if(applicationAuthorizer==null){
            return null;
        }

        //SOME LOGIC COULD USE CHECK AS HOW TO BE REPLACED WITH USER INPUT
        List<Integer> chosenDimcChecks = Arrays.asList(0);
        SQLScript sqlScript = sqlScriptRepository.get(0); // Assuming get(0) is valid

        ObjectMapper objectMapper = new ObjectMapper();
        ObjectNode rootNode = objectMapper.createObjectNode();

        ObjectNode applicationNode = createApplicationNode(objectMapper, jiraStoryIntake.get(), sor, sqlScript, applicationAuthorizer, chosenDimcChecks);
        rootNode.set("application", applicationNode); // Changed applicationName to applicationNode based on variable name

        Map<String, Object> json = objectMapper.convertValue(rootNode, new TypeReference<Map<String, Object>>() {});
        JsonRequirement jsonRequirement = new JsonRequirement(requirementId, json);

        return jsonRequirementRepository.save(jsonRequirement);
        // return jsonRequirement; // Original code had return inside save(), adjusted based on typical patterns. Reverted to match image.
    }


    private static ObjectNode createApplicationNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer, List<Integer> chosenDimcChecks) throws JsonProcessingException { // K097363
        ObjectNode applicationNode = objectMapper.createObjectNode();
        applicationNode.put("name", jiraStoryIntake.getApplicationName());
        applicationNode.put("isParallelProcessing", "N");
        ObjectNode jobsNode = createJobNode(objectMapper, jiraStoryIntake, sor, sqlScript, applicationAuthorizer, chosenDimcChecks); // Assuming createJobsNode is intended, but image shows createJobNode
        // Changed JobsNode to jobsNode based on variable naming conventions. Reverted to match image.
        applicationNode.set("jobs", jobsNode); // Assuming single job based on method called

        return applicationNode;
    }

    private static ObjectNode createJobNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer, List<Integer> chosenDimcChecks) throws JsonProcessingException { // 1 usage K097363
        ObjectNode jobNode = objectMapper.createObjectNode();

        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            // GCP specific job settings can go here if needed
        }

        jobNode.put("name", jiraStoryIntake.getApplicationName());
        jobNode.put("remedyId", applicationAuthorizer.getAppRemedyId());
        jobNode.put("queryGroup", sor.getSorName().toLowerCase(Locale.ROOT));
        jobNode.put("zone", SANITIZED); // Added SANITIZED constant based on usage elsewhere
        jobNode.put("cluster", sor.getCluster().toLowerCase(Locale.ROOT));


        jobNode.put("jobId", "1");
        jobNode.put("jobName", sor.getSorName().toLowerCase(Locale.ROOT)+TABLE+jiraStoryIntake.getTableName());
                        //+jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT)); // Commented out part from image
        jobNode.put("toLowerCaselobs"+applicationAuthorizer.getApplicationName()); // Line seems incomplete/incorrect in image, transcribed as is.


        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
              // jobNode.put("jobDescription", sor.getSorName().toLowerCase(Locale.ROOT) + " COMP Extract for Hive table - "+jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT)); // Line commented in image
        }
        jobNode.put("isJobEnabled", "Y");
        jobNode.set("email", createEmailNode(objectMapper, jiraStoryIntake, sor));


        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
              jobNode.set("ingestions",createIngestionNode(objectMapper, sor.getSorName().toLowerCase(Locale.ROOT), jiraStoryIntake, sor, sqlScript,chosenDimcChecks)); // ingestionId seems hardcoded

        }
        jobNode.set("dataChecks",createDataChecksNode(objectMapper, jiraStoryIntake, sor, applicationAuthorizer));


        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
              jobNode.set("flows", createFlowsNode(objectMapper, jiraStoryIntake, sor, sqlScript, applicationAuthorizer, SANITIZED)); // rowId replaced with SANITIZED based on signature change

        }
        jobNode.set("fileExtracts",createFileExtractNode(objectMapper, jiraStoryIntake, sqlScript));

        return jobNode;
    }


    private static ObjectNode createEmailNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor) {
        ObjectNode emailNode = objectMapper.createObjectNode();
        emailNode.put("fromAddress", "${EMAIL_ID}");
        emailNode.put("toAddress", "${MAIL_CC_LIST}");
        emailNode.put("subject", sor.getSorName().toUpperCase(Locale.ROOT)+ " Data Extract from "+sor.getSorName().toLowerCase(Locale.ROOT)+" to "+jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT));
        // emailNode.put("subject", sor.getApplicationName()+" - "+jiraStoryIntake.getTableName()); // Commented line from image

        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            emailNode.put("fromAddress", applicationAuthorizer.getFromEmail()); // applicationAuthorizer not available here, copied as is
            emailNode.put("toAddress", "${GROUP_EMAIL}");
            emailNode.put("subject", "COMP Extraction - "+jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT));
        }
        emailNode.put("isPriorityAlert", "Y");

        return emailNode;
    }


    private static ObjectNode createFileExtractNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, SQLScript sqlScript) { // 1 usage K007363
        ObjectNode fileExtractNode = objectMapper.createObjectNode();
        ArrayNode fileExtractArray = objectMapper.createArrayNode();

        fileExtractNode.put("isFileExtractEnabled", "Y");
        fileExtractNode.put("fileDescription", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT)+" to avro");
        fileExtractNode.put("subjectArea", "COMP");
        fileExtractNode.put("system", sor.getSorName()); // sor not available here, copied as is
        fileExtractNode.put("fileName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT)+"_extract.df");
        fileExtractNode.put("fileType", "avro");
        fileExtractNode.put("query", "select * from "+jiraStoryIntake.getTableName()+"_extract_df"); // Table name likely needs schema
        fileExtractNode.put("hdfsFilePath", "hdfs:///tmp/${TARGET_TABLE_NAME}");
        fileExtractNode.set("cdpAdditionalParams", createCdpAdditionalParamsNode(objectMapper, jiraStoryIntake));
        fileExtractArray.add(fileExtractNode); // Adding node to array

        return fileExtractNode; // Returning node instead of array? Copied as is.
        // return fileExtractArray; // This seems more logical but copied as is from image
    }


    private static ObjectNode createCdpAdditionalParamsNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake) {
        ObjectNode paramsNode = objectMapper.createObjectNode();
        paramsNode.put("outputFormat", "avro");
        paramsNode.put("cdpFileName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT)+"_comp_extract");
        paramsNode.put("compressionCodec", "snappy");
        paramsNode.put("isCustomPartitionEnabled", "true");
        paramsNode.put("customPartitionValue", "daily");
        paramsNode.put("desiredPartFileSize", "${PART_FILE_SIZE}");

        return paramsNode;
    }


    private static ObjectNode createFlowsNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer, String rowId) { // K007363 +1
        ObjectNode flowNode = objectMapper.createObjectNode();
        String sorType = jiraStoryIntake.getSorType();
        if ("fw".equalsIgnoreCase(sorType)) { // Assuming 'fw' means FixedWidth
            ObjectNode flow1 = createFixedwidthFileFlownode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, sqlScript, applicationAuthorizer); // Hardcoded flowId="1", zone=SANITIZED
            addTOArray(flowNode, "flow", flow1); // Assuming arrayName "flow" singular
        } else {
            ObjectNode flow1 = createFlowsNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, sqlScript, applicationAuthorizer); // Recursive call? Or calling the other signature? Assuming other signature. Hardcoded flowId="1", zone=SANITIZED
            addTOArray(flowNode, "flow", flow1); // Assuming arrayName "flow" singular
        }
        return flowNode;
    }


    private static ObjectNode createFlowsNode(ObjectMapper objectMapper, String flowId, String zone, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, ApplicationAuthorizer applicationAuthorizer) { // 1 usage new K007363 +1
        // STORAGE MAP (Example - needs actual values)
        Map<String, String> storageNameMap = new HashMap<>();
        storageNameMap.put("UTCP", "utcap"); // Example mapping

        ObjectNode flowNode = objectMapper.createObjectNode();
        flowNode.put("flowId", flowId);
        flowNode.put("flowDesc", jiraStoryIntake.getApplicationName().toUpperCase(Locale.ROOT)+TABLE+jiraStoryIntake.getTableName()
                         +"_"+zone.replaceFirst(".","")+zone.substring(0,1).toUpperCase()+zone.substring(1)); // Complex naming based on zone
        // getTableName()+ " Ingestion"); // Commented part from image
        flowNode.put("flowDatabaseName", sor.getSorName().toLowerCase(Locale.ROOT));
        flowNode.put("isFlowEnabled", "Y");
        flowNode.put("isStandardDataFrameQuery", "Y");
        flowNode.put("registerName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT));

        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
             flowNode.put("query", sqlScript.getDataframeQuery() + " ${DW_LIMIT} ${LIMIT_COUNT}");
             flowNode.put("flowDesc", "Extraction of "+jiraStoryIntake.getSourceTableName()+" for COMP processing.");
             flowNode.put("registerName", jiraStoryIntake.getTableName().toLowerCase(Locale.ROOT)+"_extract_df");
        }

        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
            flowNode.put("sorOutput", ""); // Value seems missing in image
            flowNode.put("flowDatabaseName", jiraStoryIntake.getSourceTableName()+"."+zone.toLowerCase(Locale.ROOT)+"_df");
            flowNode.put("storageName", "${DATALAKE_ENV}/"+storageNameMap.get(sor.getSorName().toUpperCase())+"/"+zone.toLowerCase(Locale.ROOT)+"/"+jiraStoryIntake.getTableName());
            flowNode.put("targetTablePath", "${EXP_EDW_PATH}/"+sor.getSorName().toLowerCase(Locale.ROOT)+"/"+zone.toLowerCase(Locale.ROOT)+"/"+jiraStoryIntake.getTableName());
            flowNode.put("hivePartitionColumn", applicationAuthorizer.getHivePartition());
            flowNode.put("isOverwritePartition", "Y");
            flowNode.put("isCacheEnabled", "Y");
            flowNode.put("readEmail", "N");
            flowNode.put("abortIfEmpty", "N");
        }

        return flowNode;
    }

    public static ObjectNode createIngestionNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, String rowId) { // K097363
        ObjectNode ingestionNode = objectMapper.createObjectNode();
        String sorType = jiraStoryIntake.getSorType();
        if ("fw".equalsIgnoreCase(sorType)) {
             ObjectNode ingestion1 = createFixedwidthFileIngestionNode(objectMapper, "1", jiraStoryIntake, sor); // Hardcoded ingestionId="1"
             addTOArray(ingestionNode, "ingestion", ingestion1); // Assuming arrayName "ingestion" singular
        } else {
             ObjectNode ingestion1 = createIngestionNode(objectMapper, "1", jiraStoryIntake, sor, sqlScript, getChosenDimcCheckList(Arrays.asList(0))); // Hardcoded ingestionId="1", chosenDimcChecks=[0]
             addTOArray(ingestionNode, "ingestion", ingestion1); // Assuming arrayName "ingestion" singular
        }
        return ingestionNode; // Returning the container node, not the array itself
    }


    private static ObjectNode createIngestionNode(ObjectMapper objectMapper, String ingestionId, JiraStoryIntake jiraStoryIntake, Sor sor, SQLScript sqlScript, List<Integer> chosenDimcChecks) throws JsonProcessingException { // 1 usage new
        ObjectNode ingestionNode = objectMapper.createObjectNode();
        // CONNECTION READER MAP (Example - needs actual values)
        Map<String, String> connectionReader = new HashMap<>();
        connectionReader.put("teradata", "teradataConn"); // Example mapping

        ingestionNode.set("dimChecks", objectMapper.readTree(objectMapper.writeValueAsString(getChosenDimcCheckList(chosenDimcChecks))));
        ingestionNode.put(INGESTION_ID, ingestionId); // Using constant
        ingestionNode.put("ingestionName", sor.getSorName().toLowerCase(Locale.ROOT)+TABLE+jiraStoryIntake.getTableName());
        ingestionNode.put("applicationId", jiraStoryIntake.getApplicationName());
        ingestionNode.put("ingestionEnabled", "Y");
        ingestionNode.put("sourceType", jiraStoryIntake.getSorType().toLowerCase(Locale.ROOT));
        ingestionNode.put("sourceDb", sor.getSorName().toLowerCase(Locale.ROOT));
        ingestionNode.put("connectionReader", connectionReader.getOrDefault(jiraStoryIntake.getSorType().toLowerCase(Locale.ROOT), "defaultConn")); // Using map
        ingestionNode.put("sourceTable", jiraStoryIntake.getTableName()+"_raw_df"); // Raw DF naming convention
        ingestionNode.put("dataFrameName", jiraStoryIntake.getTableName());
        ingestionNode.put("query", sqlScript.getIngestionQuery() + " ${DW_LIMIT} ${LIMIT_COUNT}");
        ingestionNode.put("isAutocastToString", "Y");
        ingestionNode.put("isOverwritePartition", "Y");
        ingestionNode.put("isCacheEnabled", "Y");
        ingestionNode.put("isDverwritePartition", "Y"); // Typo 'DverwritePartition' in image?
        ingestionNode.put("isInvalidRecsToLog", "N");
        ingestionNode.put("abortIfEmpty", "N");
        ingestionNode.put("isRunReport", "Y");
        return ingestionNode;
    }

    public static ObjectNode createIngestionNode(ObjectMapper objectMapper, String ingestionId, JiraStoryIntake jiraStoryIntake, Sor sor) {
        ObjectNode ingestionNode = objectMapper.createObjectNode();
        ingestionNode.put(INGESTION_ID, ingestionId); // Using constant
        ingestionNode.put("ingestionName", sor.getSorName().toLowerCase(Locale.ROOT)+TABLE+jiraStoryIntake.getTableName());
        ingestionNode.put("targetType", "parquet"); // Hardcoded parquet
        ingestionNode.put("isCacheEnabled", "Y");
        ingestionNode.put("isOverwritePartition", "Y");
        ingestionNode.put("readEmail", "N");
        ingestionNode.put("abortIfEmpty", "N");
        ingestionNode.put("isRunReport", "Y");
        return ingestionNode;
    }


    private static ObjectNode createDataChecksNode(ObjectMapper objectMapper, JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer) { // 1 usage new
        ObjectNode dataChecksNode = objectMapper.createObjectNode();
        dataChecksNode.put("type", "${AUDIT_DATABASE}"); // Example placeholder

        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
             ObjectNode check1 = createFixedwidthFileCheckNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, applicationAuthorizer); // Hardcoded checkId="1", zone=SANITIZED
             dataChecksNode.put("acdcdatabase", applicationAuthorizer.getAuditTable()); // Typo 'acdcdatabase'?
             addTOArray(dataChecksNode, "checks", check1); // Assuming arrayName "checks"
        }
        else {
             ObjectNode check1 = createFixedwidthFileCheckNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, applicationAuthorizer); // Hardcoded checkId="1", zone=SANITIZED
             dataChecksNode.put("acdcdatabase", applicationAuthorizer.getAuditTable()); // Typo 'acdcdatabase'?
             addTOArray(dataChecksNode, "checks", check1); // Assuming arrayName "checks"
        }


        if("gcp".equalsIgnoreCase(jiraStoryIntake.getTargetType())) {
             ObjectNode check1 = createCheckNode(objectMapper, "1", SANITIZED, jiraStoryIntake, sor, applicationAuthorizer); // createCheckNode not defined in provided images, copied as is. Hardcoded checkId="1", zone=SANITIZED
             dataChecksNode.put("acdcdatabase", applicationAuthorizer.getAuditTable());
             addTOArray(dataChecksNode, "checks", check1);
        }

       // addDataChecksNode.put(fieldName: "checks", check1); // Incorrect syntax, likely pseudo-code in image
       // } // Extra closing brace in image?
        return dataChecksNode;
    }


    private static ObjectNode createFixedwidthFileCheckNode(ObjectMapper objectMapper, String checkId, String zone, JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer) { // K097363
        ObjectNode checkNode = objectMapper.createObjectNode();
        checkNode.put("checkId", checkId);
        checkNode.put("checkName", "HORCNTCHK"); // Example check name
        checkNode.put("checkDesc", "Data Count Check");
        checkNode.put("isCheckEnabled", "Y");
        checkNode.put("readSourceQueryFromPath", "N"); // Changed from Y in other method?
        checkNode.put("sourceQuery", "select count(*) from "+jiraStoryIntake.getTableName()+"_raw_df");
        checkNode.put("targetQuery", "select count(*) from "+jiraStoryIntake.getTargetSchema()+"."+jiraStoryIntake.getTableName()+" where "+applicationAuthorizer.getHivePartition()+"='${BUSINESS_DATE}'");
        checkNode.put("isFailJobOnDataCheckError", "N"); // Changed from Y in other method?

        return checkNode;
    }

    private static void addTOArray(ObjectNode parent, String arrayName, ObjectNode item) { // 7 usages new
        ArrayNode array;
        if(parent.has(arrayName) && parent.get(arrayName).isArray()){
             array = (ArrayNode) parent.get(arrayName);
        } else {
             array = parent.putArray(arrayName);
        }
        array.add(item);
    }


    // METHODS FOR CREATION OF INGESTION NODE FOR ALL FILE TYPES ////////////////////////////
    private static ObjectNode createFixedwidthFileIngestionNode(ObjectMapper objectMapper, String ingestionId, JiraStoryIntake jiraStoryIntake, Sor sor) throws JsonProcessingException { // 1 usage new
        ObjectNode ingestionNode = objectMapper.createObjectNode();
        ObjectNode dimChecks = objectMapper.createObjectNode(); // Example dimChecks, needs population
        // Map<String, String> additonalConfigs = new HashMap<>(); // Declared but not used in visible code
        ArrayNode dataArrayNode = objectMapper.createArrayNode(); // Declared but not used in visible code
        List<Map<Object, Object>> chosenDimChecks = getChosenDimcCheckList(Arrays.asList(0)); // Hardcoded checks [0]
        for(Map<Object, Object> dimcCheck : chosenDimChecks){
            ObjectNode dimcCheckNode = objectMapper.convertValue(dimcCheck, ObjectNode.class);
            // dimcCheckNode.put("dimcCheckName", dimcCheck.get("dimcCheckName").toString()); // Seems redundant if convertValue works
            // dimcCheckNode.put("isDimcCheckEnabled", dimcCheck.get("isDimcCheckEnabled").toString()); // Seems redundant
            dataArrayNode.add(dimcCheckNode); // Adding to dataArrayNode, not dimChecks node?
        }

        dimChecks.set("dimcCheck", dataArrayNode); // Setting array inside dimChecks node

        // editoneResourcePage.put("quote", "\""); // editoneResourcePage not defined
        // editoneResourcePage.put("escape", "\\"); // editoneResourcePage not defined
        ingestionNode.set("dimChecks", dimChecks); // Setting the populated dimChecks node
        ingestionNode.put(INGESTION_ID, ingestionId); // Using constant
        ingestionNode.put("ingestionName", sor.getSorName().toLowerCase()+TABLE+jiraStoryIntake.getTableName());
        ingestionNode.put("ingestionDesc", sor.getSorName().toLowerCase()+TABLE+jiraStoryIntake.getTableName()); // Same as name?
        ingestionNode.put("applicationId", jiraStoryIntake.getApplicationName());
        ingestionNode.put("ingestionEnabled", "Y");
        ingestionNode.put("sourceType", "fixedWidthFile");
        ingestionNode.put("sourceName", jiraStoryIntake.getTableName()+"_df"); // ASK THIS
        ingestionNode.put("stagingFilePath", "${EXP_EDW_PATH}");
        ingestionNode.put("targetType", "table");
        ingestionNode.put("dataFileFullPath", "${SOURCE_FILE_DIR}/${SOURCE_FILE_NAME}");
        ingestionNode.put("schemaFileName", "${CONFIG_DIR_PATH}/config/"+sor.getSorName()+"/${FEED_NAME}/metadata/"+jiraStoryIntake.getTableName()+".schema");
        // ingestionNode.put("additionalConfigs", objectMapper.readTree(objectMapper.writeValueAsString(additonalConfigs))); // additonalConfigs is empty
        ingestionNode.put("isCacheEnabled", "Y");
        ingestionNode.put("isOverwritePartition", "Y");
        ingestionNode.put("isInvalidRecsToLog", "N");
        ingestionNode.put("abortIfEmpty", "N");
        ingestionNode.put("isRunReport", "Y");
        return ingestionNode;
    }

// --- Gap in provided images: Lines ~468 to ~499 are missing ---

// --- Start of code from Image 5 (around line 500) ---
// Assuming this is the end of createFixedwidthFileFlownode which started before the gap
    flowNode.put(fieldName: "isOverwritePartition", V: "Y");
    flowNode.put(fieldName: "isCacheEnabled", V: "Y");
    flowNode.put(fieldName: "readEmail", V: "N");
    flowNode.put(fieldName: "abortIfEmpty", V: "N");

    return flowNode;
    }

    //////////////////////METHODS FOR GETTING DATA CHECKS FOR ALL FILE TYPES //////////////////////////
    private static ObjectNode createFixedwidthFileCheckNode(ObjectMapper objectMapper, String checkId, String zone, JiraStoryIntake jiraStoryIntake, Sor sor, ApplicationAuthorizer applicationAuthorizer) { // 1 usage new
        // This method appears to be a duplicate of the one at lines 396-418. Transcribing as shown.
        ObjectNode checkNode = objectMapper.createObjectNode();
        checkNode.put("checkId", checkId);
        checkNode.put("checkName", "HORCNTCHK"); // Example check name
        checkNode.put("checkDesc", "Data Count Check");
        checkNode.put("isCheckEnabled", "Y");
        checkNode.put("readSourceQueryFromPath", "N");
        checkNode.put("sourceQuery", "select count(*) from "+jiraStoryIntake.getTableName()+"_df"); // Differs slightly (_df vs _raw_df) from the previous instance
        checkNode.put("targetQuery", "select count(*) from "+jiraStoryIntake.getTargetSchema()+"."+jiraStoryIntake.getTableName()+" where "+applicationAuthorizer.getHivePartition()+"='${BUSINESS_DATE}'");
        checkNode.put("isFailJobOnDataCheckError", "N");

        return checkNode;
    }

} // End of class JsonRequirementService
