package com.wellsfargo.utcap.service;

import com.wellsfargo.utcap.exception.ResourceNotFoundException;
import com.wellsfargo.utcap.model.DagSql;
import com.wellsfargo.utcap.model.HiveTable;
import com.wellsfargo.utcap.model.JiraStoryIntake;
import com.wellsfargo.utcap.model.SqlDatatype;
import com.wellsfargo.utcap.repository.DagSqlRepository;
import com.wellsfargo.utcap.repository.HiveTableRepository;
import com.wellsfargo.utcap.repository.JiraStoryIntakeRepository;
import com.wellsfargo.utcap.repository.SqlDatatypeRepository;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.*;

@Service
public class DagSqlService {

    private static final String DEFAULT_MAPPING_ID = "defaultMapping";
    private static final String DEFAULT_TYPE       = "STRING";
    private static final String INTEGER_TYPE       = "INTEGER";
    private static final int    PAIR_SIZE          = 2;

    @Autowired
    private DagSqlRepository dagSqlRepository;

    @Autowired
    private JiraStoryIntakeRepository jiraStoryIntakeRepository;

    @Autowired
    private HiveTableRepository hiveTableRepository;

    @Autowired
    private SqlDatatypeRepository sqlDatatypeRepository;

    public DagSql getDagSql(String requirementId) {
        return dagSqlRepository
            .findByRequirementId(requirementId)
            .orElseGet(() -> generateAndSaveDagSql(requirementId));
    }

    public DagSql generateAndSaveDagSql(String requirementId) {
        JiraStoryIntake jiraStory = jiraStoryIntakeRepository
            .findById(requirementId)
            .orElseThrow(() -> new ResourceNotFoundException(
                "JiraStoryIntake not found for requirementId: " + requirementId));

        String sourceSchema    = jiraStory.getSourceSchema();
        String sourceTableName = jiraStory.getSourceTableName();
        String tableIdentifier = sourceSchema + "." + sourceTableName;

        String targetSchema    = jiraStory.getTargetSchema();
        String appName         = jiraStory.getApplicationName();
        String targetTableName = jiraStory.getTargetTableName();
        String sqlDbTableName  = targetSchema + "." + appName + targetTableName;

        List<HiveTable> hiveTableList = hiveTableRepository.findByTableName(tableIdentifier);
        HiveTable hiveTable = Optional.ofNullable(hiveTableList)
            .filter(list -> !list.isEmpty())
            .map(list -> list.get(0))
            .orElseThrow(() -> new ResourceNotFoundException(
                "HiveTable not found for table: " + tableIdentifier));

        SqlDatatype sqlDatatype = sqlDatatypeRepository
            .findById(DEFAULT_MAPPING_ID)
            .orElseGet(this::getDefaultSqlDatatype);
        Map<String, String> mapping = sqlDatatype.getMappings();

        String columnDefinitions = generateColumnDefinitions(hiveTable.getFileSetAttr(), mapping);

        String createSql = "CREATE TABLE " + sqlDbTableName + "(\n"
            + columnDefinitions + "\n);";
        String deleteSql = "DROP TABLE " + sqlDbTableName;

        DagSql dagSql = new DagSql();
        dagSql.setRequirementId(requirementId);
        dagSql.setCreateSqlContent(createSql);
        dagSql.setDeleteSqlContent(deleteSql);
        dagSql.setCreatedAt(LocalDateTime.now());
        dagSql.setUpdatedAt(LocalDateTime.now());

        return dagSqlRepository.save(dagSql);
    }

    private String generateColumnDefinitions(String fileSetAttr, Map<String, String> mapping) {
        String[] tokens = fileSetAttr.split("\\|");
        List<String> clean = new ArrayList<>();
        for (String t : tokens) {
            if (t != null && !t.trim().isEmpty()) {
                clean.add(t.trim());
            }
        }
        if (clean.size() % PAIR_SIZE != 0) {
            throw new IllegalArgumentException(
                "Invalid fileSetAttr format; expected pairs of name|type");
        }

        StringBuilder builder = new StringBuilder();
        for (int i = 0; i < clean.size(); i += PAIR_SIZE) {
            String colToken = clean.get(i);
            String colName = colToken.startsWith("#")
                ? colToken.substring(1)
                : colToken;

            String hiveType = clean.get(i + 1).toUpperCase(Locale.ROOT);
            String baseType = hiveType.contains("(")
                ? hiveType.substring(0, hiveType.indexOf('('))
                : hiveType;

            String bqType = mapping.getOrDefault(baseType, DEFAULT_TYPE);

            builder.append("  ").append(colName).append(" ").append(bqType);
            if (i < clean.size() - PAIR_SIZE) {
                builder.append(",\n");
            }
        }
        return builder.toString();
    }

    public DagSql updateDagSql(String requirementId,
                               String createSqlContent,
                               String deleteSqlContent) {

        DagSql existing = dagSqlRepository
            .findByRequirementId(requirementId)
            .orElseThrow(() -> new ResourceNotFoundException(
                "DagSql record not found for requirementId: " + requirementId));

        existing.setCreateSqlContent(createSqlContent);
        existing.setDeleteSqlContent(deleteSqlContent);
        existing.setUpdatedAt(LocalDateTime.now());
        return dagSqlRepository.save(existing);
    }

    private SqlDatatype getDefaultSqlDatatype() {
        Map<String, String> defaults = new HashMap<>();
        defaults.put("STRING",   DEFAULT_TYPE);
        defaults.put("CHAR",     DEFAULT_TYPE);
        defaults.put("VARCHAR",  DEFAULT_TYPE);
        defaults.put("INT",      INTEGER_TYPE);
        defaults.put("INTEGER",  INTEGER_TYPE);
        defaults.put("BIGINT",   INTEGER_TYPE);
        defaults.put("SMALLINT", INTEGER_TYPE);
        defaults.put("TINYINT",  INTEGER_TYPE);
        defaults.put("BOOLEAN",  "BOOLEAN");
        defaults.put("FLOAT",    "FLOAT");
        defaults.put("DOUBLE",   "FLOAT");
        defaults.put("DECIMAL",  "NUMERIC");
        defaults.put("DATE",     "DATE");
        defaults.put("TIMESTAMP","TIMESTAMP");
        defaults.put("BINARY",   "BYTES");

        SqlDatatype dt = new SqlDatatype();
        dt.setId(DEFAULT_MAPPING_ID);
        dt.setMappings(defaults);
        return dt;
    }
}
